{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNIIuFfSHwgr3g6tIrKUHec",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Datarian723/AutoHPOJupyter/blob/main/AutoHpoFINAL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#TODO"
      ],
      "metadata": {
        "id": "jsclqRyJhDXr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "MIGRATION OF objective FROM: Objective TO: ConfigSpace IN PROGRESS.\n",
        "(5)MOVE_TO: KaggleMTCSVariants.ipnyb. TEST: AutoHPO.(Now you can put in other 'action_funcs' as well; 'dls_action', 'dset_action'...etc)\n",
        "(6)WHILE_RUNNING_(5): START: https://www.kaggle.com/competitions/child-mind-institute-problematic-internet-use/data\n",
        "\n",
        "\n",
        "(USER_GUIDE)GUIDE: IF (the number of action_func(s)) << (the number of params),\n",
        "                   THEN recommend the user to create original_func, original_func_var1, original_func_var2,...etc\n",
        "                   (where original_func_var(i) is simply an i'th COPY of the original_func. This works!!!!!)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "rWuRvNI7hGC4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "dec31aaa-f6ef-455a-b0ce-6820e89ab5f8"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nMIGRATION OF objective FROM: Objective TO: ConfigSpace IN PROGRESS.\\n(5)MOVE_TO: KaggleMTCSVariants.ipnyb. TEST: AutoHPO.(Now you can put in other 'action_funcs' as well; 'dls_action', 'dset_action'...etc)\\n(6)WHILE_RUNNING_(5): START: https://www.kaggle.com/competitions/child-mind-institute-problematic-internet-use/data\\n\\n\\n(USER_GUIDE)GUIDE: IF (the number of action_func(s)) << (the number of params),\\n                   THEN recommend the user to create original_func, original_func_var1, original_func_var2,...etc\\n                   (where original_func_var(i) is simply an i'th COPY of the original_func. This works!!!!!)\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fastai --upgrade --quiet\n",
        "!pip install optuna --upgrade --quiet\n",
        "!pip install optuna-integration --upgrade --quiet\n",
        "!pip install scipy --upgrade --quiet"
      ],
      "metadata": {
        "id": "4RmNR-QyRUJy"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Prep"
      ],
      "metadata": {
        "id": "LiJWSPy8mFr4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# Special Imports\n",
        "# ---------------------------\n",
        "from __future__ import annotations  # Enable forward references for type hints\n",
        "\n",
        "# ---------------------------\n",
        "# Standard Libraries\n",
        "# ---------------------------\n",
        "import os  # Operating system utilities\n",
        "import gc  # Garbage collection for memory management\n",
        "import logging  # Logging utilities\n",
        "import sys  # System-specific parameters and functions\n",
        "import unittest  # For unit-testing Python code\n",
        "import copy  # For deep and shallow copying of objects\n",
        "import math  # Math operations\n",
        "import random  # Random number generation\n",
        "import gzip  # For compressing and decompressing data\n",
        "import ast #For building an Abstract Syntax Tree; used in class 'Objective'.\n",
        "import warnings  # For handling warnings\n",
        "import textwrap #For dedenting texts.\n",
        "from functools import partial  # Partial function application\n",
        "from collections import defaultdict  # Dictionary subclass with default values\n",
        "from itertools import chain # Used for 'flattening' Collection of Collections.\n",
        "from sortedcontainers import SortedSet  # Sorted set implementation\n",
        "from unittest.mock import Mock, patch  # For mocking and patching in unit tests\n",
        "from inspect import signature  # Inspecting callable signatures\n",
        "from typing import Callable, Union  # Type annotations for type hints\n",
        "from ast import Is  # Abstract syntax tree utilities\n",
        "from abc import ABC, abstractmethod  # Abstract Base Classes for inheritance\n",
        "\n",
        "# ---------------------------\n",
        "# Scientific Libraries\n",
        "# ---------------------------\n",
        "import numpy as np  # Array and matrix operations\n",
        "import pandas as pd  # Data manipulation and analysis\n",
        "import matplotlib.pyplot as plt  # Plotting and visualization\n",
        "\n",
        "# ---------------------------\n",
        "# PyTorch and Related Modules\n",
        "# ---------------------------\n",
        "import torch  # Core PyTorch library for tensor operations\n",
        "import torch.nn as nn  # Neural network modules\n",
        "import torch.nn.functional as F  # Functional API for neural networks\n",
        "import torchvision.models as models  # Pre-trained models from torchvision\n",
        "from torch.multiprocessing import Pool, set_start_method  # Multiprocessing utilities for PyTorch\n",
        "\n",
        "# ---------------------------\n",
        "# FastAI Libraries\n",
        "# ---------------------------\n",
        "from fastai.vision.all import *  # All-in-one import for vision-specific modules\n",
        "from fastai.metrics import *  # Evaluation metrics for model performance\n",
        "from fastai.callback.hook import *  # Hooks for capturing intermediate model states\n",
        "from fastai.callback.tracker import *  # Training callback trackers (e.g., early stopping)\n",
        "from fastai.learner import Learner  # Core Learner object for model training\n",
        "\n",
        "# ---------------------------\n",
        "# Optuna for Hyperparameter Optimization\n",
        "# ---------------------------\n",
        "import optuna  # Core Optuna library for optimization\n",
        "from optuna.integration import FastAIPruningCallback  # Pruning callback for FastAI integration\n",
        "from optuna.trial import Trial, FrozenTrial  # Trial classes for defining and managing trials\n",
        "\n",
        "# ---------------------------\n",
        "# Google Colab Utilities\n",
        "# ---------------------------\n",
        "from google.colab import files  # File utilities for Colab\n",
        "from google.colab import drive  # Mounting Google Drive in Colab"
      ],
      "metadata": {
        "id": "JCc3ckuVaO6v"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Class: Var"
      ],
      "metadata": {
        "id": "VwK_hENk-VzT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Var:\n",
        "    def __init__(self, var_type: str, source: dict, belongs_to: str=None):\n",
        "        sample_method = source['sample']\n",
        "        distribution = source['params']['choices'] if sample_method=='categorical' else source['params']\n",
        "        #ERROR CHECK:\n",
        "        self._validate_inputs(distribution, sample_method)\n",
        "\n",
        "        #Member initializations:\n",
        "        self.var_type = var_type\n",
        "        self.sample_method = sample_method\n",
        "        self._belongs_to = belongs_to\n",
        "        #frozen_dist is used for value conversion, in case value NOT currently in 'self.get_dist()' is requested for conversion.\n",
        "        self._frozen_dist = distribution.copy()\n",
        "        self._dist = list(range(len(self._frozen_dist))) if sample_method=='categorical' else distribution.copy()\n",
        "\n",
        "    def _validate_inputs(self, distribution, sample_method: str):\n",
        "        if not isinstance(distribution, (list,dict)):\n",
        "            raise ValueError(f\"Input 'distribution' MUST be of type in (list,dict) but found: {type(distribution)}\")\n",
        "        if sample_method not in ['categorical', 'float', 'int']:\n",
        "            raise ValueError(f\"sample method MUST be one of ['categorical', 'float', 'int'] but found: {sample_method}\")\n",
        "        if sample_method!='categorical' and not isinstance(distribution, dict) or sample_method=='categorical' and not isinstance(distribution, list):\n",
        "            raise ValueError(f\"sample_method: {sample_method} is not compatible with distribution: {distribution}. Categorical sampling requires a list, rest requires a dictionary\")\n",
        "        cat_valid_params = sample_method!='categorical' or len(distribution)\n",
        "        non_cat_valid_params = sample_method=='categorical' or (distribution.get('low', 0) <= distribution.get('high', -1))\n",
        "        if not cat_valid_params or not non_cat_valid_params:\n",
        "            msg = f\"Categorical distribution needs at least one value in it: {distribution}\" if sample_method=='categorical' else f\"distribution needs (low,high) as keys and distribution[low]<=distribution[high] but found: {distribution}\"\n",
        "            raise ValueError(msg)\n",
        "\n",
        "    def __hash__(self):\n",
        "        return hash((self.var_type, self._belongs_to))\n",
        "\n",
        "    def __eq__(self, other: Var):\n",
        "        return all([\n",
        "            isinstance(other, Var),\n",
        "            self.var_type == other.var_type,\n",
        "            self.sample_method == other.sample_method,\n",
        "            self._belongs_to == other._belongs_to,\n",
        "            self._frozen_dist == other._frozen_dist,\n",
        "            self._dist == other._dist\n",
        "        ])\n",
        "\n",
        "    def __str__(self):\n",
        "        if self.sample_method == 'categorical':\n",
        "            config_space = f\"'choices' = {[self._frozen_dist[idx] for idx in self._dist]}\"\n",
        "        else:\n",
        "            config_space = (\n",
        "                f\"'low': {self._dist['low']}, \"\n",
        "                f\"'high': {self._dist['high']}, \"\n",
        "                f\"'log': {self._dist.get('log', False)}, \"\n",
        "                f\"'step': {self._dist.get('step', 'unspecified')}\"\n",
        "            )\n",
        "        output = f\"Var of type: {self.var_type}, signature of: {self._belongs_to}\\nConfiguration Space:\\n{config_space}\\n\"\n",
        "        return output\n",
        "\n",
        "    def is_fixed(self):\n",
        "        return len(self._dist)==1 if self.sample_method=='categorical' else self._dist['low']==self._dist['high']\n",
        "\n",
        "    def is_sig_var(self):\n",
        "        return self._belongs_to!='None'\n",
        "\n",
        "    def belongs_to(self):\n",
        "        return self._belongs_to\n",
        "\n",
        "    def contain_idx(self, idx, current: bool = False):\n",
        "        if self.sample_method!='categorical' and type(idx)!=eval(self.sample_method) or self.sample_method=='categorical' and type(idx)!=int:\n",
        "            return False\n",
        "        if self.sample_method!='categorical':\n",
        "            dist = self._dist if current else self._frozen_dist\n",
        "            return dist['low'] <= idx <= dist['high']\n",
        "        return idx in self._dist if current else 0 <= idx < len(self._frozen_dist)\n",
        "\n",
        "    def contain_val(self, val, current: bool = False):\n",
        "        if self.sample_method != 'categorical':\n",
        "            return self.contain_idx(idx=val, current=current)\n",
        "        if self.var_type=='action_func' and isinstance(val, Action):\n",
        "            val = val.action_func\n",
        "        return val in self.distribution() if current else val in self._frozen_dist\n",
        "\n",
        "    def convert_idx_to_val(self, idx):\n",
        "        if not self.contain_idx(idx=idx, current=False):\n",
        "            raise KeyError(f\"Input index '{idx}' is NOT a proper index for conversion via Var:{self.var_type}'s distribution: {list(range(len(self._frozen_dist))) if self.sample_method=='categorical' else self._frozen_dist}\")\n",
        "        if self.sample_method!='categorical':\n",
        "            return idx\n",
        "        return self._frozen_dist[idx]\n",
        "\n",
        "    def convert_val_to_idx(self, val):\n",
        "        if not self.contain_val(val=val, current=False):\n",
        "            raise ValueError(f\"Input value '{val}' is NOT a proper value for conversion via Var:{self.var_type}'s distribution: {self._frozen_dist}\")\n",
        "        if self.sample_method!='categorical':\n",
        "            return val\n",
        "        return self._frozen_dist.index(val)\n",
        "\n",
        "    def distribution(self, indices: bool = False):\n",
        "        if self.sample_method!='categorical':\n",
        "            return self._dist\n",
        "        if indices:\n",
        "            return self._dist\n",
        "        return [self._frozen_dist[idx] for idx in self._dist]\n",
        "\n",
        "    def sample(self, trial):\n",
        "        if not isinstance(trial, (Trial, FrozenTrial)):\n",
        "            raise ValueError(f\"Input 'trial' must be of type in (optuna::Trial, optuna::FrozenTrial) but found: {trial}\")\n",
        "        name=self.var_type+'>'+str(self._belongs_to)\n",
        "        if isinstance(trial, FrozenTrial):\n",
        "            idx_or_val = trial.params[name]\n",
        "            #sample_method=='categorical' <-> retrieved value is an index NOT an actual value.\n",
        "            sampled_val = self.convert_idx_to_val(idx_or_val) if self.sample_method=='categorical' else idx_or_val\n",
        "            return sampled_val\n",
        "\n",
        "        sampled_val = None\n",
        "        if self.sample_method == 'categorical':\n",
        "            idx = trial.suggest_categorical(name, choices=self._dist)\n",
        "            sampled_val = self._frozen_dist[idx]\n",
        "        elif self.sample_method == 'float':\n",
        "            sampled_val = trial.suggest_float(name, **self._dist)\n",
        "        elif self.sample_method == 'int':\n",
        "            sampled_val = trial.suggest_int(name, **self._dist)\n",
        "\n",
        "        return sampled_val\n",
        "\n",
        "    def update(self, vals: list):\n",
        "        #Proior processing:\n",
        "        if not len(vals):\n",
        "            return\n",
        "        if not all(isinstance(val, (int,float)) for val in vals):\n",
        "            raise ValueError(f\"Var of type: {self.var_type} with sample_method: {self.sample_method} recieved update request with 'vals': {vals} which contain element(s) NOT of type in (int,float)\")\n",
        "        vals = sorted(vals.copy()) if self.sample_method=='categorical' else vals\n",
        "\n",
        "        set_vals = set(vals)\n",
        "        if len(set_vals)==1:\n",
        "            vals = [vals[0]]\n",
        "\n",
        "        #ERROR CHECKS:\n",
        "        if self.sample_method!='categorical' and (min(vals)<self._dist['low'] or max(vals)>self._dist['high']):\n",
        "            raise ValueError(f\"Var of type: {self.var_type} with sample_method: {self.sample_method} recieved update request with 'vals': {vals} NOT in valid range: [{self._dist['low']},{self._dist['high']}]. Note the initial search space: [{self._frozen_dist['low']},{self._frozen_dist['high']}].\")\n",
        "        if self.sample_method=='categorical' and not set_vals.issubset(set(self._dist)):\n",
        "            raise ValueError(f\"Var of type: {self.var_type} with sample_method: {self.sample_method} recieved update request with 'vals': {vals} NOT in valid range: {self._dist}. Note the initial search space: {self._frozen_dist}\")\n",
        "\n",
        "        #UPDATE: configuration space.\n",
        "        if self.sample_method=='categorical':\n",
        "            self._dist = vals\n",
        "        else:\n",
        "            self._dist['low'] = min(vals)\n",
        "            self._dist['high'] = max(vals)"
      ],
      "metadata": {
        "id": "6FfK4Xv9-ZeR"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Test: Var"
      ],
      "metadata": {
        "id": "Yr4PS-9cU9Py"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TestVar(unittest.TestCase):\n",
        "\n",
        "    def setUp(self):\n",
        "        self.categorical_source = {\n",
        "            'params': {'choices': [1, 2, 3, 4, 5]},\n",
        "            'sample': 'categorical'\n",
        "        }\n",
        "        self.float_source = {\n",
        "            'params': {'low': 0.1, 'high': 1.0},\n",
        "            'sample': 'float'\n",
        "        }\n",
        "        self.int_source = {\n",
        "            'params': {'low': 1, 'high': 10},\n",
        "            'sample': 'int'\n",
        "        }\n",
        "\n",
        "    def test_init_categorical(self):\n",
        "        var = Var(var_type='test_var', source=self.categorical_source)\n",
        "        self.assertEqual(var.var_type, 'test_var')\n",
        "        self.assertEqual(var.sample_method, 'categorical')\n",
        "        self.assertEqual(var._frozen_dist, [1, 2, 3, 4, 5])\n",
        "        self.assertEqual(var._dist, [0, 1, 2, 3, 4])\n",
        "        self.assertFalse(var.is_fixed())\n",
        "\n",
        "    def test_init_float(self):\n",
        "        var = Var(var_type='test_var', source=self.float_source)\n",
        "        self.assertEqual(var.var_type, 'test_var')\n",
        "        self.assertEqual(var.sample_method, 'float')\n",
        "        self.assertEqual(var._frozen_dist, {'low': 0.1, 'high': 1.0})\n",
        "        self.assertEqual(var._dist, {'low': 0.1, 'high': 1.0})\n",
        "        self.assertFalse(var.is_fixed())\n",
        "\n",
        "    def test_init_int(self):\n",
        "        var = Var(var_type='test_var', source=self.int_source)\n",
        "        self.assertEqual(var.var_type, 'test_var')\n",
        "        self.assertEqual(var.sample_method, 'int')\n",
        "        self.assertEqual(var._frozen_dist, {'low': 1, 'high': 10})\n",
        "        self.assertEqual(var._dist, {'low': 1, 'high': 10})\n",
        "        self.assertFalse(var.is_fixed())\n",
        "\n",
        "    def test_init_error_invalid_sample_method(self):\n",
        "        with self.assertRaises(ValueError):\n",
        "            Var(var_type='test_var', source={'params': {'choices': [1, 2]}, 'sample': 'invalid'})\n",
        "\n",
        "    def test_init_error_invalid_distribution_type(self):\n",
        "        with self.assertRaises(ValueError):\n",
        "            Var(var_type='test_var', source={'params': {'choices': {'key': 'value'}}, 'sample': 'categorical'})\n",
        "\n",
        "    def test_contain_idx(self):\n",
        "        var = Var(var_type='test_var', source=self.categorical_source)\n",
        "        self.assertTrue(var.contain_idx(2))\n",
        "        self.assertFalse(var.contain_idx(5))\n",
        "\n",
        "        var_float = Var(var_type='test_var', source=self.float_source)\n",
        "        self.assertTrue(var_float.contain_idx(0.5))\n",
        "        self.assertFalse(var_float.contain_idx(1.1))\n",
        "\n",
        "    def test_contain_val(self):\n",
        "        var = Var(var_type='test_var', source=self.categorical_source)\n",
        "        self.assertTrue(var.contain_val(3))\n",
        "        self.assertFalse(var.contain_val(6))\n",
        "\n",
        "        var_float = Var(var_type='test_var', source=self.float_source)\n",
        "        self.assertTrue(var_float.contain_val(0.5))\n",
        "        self.assertFalse(var_float.contain_val(1.1))\n",
        "\n",
        "    def test_convert_idx_to_val(self):\n",
        "        var = Var(var_type='test_var', source=self.categorical_source)\n",
        "        self.assertEqual(var.convert_idx_to_val(2), 3)\n",
        "\n",
        "        var_float = Var(var_type='test_var', source=self.float_source)\n",
        "        self.assertEqual(var_float.convert_idx_to_val(0.5), 0.5)\n",
        "\n",
        "    def test_convert_val_to_idx(self):\n",
        "        var = Var(var_type='test_var', source=self.categorical_source)\n",
        "        self.assertEqual(var.convert_val_to_idx(3), 2)\n",
        "\n",
        "        with self.assertRaises(ValueError):\n",
        "            var.convert_val_to_idx(6)\n",
        "\n",
        "    def test_distribution(self):\n",
        "        var = Var(var_type='test_var', source=self.categorical_source)\n",
        "        self.assertEqual(var.distribution(), [1, 2, 3, 4, 5])\n",
        "\n",
        "        var_float = Var(var_type='test_var', source=self.float_source)\n",
        "        self.assertEqual(var_float.distribution(), {'low': 0.1, 'high': 1.0})\n",
        "\n",
        "    def test_sample_categorical(self):\n",
        "        var = Var(var_type='test_var', source=self.categorical_source)\n",
        "        trial = optuna.create_study(direction='minimize').ask()\n",
        "        trial.suggest_categorical = Mock(return_value=2)\n",
        "        sampled_val = var.sample(trial)\n",
        "        self.assertEqual(sampled_val, 3)\n",
        "\n",
        "    def test_sample_float(self):\n",
        "        var = Var(var_type='test_var', source=self.float_source)\n",
        "        trial = optuna.create_study(direction='minimize').ask()\n",
        "        trial.suggest_float = Mock(return_value=0.5)\n",
        "        sampled_val = var.sample(trial)\n",
        "        self.assertEqual(sampled_val, 0.5)\n",
        "\n",
        "    def test_sample_error(self):\n",
        "        var = Var(var_type='test_var', source=self.categorical_source)\n",
        "        with self.assertRaises(ValueError):\n",
        "            var.sample('not_a_trial')\n",
        "\n",
        "    def test_update_categorical(self):\n",
        "        var = Var(var_type='test_var', source=self.categorical_source)\n",
        "        var.update([1, 2])\n",
        "        self.assertEqual(var.distribution(indices=True), [1, 2])\n",
        "        self.assertEqual(var.distribution(), [2, 3])\n",
        "        self.assertFalse(var.is_fixed())\n",
        "\n",
        "    def test_update_float(self):\n",
        "        var = Var(var_type='test_var', source=self.float_source)\n",
        "        var.update([0.2, 0.9])\n",
        "        self.assertEqual(var._dist['low'], 0.2)\n",
        "        self.assertEqual(var._dist['high'], 0.9)\n",
        "\n",
        "    def test_update_error_out_of_range(self):\n",
        "        var = Var(var_type='test_var', source=self.categorical_source)\n",
        "        with self.assertRaises(ValueError):\n",
        "            var.update([0, 6])\n",
        "\n",
        "    def test_str(self):\n",
        "        var = Var(var_type='test_var', source=self.categorical_source)\n",
        "        expected_str = f\"Var of type: test_var, signature of: {'None'}\\nConfiguration Space:\\n'choices' = [1, 2, 3, 4, 5]\\n\"\n",
        "        self.assertEqual(str(var), expected_str)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    unittest.main(argv=['first-arg-is-ignored'], exit=False)"
      ],
      "metadata": {
        "id": "BEzOc3Qjygsf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Class: Action, ModelAction"
      ],
      "metadata": {
        "id": "PFzep6tqVNNy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Action:\n",
        "    def __init__(self, action_func: Callable, sig_dict: dict):\n",
        "        if not all(isinstance(var,Var) for var in sig_dict.values()):\n",
        "            raise ValueError(f\"Constructor of object 'Action' with action_func: '{action_func.__name__}' received an invalid dictionary sig_dict: {sig_dict}\")\n",
        "        self.action_func = action_func\n",
        "        self.sig_dict = sig_dict\n",
        "        self.sig = frozenset(sig_dict.keys())\n",
        "        self.params = {}\n",
        "        self.action_state = None\n",
        "        self._model_loader = None\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sig_dict)\n",
        "\n",
        "    def __hash__(self):\n",
        "        return hash(self.action_func)\n",
        "\n",
        "    def __eq__(self, other: Action):\n",
        "        \"\"\"Two Action objects are considered 'equal'\n",
        "        IFF (1)other is Action object\n",
        "            (2)self.action_func == other.action_func\n",
        "            (3)self.params == other.params\n",
        "            (4)self.action_state == other.action_state\n",
        "        \"\"\"\n",
        "        #TYPE CHECK:\n",
        "        if (isinstance(self, ModelAction) and not isinstance(other, ModelAction)) or not isinstance(other, Action):\n",
        "            return False\n",
        "        #Model loader comparison:\n",
        "        self_model_params = self._model_loader if self._model_loader is None else self._model_loader.keywords\n",
        "        other_model_params = other._model_loader if other._model_loader is None else other._model_loader.keywords\n",
        "        if self_model_params != other_model_params:\n",
        "            return False\n",
        "\n",
        "        #Compare action states if both are dictionaries\n",
        "        if isinstance(self.action_state, dict) and isinstance(other.action_state, dict):\n",
        "            if self.action_state.keys() != other.action_state.keys():\n",
        "                return False\n",
        "            if not all(torch.equal(self.action_state[key], other.action_state[key]) for key in self.action_state.keys()):\n",
        "                return False\n",
        "        elif self.action_state != other.action_state: #IF one is None and the other is not None.\n",
        "            return False\n",
        "\n",
        "        return all([\n",
        "            self.action_func == other.action_func,\n",
        "            self.sig_dict == other.sig_dict,\n",
        "        ])\n",
        "\n",
        "    def __str__(self):\n",
        "        if self._model_loader:\n",
        "            parameters = \" \".join([f\"{key}={val}\" for key,val in self._model_loader.keywords.items()])\n",
        "        else:\n",
        "            parameters = \" \".join([f\"{key}\" for key in self.sig_dict.keys()])\n",
        "        return f\"Action of func: {self.action_func.__name__}\\n\" + f\"Model loader parameters: {parameters}\\n\"\n",
        "\n",
        "    def get_var(self, var_type):\n",
        "        if var_type not in self.keys():\n",
        "            raise KeyError(f\"'{var_type}' is not a valid signature key for Action: '{self.action_func.__name__}'. Available keys: {set(self.keys())}\")\n",
        "        return self.sig_dict[var_type]\n",
        "\n",
        "    def keys(self):\n",
        "        return self.sig_dict.keys()\n",
        "\n",
        "    def values(self):\n",
        "        return self.sig_dict.values()\n",
        "\n",
        "    def items(self):\n",
        "        return self.sig_dict.items()\n",
        "\n",
        "    def is_loadable(self):\n",
        "        return self._model_loader is not None\n",
        "\n",
        "    def is_fixed(self):\n",
        "        return all(var.fixed for var in self.sig_dict.values())\n",
        "\n",
        "    def parameterize(self, trial: optuna.Trial):\n",
        "        \"\"\"\n",
        "        ACCEPTS:\n",
        "        trial := Active optuna trial objet.\n",
        "                 IF input 'manual_action_params' is not empty, THEN 'trial' will NOT be used.\n",
        "        RETURNS: void.\n",
        "                 The function parameterizes the caller Action object.\n",
        "                 Without any saved action_state, the caller Action object needs to be parameterized every time before .__call__()\n",
        "        \"\"\"\n",
        "        #ERROR CHECKS:\n",
        "        if self.action_state is not None and not self.params:\n",
        "            raise RuntimeError(f\"Action: {self.action_func.__name__} has a saved action state(self.action_state!=None) but NO saved params(self.params==None).\")\n",
        "        if self.action_state is None and trial is None:\n",
        "            raise ValueError(f\"Action: {self.action_func.__name__} has no saved action state(self.action_state==None) and no input trial passed in(self.trial==None). No sampling can be done.\")\n",
        "\n",
        "        if self.action_state is None:\n",
        "            self.params = {cat:var.sample(trial=trial) for cat,var in self.items()}\n",
        "\n",
        "        #self._model_loader <- Fully parameterized model with self.params\n",
        "        #NOTE: IF it has a saved action state, the consistent self.params will restore the recenmost state(based on the recentmost top sampling results)\n",
        "        self._model_loader = partial(self.action_func, **self.params)\n",
        "\n",
        "    def __call__(self, device: torch.device):\n",
        "        if self.action_state is not None and not self.params:\n",
        "            raise RuntimeError(f\"Action: {self.action_func.__name__} has a saved action state(self.action_state!=None) but no saved params exists(self.params empty). Likely an internal logic issue\")\n",
        "        if self._model_loader is None:\n",
        "            raise RuntimeError(f\"Action: {self.action_func.__name__} cannot be called. Call Action.parameterize() first and then try it again.\")\n",
        "\n",
        "        #Try: Applying 'device' to the model_loader and load.\n",
        "        try:\n",
        "            model = self._model_loader(device=device)\n",
        "        except Exception as e: #IF fails: Load the model w/o device.\n",
        "            model = self._model_loader()\n",
        "\n",
        "        #IF 'self.action_state' exists, THEN load the saved weights from the state dictionary(=self.action_state) to the model\n",
        "        if self.action_state:\n",
        "            try:\n",
        "                name = self.action_func.__name__\n",
        "                model.load_state_dict(self.action_state)\n",
        "            except torch.nn.modules.module.ModuleAttributeError as e:\n",
        "                raise ValueError(f\"Attribute error: {e}. The state dictionary of Action: '{name}' may not match the model.\")\n",
        "            except RuntimeError as e:\n",
        "                raise RuntimeError(f\"Runtime error: {e}. The state dictionary of Action: '{name}' may have unmatching layer sizes or missing keys.\")\n",
        "            except Exception as e:\n",
        "                raise ValueError(f\"An unexpected error occurred while loading state dictionary to Action '{name}': {e}\")\n",
        "\n",
        "        #Last but not least, return the model.\n",
        "        return model\n",
        "\n",
        "    def update_sig_vars(self, update_dict: dict):\n",
        "        #IF Action (1)has a saved 'action_state' OR (2)0 elements in the 'list_params'\n",
        "        #THEN NO update shall be done.\n",
        "        if self.action_state or not len(update_dict):\n",
        "            return\n",
        "\n",
        "        leftovers = set(self.keys()) - set(update_dict.keys())\n",
        "        if leftovers:\n",
        "            raise RuntimeError(f\"Action: {self.action_func.__name__} requires following missing keys in 'update_dict': {leftovers}\")\n",
        "\n",
        "        #Update: signature configuration space(=='self.sig_dict')\n",
        "        for var_type,var in self.items():\n",
        "            var.update(vals=update_dict[var_type])\n",
        "\n",
        "    #Calling Action.update_state() will raise a helpful Error.\n",
        "    def update_state(self, obj):\n",
        "        return NotImplementedError(\"Erroneous call to Action.update_state()\")\n",
        "\n",
        "class ModelAction(Action):\n",
        "    def __init__(self, action_func: Callable, sig_dict: dict):\n",
        "        super().__init__(action_func=action_func, sig_dict=sig_dict)\n",
        "\n",
        "    def update_state(self, obj):\n",
        "        #Ensure that input obj has a correct type/format.\n",
        "        if not(isinstance(obj, (nn.Module, dict)) or hasattr(obj, 'model')):\n",
        "            raise ValueError(f\"Updating action state of Action: {self.action_func.__name__} failed. Input object 'obj' must either be of type in (nn.Module, dict) or must contain 'model' as an attribute but none of the two conditions are satisfied.\")\n",
        "        #Ensure that if obj.model exists, then it points to an instance of nn.Module.\n",
        "        if hasattr(obj, 'model') and not isinstance(obj.model, nn.Module):\n",
        "            raise ValueError(f\"Input obj.model must be of type nn.Module but found type(obj.model): {type(obj.model)}\")\n",
        "        #Ensure that no update is done if the model was not sampled in the previous run.\n",
        "        if self.action_state is not None and self.params != self._model_loader.keywords:\n",
        "            print(f\"ModelAction: {self.action_func.__name__} was not sampled in the previous HPO run or no HPO run has been executed yet. This call will not change the action state\")\n",
        "            return\n",
        "\n",
        "        if isinstance(obj, dict):\n",
        "            self.action_state = obj.copy() #shallow copy it for safety\n",
        "        else:\n",
        "            self.action_state = obj.state_dict() if isinstance(obj, nn.Module) else obj.model.state_dict()\n",
        "\n",
        "        #Manually set 'self.params'. This way, saved action_state -> self.params available!\n",
        "        self.params = self._model_loader.keywords.copy()"
      ],
      "metadata": {
        "id": "Y2qBB2qFHoUo"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Test: Action, ModelAction"
      ],
      "metadata": {
        "id": "57JeyleVJWz4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Parameters used in the testin of class 'ModelAction' defined in this cell\n",
        "\"\"\"\n",
        "if __name__ == '__main__':\n",
        "    class ModelDefault(nn.Module):\n",
        "        def __init__(self, rand_number: int):\n",
        "            super().__init__()\n",
        "            self.net = nn.Linear(16,10)\n",
        "            self.rand_number = torch.tensor(rand_number)\n",
        "\n",
        "        def forward(self, x):\n",
        "            return self.net(x) + self.rand_number\n",
        "\n",
        "    class ModelDropout(nn.Module):\n",
        "        def __init__(self, activation: nn.Module, dropout_p: int):\n",
        "            super().__init__()\n",
        "            self.net = nn.Sequential(\n",
        "                nn.Linear(in_features=16,out_features=10),\n",
        "                activation(),\n",
        "                nn.Dropout(p=dropout_p),\n",
        "                nn.Linear(in_features=10,out_features=10)\n",
        "            )\n",
        "\n",
        "        def forward(self, x):\n",
        "            return self.net(x)\n",
        "\n",
        "    def action_default(rand_number: int, device: torch.device):\n",
        "        return ModelDefault(rand_number=rand_number).to(device)\n",
        "\n",
        "    def action_dropout(activation, dropout_p, device: torch.device):\n",
        "        #model_dropout := Class header of 'ModelDropout'\n",
        "        return ModelDropout(activation=activation, dropout_p=dropout_p).to(device)\n",
        "\n",
        "    #ASSUMED: Input size of 10x16. (Single batch == 10 (4x4 flattened vector)s )\n",
        "    #FIRST_LAYER: Fully connected linear layer of size 16x10\n",
        "    #ACTIVATION: One of nn.ReLU, nn.PReLU, nn.SiLU\n",
        "    #DROPOUT: nn.Dropout(p=dropout_p)\n",
        "    #OUT: .shape == [10,10]\n",
        "    #NOTE: Input 'model: Callable' will be the class header 'Model'."
      ],
      "metadata": {
        "id": "WouVr4SYNPPP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TestModelAction(unittest.TestCase):\n",
        "    def setUp(self):\n",
        "        default_sig_dict = {\n",
        "            'rand_number': Var(var_type='rand_number', source={'params': {'choices': list(range(1, 11))}, 'sample': 'categorical'})\n",
        "        }\n",
        "        dropout_sig_dict = {\n",
        "            'activation': Var(var_type='activation', source={'params': {'choices': [nn.ReLU, nn.PReLU, nn.SiLU]}, 'sample': 'categorical'}),\n",
        "            'dropout_p': Var(var_type='dropout_p', source={'params': {'low': 0.0, 'high': 1.0}, 'sample': 'float'})\n",
        "        }\n",
        "\n",
        "        self.action_sampler = {\n",
        "            action_default: ModelAction(action_default, default_sig_dict),\n",
        "            action_dropout: ModelAction(action_dropout, dropout_sig_dict)\n",
        "        }\n",
        "\n",
        "        #Create a trial object for parameterization\n",
        "        self.trial = optuna.create_study(direction='minimize').ask()\n",
        "\n",
        "    def test_initialization(self):\n",
        "        default_action = self.action_sampler[action_default]\n",
        "        dropout_action = self.action_sampler[action_dropout]\n",
        "\n",
        "        #Test if ModelAction objects are initialized correctly\n",
        "        self.assertIsInstance(default_action, ModelAction)\n",
        "        self.assertIsInstance(dropout_action, ModelAction)\n",
        "\n",
        "        self.assertTrue(default_action.action_func == action_default)\n",
        "        self.assertIsInstance(default_action.get_var('rand_number'), Var)\n",
        "        self.assertTrue(\n",
        "            default_action.get_var('rand_number').distribution() == list(range(1, 11)),\n",
        "            msg=f\"{default_action.get_var('rand_number').distribution()} != {list(range(1, 11))}\"\n",
        "        )\n",
        "        self.assertFalse(default_action.params)\n",
        "        self.assertIsNone(default_action.action_state)\n",
        "        self.assertIsNone(default_action._model_loader)\n",
        "\n",
        "        self.assertTrue(dropout_action.action_func == action_dropout)\n",
        "        self.assertIsInstance(dropout_action.get_var('activation'), Var)\n",
        "        self.assertIsInstance(dropout_action.get_var('dropout_p'), Var)\n",
        "        self.assertTrue(dropout_action.get_var('activation').distribution() == [nn.ReLU, nn.PReLU, nn.SiLU])\n",
        "        self.assertFalse(dropout_action.params)\n",
        "        self.assertIsNone(dropout_action.action_state)\n",
        "        self.assertIsNone(dropout_action._model_loader)\n",
        "\n",
        "    def test_default_parameterize_with_action_state(self):\n",
        "        default_action = self.action_sampler[action_default]\n",
        "        default_action.action_state = 'dummy'\n",
        "        default_action.params = {'rand_number': 1}\n",
        "\n",
        "        #Test parameterization with the default action\n",
        "        default_action.parameterize(trial=self.trial)\n",
        "\n",
        "        #Posterior testing\n",
        "        self.assertIsInstance(default_action, ModelAction)\n",
        "        self.assertIsNotNone(default_action.params)\n",
        "        self.assertIsNotNone(default_action.action_state)\n",
        "        self.assertIsNotNone(default_action._model_loader)\n",
        "\n",
        "        try:\n",
        "            model = default_action(device=torch.device('cpu'))\n",
        "        except Exception as e:\n",
        "            pass\n",
        "\n",
        "    def test_default_parameterize(self):\n",
        "        default_action = self.action_sampler[action_default]\n",
        "        #Test parameterization with the default action\n",
        "        default_action.parameterize(trial=self.trial)\n",
        "        self.assertTrue(set(default_action.params.keys()), set(['rand_number']))\n",
        "        self.assertTrue(default_action.params['rand_number'] in list(range(1, 11)))\n",
        "        self.assertIsNone(default_action.action_state)\n",
        "        self.assertIsNotNone(default_action._model_loader)\n",
        "        model = default_action(device=torch.device('cpu'))\n",
        "        X = torch.randn(size=[10, 16])\n",
        "        Y = model(X)\n",
        "        self.assertEqual(Y.shape, torch.Size([10, 10]))\n",
        "\n",
        "        self.assertIsNotNone(default_action.params)\n",
        "        self.assertIsNone(default_action.action_state)\n",
        "        self.assertIsNotNone(default_action._model_loader)\n",
        "\n",
        "    def test_dropout_parameterize_with_action_state(self):\n",
        "        dropout_action = self.action_sampler[action_dropout]\n",
        "        dropout_action.params = {\n",
        "            'activation': nn.ReLU,\n",
        "            'dropout_p': 0.5\n",
        "        }\n",
        "        model = ModelDropout(activation=nn.ReLU, dropout_p=0.5).to(torch.device('cpu'))\n",
        "        dropout_action.action_state = model.state_dict()\n",
        "\n",
        "        #Test parameterization with the default action\n",
        "        dropout_action.parameterize(trial=self.trial)\n",
        "\n",
        "        #Posterior testing\n",
        "        self.assertIsInstance(dropout_action, ModelAction)\n",
        "        self.assertIsNotNone(dropout_action.params)\n",
        "        self.assertIsNotNone(dropout_action.action_state)\n",
        "        self.assertIsNotNone(dropout_action._model_loader)\n",
        "\n",
        "        try:\n",
        "            model = dropout_action(device=torch.device('cpu'))\n",
        "            self.assertTrue(model(torch.randn(size=[10, 16])).shape, torch.Size([10, 10]))\n",
        "        except Exception as e:\n",
        "            pass\n",
        "\n",
        "        #Final testing\n",
        "        self.assertIsNotNone(dropout_action.params)\n",
        "        self.assertIsNotNone(dropout_action.action_state)\n",
        "        self.assertIsNotNone(dropout_action._model_loader)\n",
        "\n",
        "    def test_dropout_parameterize(self):\n",
        "        dropout_action = self.action_sampler[action_dropout]\n",
        "\n",
        "        #Prior Testing\n",
        "        self.assertIsInstance(dropout_action, ModelAction)\n",
        "        self.assertTrue(dropout_action.params == {})\n",
        "        self.assertIsNone(dropout_action.action_state)\n",
        "        self.assertIsNone(dropout_action._model_loader)\n",
        "\n",
        "        #Test parameterization with the default action\n",
        "        dropout_action.parameterize(trial=self.trial)\n",
        "\n",
        "        #Posterior Testing\n",
        "        self.assertTrue(set(dropout_action.params.keys()), set(['activation', 'dropout_p']))\n",
        "        self.assertTrue(dropout_action.params['activation'] in [nn.ReLU, nn.PReLU, nn.SiLU])\n",
        "        self.assertTrue(0.0 <= dropout_action.params['dropout_p'] <= 1.0)\n",
        "        self.assertIsNone(dropout_action.action_state)\n",
        "        self.assertIsNotNone(dropout_action._model_loader)\n",
        "\n",
        "        #Model loading & testing\n",
        "        model = dropout_action(device=torch.device('cpu'))\n",
        "        self.assertEqual(model(torch.randn(size=[10, 16])).shape, torch.Size([10, 10]))\n",
        "\n",
        "        #Final testing\n",
        "        self.assertIsNotNone(dropout_action.params)\n",
        "        self.assertIsNone(dropout_action.action_state)\n",
        "        self.assertIsNotNone(dropout_action._model_loader)\n",
        "\n",
        "    def test_update_sig_vars(self):\n",
        "        #Preparation\n",
        "        default_action = self.action_sampler[action_default]\n",
        "        dropout_action = self.action_sampler[action_dropout]\n",
        "\n",
        "        default_new_sig_params = {\n",
        "            'rand_number': [2]\n",
        "        }\n",
        "        dropout_new_sig_params = {\n",
        "            'activation': [1, 2],\n",
        "            'dropout_p': [0.45, 0.5, 0.72]\n",
        "        }\n",
        "\n",
        "        #TEST: default_action FOR: sig_dict\n",
        "        default_action.update_sig_vars(update_dict=default_new_sig_params)\n",
        "        self.assertIsInstance(default_action.get_var('rand_number'), Var)\n",
        "        self.assertTrue(default_action.get_var('rand_number').distribution() == [3])\n",
        "        self.assertTrue(default_action.get_var('rand_number').distribution(indices=True) == [2])\n",
        "\n",
        "        #TEST: dropout_action FOR: sig_dict\n",
        "        dropout_action.update_sig_vars(update_dict=dropout_new_sig_params)\n",
        "        self.assertIsInstance(dropout_action.get_var('activation'), Var)\n",
        "        self.assertTrue(dropout_action.get_var('activation').distribution() == [nn.PReLU, nn.SiLU],\n",
        "                        msg=f\"{dropout_action.get_var('activation').distribution()} != {[nn.PReLU, nn.SiLU]}\")\n",
        "        self.assertTrue(dropout_action.get_var('activation').distribution(True) == [1, 2],\n",
        "                        msg=f\"{dropout_action.get_var('activation').distribution(True)} != {[1, 2]}\")\n",
        "        self.assertIsInstance(dropout_action.get_var('dropout_p'), Var)\n",
        "        dropout_p_var = dropout_action.get_var('dropout_p')\n",
        "        self.assertTrue(dropout_p_var.distribution()['low'] == 0.45 and dropout_p_var.distribution()['high'] == 0.72)\n",
        "\n",
        "    #Additional method to test update_state if needed\n",
        "    def test_update_state(self):\n",
        "        for i in range(5):\n",
        "            default_action = self.action_sampler[action_default]\n",
        "            dropout_action = self.action_sampler[action_dropout]\n",
        "\n",
        "            if not i:\n",
        "                self.assertTrue(default_action.params == {})\n",
        "                self.assertIsNone(default_action.action_state)\n",
        "                self.assertIsNone(default_action._model_loader)\n",
        "\n",
        "                self.assertTrue(dropout_action.params == {})\n",
        "                self.assertIsNone(dropout_action.action_state)\n",
        "                self.assertIsNone(dropout_action._model_loader)\n",
        "            else:\n",
        "                self.assertFalse(default_action.params == {})\n",
        "                self.assertIsNotNone(default_action.action_state)\n",
        "                self.assertIsNotNone(default_action._model_loader)\n",
        "\n",
        "                self.assertFalse(dropout_action.params == {})\n",
        "                self.assertIsNotNone(dropout_action.action_state)\n",
        "                self.assertIsNotNone(dropout_action._model_loader)\n",
        "\n",
        "            default_action.parameterize(trial=self.trial)\n",
        "            dropout_action.parameterize(trial=self.trial)\n",
        "\n",
        "            default_model = default_action(device=torch.device('cpu'))\n",
        "            dropout_model = dropout_action(device=torch.device('cpu'))\n",
        "\n",
        "            self.assertEqual(default_model(torch.randn(size=[10, 16])).shape, torch.Size([10, 10]))\n",
        "            self.assertEqual(dropout_model(torch.randn(size=[10, 16])).shape, torch.Size([10, 10]))\n",
        "\n",
        "            default_action.update_state(obj=default_model)\n",
        "            dropout_action.update_state(obj=dropout_model)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    unittest.main(argv=['first-arg-is-ignored'], exit=False)"
      ],
      "metadata": {
        "id": "aJ_OCQ2DzSyX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Class ActionVar"
      ],
      "metadata": {
        "id": "spoZ-zvLuqrh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ActionVar(Var):\n",
        "    \"\"\"\n",
        "    ActionVar is a subclass of Var that stores Action/ModelAction objects in its `_frozen_dist`.\n",
        "    This enables automated sampling of actions and their parameters during HPO.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, var_type: str, source: dict, belongs_to: str=None):\n",
        "        super().__init__(var_type=var_type, source=source, belongs_to=belongs_to)\n",
        "\n",
        "    def is_fixed(self):\n",
        "        \"\"\"\n",
        "        ActionVar is fixed if:\n",
        "        - Only one sampleable value in the configuration space.\n",
        "        - All signature variables of the ActionVar are fixed.\n",
        "        \"\"\"\n",
        "        if len(self._dist) > 1:\n",
        "            return False\n",
        "        singleton_action = self._frozen_dist[self._dist[0]]\n",
        "        return all(var.is_fixed() for var in singleton_action.values())\n",
        "\n",
        "    def get_action(self, action_idx_or_action_func):\n",
        "        if isinstance(action_idx_or_action_func, int):\n",
        "            return self.convert_idx_to_val(action_idx_or_action_func)\n",
        "        action_list = [action for action in self._frozen_dist if action.action_func == action_idx_or_action_func]\n",
        "        if not action_list:\n",
        "            raise KeyError(f\"No action found with action_func: {action_idx_or_action_func} in ActionVar: {self.var_type}.\")\n",
        "        return action_list[0]\n",
        "\n",
        "    def get_var(self, action_idx_or_action_func, var_type):\n",
        "        action = self.get_action(action_idx_or_action_func)\n",
        "        return action.get_var(var_type)\n",
        "\n",
        "    def get_loadable_action(self, action_identifier, source) -> Action:\n",
        "        \"\"\"\n",
        "        Retrieves a fully parameterized Action object.\n",
        "\n",
        "        Args:\n",
        "            action_identifier: An instance of Action, a Callable action function, or an integer index.\n",
        "            source: A dictionary or optuna Trial/FrozenTrial containing parameter mappings.\n",
        "\n",
        "        Returns:\n",
        "            A fully parameterized Action object.\n",
        "        \"\"\"\n",
        "        if not isinstance(source, (Trial, FrozenTrial, dict)):\n",
        "            raise ValueError(f\"Input 'source' must be of type dict, Trial, or FrozenTrial, but found: {type(source)}\")\n",
        "        if not (isinstance(action_identifier, (int, Action)) or callable(action_identifier)):\n",
        "            raise TypeError(f\"Input 'action_identifier' should be an int, Action, or Callable, but found: {type(action_identifier)}\")\n",
        "\n",
        "        #Get the action based on the identifier\n",
        "        if isinstance(action_identifier, Action):\n",
        "            action = action_identifier\n",
        "        else:\n",
        "            action = self.get_action(action_identifier)\n",
        "\n",
        "        #If the action has a saved state, return it\n",
        "        if action.action_state:\n",
        "            return action\n",
        "\n",
        "        #Extract parameters from the source\n",
        "        source = source if isinstance(source, dict) else source.params\n",
        "        params = {\n",
        "            var_type: val for (var_type, belongs_to), val in source.items()\n",
        "            if var_type in action.keys() and belongs_to == self.var_type\n",
        "        }\n",
        "\n",
        "        #Check for missing parameters\n",
        "        missing_params = set(action.keys()) - set(params.keys())\n",
        "        if missing_params:\n",
        "            raise KeyError(f\"Action '{action.action_func.__name__}' requires missing keys in 'source': {missing_params}. \\nNOTE: 'source': {source}, 'params': {params}\")\n",
        "\n",
        "        #Convert indices to actual values\n",
        "        for var_type, var in action.items():\n",
        "            val = params[var_type]\n",
        "            if var.contain_idx(val):\n",
        "                val = var.convert_idx_to_val(val)\n",
        "            params[var_type] = val\n",
        "\n",
        "        #Update the action's model loader\n",
        "        action._model_loader = partial(action.action_func, **params)\n",
        "\n",
        "        return action\n",
        "\n",
        "    def update_actions(self, update_action_idxs: list, list_params: list, update_state_window: int, device: torch.device):\n",
        "        \"\"\"\n",
        "        Updates actions based on recent sampling results.\n",
        "\n",
        "        Args:\n",
        "            update_action_idxs: List of unique indices representing actions to update.\n",
        "            list_params: List of parameter dictionaries from recent optuna trials.\n",
        "                         Every 'params' in the list_params is guaranteed to have (self.var_type,'None') as its key and therefore,\n",
        "                         will have all the (self.keys(),self.var_type) as keys as well.\n",
        "            update_state_window: Number of top actions to update states for.\n",
        "            device: Torch device to use for model actions.\n",
        "        \"\"\"\n",
        "        update_state_window = min(update_state_window, len(update_action_idxs))\n",
        "        has_meaningful_state = isinstance(self.distribution()[0], ModelAction)\n",
        "        for count,idx in enumerate(update_action_idxs):\n",
        "            action = self.get_action(action_idx_or_action_func=idx)\n",
        "\n",
        "            #Update signature vars of the retrieved action.\n",
        "            update_dict = {\n",
        "                var_type:[params[(var_type, self.var_type)] for params in list_params if (var_type, self.var_type) in params.keys()] for var_type in action.keys()\n",
        "            }\n",
        "            action.update_sig_vars(update_dict=update_dict)\n",
        "\n",
        "            if not has_meaningful_state or count >= update_state_window:\n",
        "                continue\n",
        "\n",
        "            #Ensure that there's at least one params in 'list_params' that sampled idx.\n",
        "            matching_params =  [params for params in list_params if params[(self.var_type,'None')]==idx]\n",
        "            if not matching_params:\n",
        "                raise ValueError(f\"index: {idx} in 'update_action_idxs' does NOT have any matching params in 'list_params'. Make sure to pass in 'list_params' with at least one params that contain: ({self.var_type},'None')->{idx}\")\n",
        "\n",
        "            #Find the best parameters for this action\n",
        "            best_params = matching_params[0]\n",
        "\n",
        "            #Get a fully loadable action\n",
        "            action = self.get_loadable_action(idx,best_params)\n",
        "\n",
        "            #Prepare parameters for objective execution\n",
        "            default_dict = {var_type:val for (var_type,belongs_to),val in best_params.items() if belongs_to == 'None'}\n",
        "            default_dict[self.var_type] = action\n",
        "\n",
        "            #Prepare and execute Objective.\n",
        "            _, model_container = Objective(\n",
        "                trial=None,\n",
        "                combined_sampler=None,\n",
        "                device=device,\n",
        "                default_dict=default_dict\n",
        "            ).execute_objective()\n",
        "\n",
        "            #Update the action's state with the model_container.\n",
        "            action.update_state(obj=model_container)"
      ],
      "metadata": {
        "id": "eSIGyvrzupPs"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Test: ActionVar"
      ],
      "metadata": {
        "id": "DMIIq0XTvgzr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Assuming Var, Action, ModelAction, and ActionVar are defined as per the improved code.\n",
        "\n",
        "#Fake Objective class\n",
        "class Objective:\n",
        "    def __init__(self, trial, combined_sampler, device, default_dict):\n",
        "        self.trial = trial\n",
        "        self.combined_sampler = combined_sampler\n",
        "        self.device = device\n",
        "        self.default_dict = default_dict\n",
        "\n",
        "    def execute_objective(self):\n",
        "        model = self.default_dict['model_action'](device=self.device)\n",
        "\n",
        "        for param in model.parameters():\n",
        "            param.data.fill_(0)\n",
        "\n",
        "        model_container = Mock(spec=Learner)\n",
        "        model_container.model = model\n",
        "\n",
        "        return (0, model)\n",
        "\n",
        "#Sample model classes\n",
        "class ModelDefault(nn.Module):\n",
        "    def __init__(self, rand_number: int):\n",
        "        super().__init__()\n",
        "        self.net = nn.Linear(16, 10)\n",
        "        self.rand_number = torch.tensor(rand_number)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x) + self.rand_number\n",
        "\n",
        "class ModelDropout(nn.Module):\n",
        "    def __init__(self, activation: nn.Module, dropout_p: float):\n",
        "        super().__init__()\n",
        "        self.activation = activation\n",
        "        self.dropout_p = dropout_p\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(16, 10),\n",
        "            activation(),\n",
        "            nn.Dropout(p=dropout_p),\n",
        "            nn.Linear(10, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "#model_action functions\n",
        "def model_action_default(rand_number: int, device: torch.device):\n",
        "    return ModelDefault(rand_number=rand_number).to(device)\n",
        "\n",
        "def model_action_dropout(activation, dropout_p, device: torch.device):\n",
        "    return ModelDropout(activation=activation, dropout_p=dropout_p).to(device)\n",
        "\n",
        "#dls_action functions\n",
        "def dls_action_default(batch_size: int, device: torch.device):\n",
        "    return {'batch_size': batch_size, 'device': device}\n",
        "\n",
        "def dls_action_normalize(batch_size: int, normalize: bool):\n",
        "    return {'batch_size': batch_size, 'normalize': normalize}\n",
        "\n",
        "def dls_action_activation(batch_size: int, normalize: bool, activation: nn.Module):\n",
        "    return {'batch_size': batch_size, 'normalize': normalize, 'activation': activation}\n",
        "\n",
        "class TestActionVar(unittest.TestCase):\n",
        "    def setUp(self):\n",
        "        #Define Var objects for parameters of 'model_actions'.\n",
        "        rand_number_var = Var(\n",
        "            var_type='rand_number',\n",
        "            source={'params': {'choices': list(range(1, 11))}, 'sample': 'categorical'},\n",
        "            belongs_to='model_action'\n",
        "        )\n",
        "        activation_var = Var(\n",
        "            var_type='activation',\n",
        "            source={'params': {'choices': [nn.ReLU, nn.PReLU, nn.SiLU]}, 'sample': 'categorical'},\n",
        "            belongs_to='model_action'\n",
        "        )\n",
        "        dropout_p_var = Var(\n",
        "            var_type='dropout_p',\n",
        "            source={'params': {'low': 0.0, 'high': 1.0}, 'sample': 'float'},\n",
        "            belongs_to='model_action'\n",
        "        )\n",
        "        #Define Var objects for parameters of 'dls_actions'.\n",
        "        batch_size_var_default = Var(\n",
        "            var_type='batch_size',\n",
        "            source={'params': {'low': 16, 'high': 64}, 'sample': 'int'},\n",
        "            belongs_to='dls_action'\n",
        "        )\n",
        "        batch_size_var_normalize = Var(\n",
        "            var_type='batch_size',\n",
        "            source={'params': {'low': 16, 'high': 64}, 'sample': 'int'},\n",
        "            belongs_to='dls_action'\n",
        "        )\n",
        "        batch_size_var_activation = Var(\n",
        "            var_type='batch_size',\n",
        "            source={'params': {'low': 16, 'high': 64}, 'sample': 'int'},\n",
        "            belongs_to='dls_action'\n",
        "        )\n",
        "        normalize_var_normalize = Var(\n",
        "            var_type='normalize',\n",
        "            source={'params': {'choices': [True, False]}, 'sample': 'categorical'},\n",
        "            belongs_to='dls_action'\n",
        "        )\n",
        "        normalize_var_activation = Var(\n",
        "            var_type='normalize',\n",
        "            source={'params': {'choices': [True, False]}, 'sample': 'categorical'},\n",
        "            belongs_to='dls_action'\n",
        "        )\n",
        "        activation_var_activation = Var(\n",
        "            var_type='activation',\n",
        "            source={'params': {'choices': [nn.ReLU, nn.SiLU]}, 'sample': 'categorical'},\n",
        "            belongs_to='dls_action'\n",
        "        )\n",
        "\n",
        "        #Create ModelAction objects\n",
        "        self.action_default = ModelAction(\n",
        "            action_func=model_action_default,\n",
        "            sig_dict={'rand_number': rand_number_var}\n",
        "        )\n",
        "        self.action_dropout = ModelAction(\n",
        "            action_func=model_action_dropout,\n",
        "            sig_dict={'activation': activation_var, 'dropout_p': dropout_p_var}\n",
        "        )\n",
        "        #Create an ActionVar ModelAction(s).\n",
        "        self.model_action_var = ActionVar(\n",
        "            var_type='model_action',\n",
        "            source={\n",
        "                'params': {'choices': [self.action_default, self.action_dropout]},\n",
        "                'sample': 'categorical'\n",
        "            }\n",
        "        )\n",
        "\n",
        "        #Create Action objects for non-stateful actions\n",
        "        self.dls_action_default = Action(\n",
        "            action_func=dls_action_default,\n",
        "            sig_dict={'batch_size': batch_size_var_default}\n",
        "        )\n",
        "        self.dls_action_normalize = Action(\n",
        "            action_func=dls_action_normalize,\n",
        "            sig_dict={'batch_size': batch_size_var_normalize, 'normalize': normalize_var_normalize}\n",
        "        )\n",
        "        self.dls_action_activation = Action(\n",
        "            action_func=dls_action_activation,\n",
        "            sig_dict={'batch_size': batch_size_var_activation, 'normalize': normalize_var_activation, 'activation': activation_var_activation}\n",
        "        )\n",
        "        #Create an ActionVar for these Actions\n",
        "        self.dls_action_var = ActionVar(\n",
        "            var_type='dls_action',\n",
        "            source={\n",
        "                'params': {'choices': [self.dls_action_default, self.dls_action_normalize, self.dls_action_activation]},\n",
        "                'sample': 'categorical'\n",
        "            }\n",
        "        )\n",
        "\n",
        "    def test_model_action_var_init(self):\n",
        "        #Test initialization of the model_action_var\n",
        "        self.assertEqual(self.model_action_var.var_type, 'model_action')\n",
        "        self.assertEqual(self.model_action_var.sample_method, 'categorical')\n",
        "        self.assertEqual(len(self.model_action_var._frozen_dist), 2)\n",
        "        self.assertIsInstance(self.model_action_var._frozen_dist[0], ModelAction)\n",
        "        self.assertIsInstance(self.model_action_var._frozen_dist[1], ModelAction)\n",
        "\n",
        "    def test_dls_action_var_init(self):\n",
        "        #Test initialization of the dls_action_var\n",
        "        self.assertEqual(self.dls_action_var.var_type, 'dls_action')\n",
        "        self.assertEqual(self.dls_action_var.sample_method, 'categorical')\n",
        "        self.assertEqual(len(self.dls_action_var._frozen_dist), 3)\n",
        "        self.assertIsInstance(self.dls_action_var._frozen_dist[0], Action)\n",
        "        self.assertIsInstance(self.dls_action_var._frozen_dist[1], Action)\n",
        "        self.assertIsInstance(self.dls_action_var._frozen_dist[2], Action)\n",
        "\n",
        "    def test_model_is_fixed(self):\n",
        "        #Initially, the ActionVar is not fixed\n",
        "        self.assertFalse(self.model_action_var.is_fixed())\n",
        "\n",
        "        #Fix all variables in the first action\n",
        "        for var in self.model_action_var._frozen_dist[0].values():\n",
        "            var._dist = [0]\n",
        "\n",
        "        #Update the distribution to only include the first action\n",
        "        self.model_action_var._dist = [0]\n",
        "        self.assertTrue(self.model_action_var.is_fixed())\n",
        "\n",
        "    def test_dls_is_fixed(self):\n",
        "        #Initially, the ActionVar is not fixed\n",
        "        self.assertFalse(self.dls_action_var.is_fixed())\n",
        "\n",
        "        #Fix all variables in the first action\n",
        "        for var in self.dls_action_var._frozen_dist[0].values():\n",
        "            if var.sample_method == 'categorical':\n",
        "                var._dist = [0]\n",
        "            else:\n",
        "                var._dist['low'] = var._dist['high']\n",
        "\n",
        "        #Update the distribution to only include the first action\n",
        "        self.dls_action_var._dist = [0]\n",
        "        try:\n",
        "            self.assertTrue(self.dls_action_var.is_fixed())\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Distribution: [{self.dls_action_var.distribution()[0].action_func}], \")\n",
        "\n",
        "    def test_model_get_action(self):\n",
        "        #Get action by index\n",
        "        action = self.model_action_var.get_action(0)\n",
        "        self.assertEqual(action, self.action_default)\n",
        "\n",
        "        #Get action by function\n",
        "        action = self.model_action_var.get_action(model_action_dropout)\n",
        "        self.assertEqual(action, self.action_dropout)\n",
        "\n",
        "        #Test for KeyError when action not found\n",
        "        with self.assertRaises(KeyError):\n",
        "            self.model_action_var.get_action(lambda x: x)\n",
        "        with self.assertRaises(KeyError):\n",
        "            self.model_action_var.get_action(2)\n",
        "\n",
        "    def test_dls_get_action(self):\n",
        "        #Get action by index\n",
        "        action = self.dls_action_var.get_action(0)\n",
        "        self.assertEqual(action, self.dls_action_default)\n",
        "\n",
        "        #Get action by function\n",
        "        action = self.dls_action_var.get_action(dls_action_normalize)\n",
        "        self.assertEqual(action, self.dls_action_normalize)\n",
        "\n",
        "        #Test for KeyError when action not found\n",
        "        with self.assertRaises(KeyError):\n",
        "            self.dls_action_var.get_action(lambda x: x)\n",
        "\n",
        "    def test_model_get_var(self):\n",
        "        #Retrieve 'rand_number'\n",
        "        var = self.model_action_var.get_var(0, 'rand_number')\n",
        "        self.assertEqual(var.var_type, 'rand_number')\n",
        "        var = self.model_action_var.get_var(model_action_default, 'rand_number')\n",
        "        self.assertEqual(var.var_type, 'rand_number')\n",
        "\n",
        "        #Retrieve 'dropout_p'\n",
        "        var = self.model_action_var.get_var(1, 'dropout_p')\n",
        "        self.assertEqual(var.var_type, 'dropout_p')\n",
        "        var = self.model_action_var.get_var(model_action_dropout, 'dropout_p')\n",
        "        self.assertEqual(var.var_type, 'dropout_p')\n",
        "\n",
        "        #Retrieve 'activation'\n",
        "        var = self.model_action_var.get_var(1, 'activation')\n",
        "        self.assertEqual(var.var_type, 'activation')\n",
        "        var = self.model_action_var.get_var(model_action_dropout, 'activation')\n",
        "        self.assertEqual(var.var_type, 'activation')\n",
        "\n",
        "    def test_dls_get_var(self):\n",
        "        #Retrieve 'batch_size' from all actions\n",
        "        for idx in range(3):\n",
        "            var = self.dls_action_var.get_var(idx, 'batch_size')\n",
        "            self.assertEqual(var.var_type, 'batch_size')\n",
        "\n",
        "        #'normalize' exists in actions at dls_action_normalize, dls_action_dropout.\n",
        "        var = self.dls_action_var.get_var(1, 'normalize')\n",
        "        self.assertEqual(var.var_type, 'normalize')\n",
        "\n",
        "        var = self.dls_action_var.get_var(2, 'normalize')\n",
        "        self.assertEqual(var.var_type, 'normalize')\n",
        "\n",
        "        #'activation' exists only in action at dls_action_activation.\n",
        "        var = self.dls_action_var.get_var(2, 'activation')\n",
        "        self.assertEqual(var.var_type, 'activation')\n",
        "\n",
        "        #Test KeyError for specification of wrong sig_var.\n",
        "        with self.assertRaises(KeyError):\n",
        "            self.dls_action_var.get_var(0, 'normalize')\n",
        "        with self.assertRaises(KeyError):\n",
        "            self.dls_action_var.get_var(1, 'activation')\n",
        "\n",
        "    def test_get_loadable_action_model_action_var(self):\n",
        "        #Create a FrozenTrial with fixed parameters\n",
        "        frozen_trial = optuna.trial.FrozenTrial(\n",
        "            number=0,  #You can set any trial number.\n",
        "            trial_id=0,  #You can set any trial id.\n",
        "            value=None,  #Objective value (set if you have an objective function)\n",
        "            datetime_start=None,  # Start time of the trial\n",
        "            datetime_complete=None,  # Completion time of the trial\n",
        "            params={\n",
        "                ('rand_number', 'model_action'): 5,  # Index 5 <-> Real value 6.\n",
        "                ('activation', 'model_action'): 0,   # Index of nn.ReLU\n",
        "                ('dropout_p', 'model_action'): 0.5,\n",
        "                ('model_action', 'None'): 1,          # Index of action_dropout\n",
        "                ('batch_size', 'dls_action'): 1,\n",
        "                ('normalize', 'dls_action'): 0,\n",
        "                ('activation', 'dls_action'): 1\n",
        "            },\n",
        "            distributions={\n",
        "                ('rand_number', 'model_action'): optuna.distributions.CategoricalDistribution(choices=list(range(1, 11))),\n",
        "                ('activation', 'model_action'): optuna.distributions.CategoricalDistribution(choices=[0, 1, 2]),\n",
        "                ('dropout_p', 'model_action'): optuna.distributions.FloatDistribution(low=0.0, high=1.0),\n",
        "                ('model_action', 'None'): optuna.distributions.CategoricalDistribution(choices=[0, 1]),\n",
        "                ('batch_size', 'dls_action'): optuna.distributions.CategoricalDistribution(choices=[0, 1]),\n",
        "                ('normalize', 'dls_action'): optuna.distributions.CategoricalDistribution(choices=[0, 1]),\n",
        "                ('activation', 'dls_action'): optuna.distributions.CategoricalDistribution(choices=[0, 1])\n",
        "            },\n",
        "            user_attrs={},  # Optional: any user-defined attributes\n",
        "            system_attrs={},  # Optional: system-specific attributes\n",
        "            intermediate_values={},  # Optional: store intermediate objective values\n",
        "            state=optuna.trial.TrialState.COMPLETE,  # State of the trial\n",
        "            values=None,  # Optional: list of objective values for multi-objective optimization\n",
        "        )\n",
        "\n",
        "        #Test action(model_action_default) retrieved from .get_loadable_action\n",
        "        action = self.model_action_var.get_loadable_action(0, frozen_trial)\n",
        "        self.assertIsNotNone(action._model_loader)\n",
        "        self.assertEqual(action._model_loader.keywords['rand_number'], 6)\n",
        "        action_two = self.model_action_var.get_loadable_action(model_action_default, frozen_trial)\n",
        "        self.assertEqual(action, action_two)\n",
        "        action_three = self.model_action_var.get_loadable_action(self.model_action_var.get_action(0), frozen_trial)\n",
        "        self.assertEqual(action, action_three)\n",
        "        #Test model created by action(model_action_default)\n",
        "        model = action(device=None)\n",
        "        self.assertIsInstance(model, nn.Module)\n",
        "        self.assertEqual(model.rand_number.item(), 6)\n",
        "        Y = model(torch.randn(32, 16))\n",
        "        self.assertEqual(Y.shape, (32, 10))\n",
        "\n",
        "        #Test action(model_action_dropout) retrieved from .get_loadable_action\n",
        "        action = self.model_action_var.get_loadable_action(1, frozen_trial)\n",
        "        self.assertIsNotNone(action._model_loader)\n",
        "        self.assertEqual(action._model_loader.keywords['activation'], nn.ReLU)\n",
        "        self.assertEqual(action._model_loader.keywords['dropout_p'], 0.5)\n",
        "        action_two = self.model_action_var.get_loadable_action(model_action_dropout, frozen_trial)\n",
        "        self.assertEqual(action, action_two)\n",
        "        action_three = self.model_action_var.get_loadable_action(self.model_action_var.get_action(1), frozen_trial)\n",
        "        self.assertEqual(action, action_three)\n",
        "\n",
        "        #Test model created by action(model_action_dropout)\n",
        "        model = action(device=None)\n",
        "        self.assertIsInstance(model, nn.Module)\n",
        "        self.assertEqual(model.activation, nn.ReLU)\n",
        "        self.assertEqual(model.dropout_p, 0.5)\n",
        "        Y = model(torch.randn(32, 16))\n",
        "        self.assertEqual(Y.shape, (32, 10))\n",
        "\n",
        "    def test_update_actions_model_action_var(self):\n",
        "        #Prepare list_params\n",
        "        list_params = [\n",
        "            #rand_number: 4\n",
        "            {('model_action', 'None'): 0, ('rand_number', 'model_action'): 3},\n",
        "            #activation: nn.SiLU, dropout_p: 0.25.\n",
        "            {('model_action', 'None'): 1, ('activation', 'model_action'): 2, ('dropout_p', 'model_action'): 0.25},\n",
        "            #rand_number: 1\n",
        "            {('model_action', 'None'): 0, ('rand_number', 'model_action'): 0},\n",
        "            #rand_number: 1\n",
        "            {('model_action', 'None'): 0, ('rand_number', 'model_action'): 0},\n",
        "            #activation: nn.PReLU, dropout_p: 0.112.\n",
        "            {('model_action', 'None'): 1, ('activation', 'model_action'): 1, ('dropout_p', 'model_action'): 0.112},\n",
        "            #activation: nn.ReLU, dropout_p: 0.998.\n",
        "            {('model_action', 'None'): 1, ('activation', 'model_action'): 0, ('dropout_p', 'model_action'): 0.998},\n",
        "        ]\n",
        "\n",
        "        for i in range(3):\n",
        "            #Update actions\n",
        "            self.model_action_var.update_actions([0, 1], list_params, update_state_window=i, device=torch.device('cpu'))\n",
        "\n",
        "            #Verify updated configuration spaces.\n",
        "            self.assertEqual(self.model_action_var.distribution(), [self.action_default, self.action_dropout])\n",
        "            self.assertEqual(self.model_action_var.get_var(0, 'rand_number').distribution(), [1,1,4])\n",
        "            self.assertEqual(self.model_action_var.get_var(1, 'activation').distribution(), [nn.ReLU,nn.PReLU,nn.SiLU])\n",
        "            self.assertEqual(self.model_action_var.get_var(1, 'dropout_p').distribution(), {'low':0.112,'high':0.998})\n",
        "\n",
        "            #action_default <- Action(model_action_default)\n",
        "            action_default = self.model_action_var.get_action(0)\n",
        "            #action_dropout <- Action(model_action_dropout)\n",
        "            action_dropout = self.model_action_var.get_action(1)\n",
        "\n",
        "            #Test the saved action states\n",
        "            if i==0:\n",
        "                self.assertIsNone(action_default.action_state)\n",
        "                self.assertIsNone(action_dropout.action_state)\n",
        "            elif i==1:\n",
        "                self.assertIsNotNone(action_default.action_state)\n",
        "                self.assertIsNone(action_dropout.action_state)\n",
        "            else:\n",
        "                self.assertIsNotNone(action_default.action_state)\n",
        "                self.assertIsNotNone(action_dropout.action_state)\n",
        "            action_default.action_state = None\n",
        "            action_dropout.action_state = None\n",
        "\n",
        "    def test_error_handling_in_get_loadable_action(self):\n",
        "        #Test with missing parameters\n",
        "        params = {('model_action', 'None'): 0}\n",
        "        with self.assertRaises(KeyError):\n",
        "            self.model_action_var.get_loadable_action(0, params)\n",
        "\n",
        "    def test_error_handling_in_update_actions(self):\n",
        "        #Prepare list_params with missing keys\n",
        "        list_params = [\n",
        "            {('model_action', 'None'): 1, ('dropout_p', 'model_action'): 0.112}\n",
        "        ]\n",
        "        #Should raise KeyError in .get_action() part of called INSIDE the .update_actions()\n",
        "        with self.assertRaises(KeyError):\n",
        "            self.model_action_var.update_actions([1], list_params, update_state_window=10, device=torch.device('cpu'))\n",
        "\n",
        "        #Prepare list_params that only sampled model_action->action_dropout.\n",
        "        list_params = [\n",
        "            {('model_action', 'None'): 1, ('dropout_p', 'model_action'): 0.112, ('activation', 'model_action'): 0},\n",
        "            {('model_action', 'None'): 1, ('dropout_p', 'model_action'): 1.0, ('activation', 'model_action'): 2},\n",
        "        ]\n",
        "        #Pass an unmatching update_indices == [0,1]. This should raise a ValueError.\n",
        "        with self.assertRaises(ValueError):\n",
        "            self.model_action_var.update_actions([0,1], list_params, update_state_window=10, device=torch.device('cpu'))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    unittest.main(argv=['first-arg-is-ignored'], exit=False)"
      ],
      "metadata": {
        "id": "BGjeA-HXvl8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Class: SampleCallVisitor, ConfigSpace, UserConfigSpace"
      ],
      "metadata": {
        "id": "iMbz8w6ITEZn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "#Define a custom AST NodeVisitor to collect var_types\n",
        "class SampleCallVisitor(ast.NodeVisitor):\n",
        "    def __init__(self):\n",
        "        self.user_requests = set()\n",
        "        self.user_requests_with_defaults = set()\n",
        "\n",
        "    def visit_Call(self, node):\n",
        "        if isinstance(node.func, ast.Attribute) and node.func.attr == 'sample' and \\\n",
        "           isinstance(node.func.value, ast.Attribute) and node.func.value.attr == 'self':\n",
        "            target = node.func.value.value\n",
        "            msg = target.id if isinstance(target, ast.Name) else target.attr\n",
        "            raise ValueError(f\"Following code sippet '...{target}.self.sample' is not allowed. 'self' keyword cannot be used for attribute access\")\n",
        "\n",
        "        if isinstance(node.func, ast.Attribute) and node.func.attr == 'sample' and \\\n",
        "           isinstance(node.func.value, ast.Name) and node.func.value.id == 'self':\n",
        "            if not node.args:\n",
        "                raise ValueError(f\"Call self.sample with no argument detected. self.sample must recieve a string argument 'var_type' and an optional literal argument 'default_val'.\")\n",
        "            if len(node.args) > 2:\n",
        "                raise ValueError(f\"Call self.sample with more than 2 arguments detected. self.sample must recieve a string argument 'var_type' and an optional literal argument 'default_val'.\")\n",
        "\n",
        "            var_type_node, default_val = node.args[0], None if len(node.args)==1 else node.args[1]\n",
        "            if not isinstance(var_type_node, ast.Constant) or not isinstance(var_type_node.value, str):\n",
        "                raise ValueError(f\"Call self.sample with non-string literal argument: '{var_type_node}' detected. self.sample must recieve a string argument 'var_type' and an optional literal argument 'default_val'.\")\n",
        "\n",
        "            #We only check the accuracy of the node IFF 'default_val' is None or literal(ast.Constant)\n",
        "            #The rest of the cases will be checked during runtime.\n",
        "            if default_val is None or isinstance(default_val, ast.Constant):\n",
        "                default_val = default_val.value if default_val else None\n",
        "                cls = self.user_requests if default_val is None else self.user_requests_with_defaults\n",
        "                cls.add((var_type_node.value, default_val))\n",
        "\n",
        "        #Continue walking the tree\n",
        "        self.generic_visit(node)"
      ],
      "metadata": {
        "id": "pjYtD13CfpfM"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConfigSpace(ABC):\n",
        "    \"\"\"\n",
        "    ConfigSpace is the user interface for building a properly-formatted configuration space, to be used by the AutoHPO process.\n",
        "    (1)Allows the user to add,modify,retrieve configuration hyperparameters(whose syntax is identical to that of optuna.sample*)\n",
        "    (2)Allows the user to retrieve Var/ActionVar object(s).\n",
        "    (3)Requires the user to subclass it AND order to overwrite 'objective' in the process.\n",
        "       Additionally, it provides .verify_on_objective() the alignment between configuration space AND the objective function.\n",
        "    \"\"\"\n",
        "    def __init__(self, init_config=None):\n",
        "        \"\"\"\n",
        "        Initialize ConfigSpace.\n",
        "\n",
        "        Parameters:\n",
        "        config_space(optional): Another ConfigSpace\n",
        "        \"\"\"\n",
        "        if init_config is not None and not isinstance(init_config, (dict, ConfigSpace)):\n",
        "            raise ValueError(f\"Input 'init_config' must either be None or of type in (dict, ConfigSpace) but found type(init_config): {type(init_config)}\")\n",
        "\n",
        "        self._config = {}\n",
        "        if isinstance(init_config, ConfigSpace):\n",
        "            self._config = init_config._config.copy()\n",
        "            #Inherit the objective fuction as well.\n",
        "            setattr(self, 'objective', init_config.objective.__func__)\n",
        "            self.verify_syntax(warn_missing_sig_var_types=True)\n",
        "            return\n",
        "        #IF init_config is a dictionary, then shallow-copy it.\n",
        "        init_config = {} if init_config is None else init_config.copy()\n",
        "\n",
        "        #Now, we're guaranteed that 'init_config' is a dictionary, and self._config = {} for now.\n",
        "        #(0)Tools for construction\n",
        "        init_keys = set(init_config.keys())\n",
        "        action_keys = {key for key in init_config.keys() if key[0].endswith('--action')}\n",
        "        non_action_keys = {key for key in init_keys - action_keys if key[1] is None}\n",
        "        sig_keys = init_keys - action_keys - non_action_keys\n",
        "\n",
        "        #(1)Verify the syntax/errors in the dictionary 'init_config'\n",
        "        invalid_action_keys = {key for key in init_keys if '--action' in key[0] and key not in action_keys}\n",
        "        if invalid_action_keys:\n",
        "            raise ValueError(f\"'--action' suffix can only be appended at the end of each var_type to signify that it's an action parameter but wrongly formatted found: {invalid_action_keys}\")\n",
        "        action_var_types = {var_type[:-len('--action')] for var_type, _ in action_keys}\n",
        "        non_action_var_types = {var_type for var_type, _ in non_action_keys}\n",
        "        overlap_var_types = action_var_types & non_action_var_types\n",
        "        if overlap_var_types:\n",
        "            overlap_keys = filter(\n",
        "                lambda key: (key[0] in overlap_var_types) or (key[0][:-len('--action')] in overlap_var_types),\n",
        "                action_keys | non_action_keys\n",
        "            )\n",
        "            raise ValueError(f\"Non-action hyperparameters and action hyperparameters cannot share the same var_type but overlap found: {list(overlap_keys)}\")\n",
        "        invalid_sample_methods = {var_type:source for var_type, source in init_config.items() if source['sample'] not in ('categorical','int','float')}\n",
        "        if invalid_sample_methods:\n",
        "            raise ValueError(f\"All values in 'init_config' must have 'sample' set to one of ('categorical','int','float') but found: {invalid_sample_methods}\")\n",
        "\n",
        "        #(2)Transfer the info from the dictionary to ConfigSpace.\n",
        "        #Firstly, define all the action parameters\n",
        "        for key in action_keys:\n",
        "            source = init_config[key]\n",
        "            sample_method = source['sample']\n",
        "            params = source['params']\n",
        "            if sample_method!='categorical':\n",
        "                raise ValueError(f\"init_config[{key}]['sample'] must be set to 'categorical' but found: {sample_method}\")\n",
        "            action_var_type, belongs_to = key[0][:-len('--action')] , key[1]\n",
        "            self.set_categorical(\n",
        "                var_type=action_var_type,\n",
        "                belongs_to=belongs_to,\n",
        "                action=True,\n",
        "                **params\n",
        "            )\n",
        "        ordered_keys = [key for key in non_action_keys] + [key for key in sig_keys]\n",
        "        #Secondly, define all the non-action parameters and sig parameters, IN ORDER.\n",
        "        for key in ordered_keys:\n",
        "            source = init_config[key]\n",
        "            sample_method = source['sample']\n",
        "            params = source['params']\n",
        "            var_type, belongs_to = key\n",
        "            cls = self.set_int if sample_method == 'int' \\\n",
        "                               else self.set_float if sample_method == 'float' \\\n",
        "                               else partial(self.set_categorical, action=False)\n",
        "            cls(var_type=var_type, belongs_to=belongs_to, **params)\n",
        "\n",
        "        #(3)Perform post verification\n",
        "        self.verify_syntax(True)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._config)\n",
        "\n",
        "    def verify_objective(self, warn_missing_var_types: bool):\n",
        "        \"\"\"\n",
        "        -Parameter: warn_missing_var_types := Whether to raise Warning instead of Errors for missing variables.\n",
        "                    Can be set to True if user is actively adjusting the ConfigSpace.\n",
        "        \"\"\"\n",
        "        #Get the source code of the 'self.objective'\n",
        "        try:\n",
        "            source = inspect.getsource(self.objective)\n",
        "        except OSError as e:\n",
        "            raise RuntimeError(\"Unable to retrieve source code of '.objective'\"\n",
        "                                \"Ensure that the method is defined in a way that allows source code inspection.\") from e\n",
        "\n",
        "        #Dedent the source code\n",
        "        source = textwrap.dedent(source)\n",
        "        #Parse the source code into an AST\n",
        "        tree = ast.parse(source)\n",
        "        #Create an instance of the visitor and walk the tree\n",
        "        visitor = SampleCallVisitor()\n",
        "        visitor.visit(tree)\n",
        "\n",
        "        config_var_types, sig_var_types = self.var_types(), self.sig_var_types()\n",
        "        #Extract only the user requests that have either None or an instance of ast.Constant\n",
        "        user_requests = visitor.user_requests\n",
        "        user_requests_with_defaults = visitor.user_requests_with_defaults\n",
        "        total_requests = user_requests | user_requests_with_defaults\n",
        "\n",
        "        #Catch any illegal request(s) to signature parameters.\n",
        "        #NOTE: Overlap between config_var_types and sig_var_types IS allowed,\n",
        "        #      and we only check incidents for sig_var_types - config_var_types\n",
        "        illegal_sig_var_types = sig_var_types - config_var_types\n",
        "        illegal_sig_calls = set(filter(\n",
        "            lambda req: req[0] in illegal_sig_var_types,\n",
        "            total_requests\n",
        "        ))\n",
        "        if illegal_sig_calls:\n",
        "            formatted = [f'self.sample({var_type},{default_val})' for var_type,default_val in illegal_sig_calls]\n",
        "            raise SyntaxError(f\"Following calls in '.objective' are illegal due to var_type referring to a signature parameter: {formatted}\")\n",
        "\n",
        "        #If independent user_requests are not covered by config_space.var_types(),\n",
        "        #then EITHER raise ValueError or raise Warning.\n",
        "        calls_with_missing_var_types = {req for req in user_requests if req[0] not in config_var_types}\n",
        "        if calls_with_missing_var_types:\n",
        "            formatted = [f'self.sample({req[0]},{req[1]})' for req in calls_with_missing_var_types]\n",
        "            msg = f\"Following calls in '.objective' are illegal due to var_type referring to a parameter that does not exist in the configuration space: {formatted}\"\n",
        "            if not warn_missing_var_types:\n",
        "                raise ValueError(msg)\n",
        "            warnings.warn(msg)\n",
        "\n",
        "        #If non-independent user_requests are made with var_types not covered by config_space.var_types(),\n",
        "        #then throw warning.\n",
        "        calls_with_potentially_missing_var_types = {\n",
        "            req for req in user_requests_with_defaults\n",
        "            if req[0] not in config_var_types\n",
        "        }\n",
        "        if calls_with_potentially_missing_var_types:\n",
        "            formatted = [f'self.sample({req[0]},{req[1]})' for req in calls_with_potentially_missing_var_types]\n",
        "            warnings.warn(f\"Following calls in '.objective' are referring to non-existing var_type(s) and will simply return respective default_val(s): {formatted}.\")\n",
        "\n",
        "        #Catch any (var_type, default_val) such that var_type exists in config_var_types BUT default_val is a value NOT allowed in the configuration space of 'var_type'.\n",
        "        existent_requests_with_defaults = {req for req in user_requests_with_defaults if req[0] in config_var_types}\n",
        "        req_to_sources = {req: self.get_source(req[0]) for req in existent_requests_with_defaults}\n",
        "        requests_with_illegal_defaults = set()\n",
        "        for (var_type,default_val),source in req_to_sources.items():\n",
        "            params, sample_method = source['params'], source['sample']\n",
        "            if any([\n",
        "                sample_method=='float' and not(isinstance(default_val, (float,int)) and  params['low']<=default_val<=params['high']),\n",
        "                sample_method=='int' and not(isinstance(default_val, int) and params['low']<=default_val<=params['high']),\n",
        "                sample_method=='categorical' and default_val not in params['choices']\n",
        "            ]):\n",
        "                requests_with_illegal_defaults.add((var_type,default_val))\n",
        "        if requests_with_illegal_defaults:\n",
        "            formatted = [f'self.sample({req[0]},{req[1]})' for req in requests_with_illegal_defaults]\n",
        "            raise ValueError(f\"Following calls in '.objective' are illegal due to 'default_val' referring to values that are not allowed in the configuration space of corresponding var_type: {formatted}\")\n",
        "\n",
        "    def verify_syntax(self, warn_missing_sig_var_types: bool):\n",
        "        \"\"\"\n",
        "        Verify the hyperparameter configuration.\n",
        "\n",
        "        Raises ValueError or SyntaxError depending on the point of failure.\n",
        "        \"\"\"\n",
        "        #First and foremost, ensure that there's no overlap between non-action_var_types AND action_var_types\n",
        "        overlaps = self.non_action_var_types() & self.action_var_types()\n",
        "        if overlaps:\n",
        "            raise ValueError(f\"Non-action hyperparameters and action hyperparameters overlap: {overlaps}\")\n",
        "        #Second, check for missing variable types:\n",
        "        self._verify_missing_sig_var_types(warn_missing_sig_var_types)\n",
        "        #Add more if needed...\n",
        "\n",
        "    def _verify_missing_sig_var_types(self, warn_missing_sig_var_types: bool):\n",
        "        msg = ''\n",
        "        for key, source in self._config.items():\n",
        "            var_type, belongs_to = key\n",
        "            if '--action' in var_type and belongs_to is None:\n",
        "                action_var_type = var_type[:-len('--action')]\n",
        "                sig_var_types = set()\n",
        "                for func in source['params']['choices']:\n",
        "                    sig_var_types |= set(signature(func).parameters.keys()) - {'device'}\n",
        "\n",
        "                #Below segment checks for missing parameters. We don't output an error immediately.\n",
        "                existing_var_types = self.sig_var_types(action_var_type)\n",
        "                missing_var_types = sig_var_types - existing_var_types\n",
        "                if missing_var_types:\n",
        "                    msg += f\"Action hyperparameter '{action_var_type}' requires additional hyperparameters: {[(p, action_var_type) for p in missing_var_types]}\\n\"\n",
        "\n",
        "        if len(msg) and not warn_missing_sig_var_types: #raise ValueError\n",
        "            raise ValueError(msg)\n",
        "        if len(msg): #warn the user.\n",
        "            warnings.warn(msg)\n",
        "\n",
        "    def var_types(self) -> set:\n",
        "        return {var_type[:-len('--action')] if '--action' in var_type else var_type for (var_type, belongs_to) in self._config.keys()\n",
        "                if belongs_to is None}\n",
        "\n",
        "    def action_var_types(self) -> set:\n",
        "        return {var_type[:-len('--action')] for (var_type, _) in self._config.keys() if '--action' in var_type}\n",
        "\n",
        "    def non_action_var_types(self) -> set:\n",
        "        return self.var_types() - self.action_var_types()\n",
        "\n",
        "    def sig_var_types(self, action_var_type: str = None) -> set:\n",
        "        if action_var_type is None:\n",
        "            return {var_type for (var_type, belongs_to) in self._config.keys() if belongs_to is not None}\n",
        "        else:\n",
        "            action_var_types = self.action_var_types()\n",
        "            if action_var_type not in action_var_types:\n",
        "                raise KeyError(f\"Input 'action_var_type': {action_var_type} is not in the set of registered action hyperparameters: {action_var_types}\")\n",
        "            return {var_type for (var_type, belongs_to) in self._config.keys() if belongs_to == action_var_type}\n",
        "\n",
        "    def get_source(self, var_type: str, belongs_to: str = None) -> dict:\n",
        "        config_keys = self._config.keys()\n",
        "        var_types = self.var_types()\n",
        "        action_var_types = self.action_var_types()\n",
        "        sig_var_types = self.sig_var_types()\n",
        "        if belongs_to is not None and belongs_to not in action_var_types:\n",
        "            raise KeyError(f\"Input 'belongs_to': {belongs_to} is not in the set of registered action hyperparameters: {action_var_types}\")\n",
        "        if var_type not in var_types and belongs_to is None:\n",
        "            raise KeyError(f\"Input 'var_type': {var_type} is not in the set of registered hyperparameters: {var_types}\")\n",
        "        if var_type in var_types and belongs_to is not None:\n",
        "            raise KeyError(f\"Input 'belongs_to': {belongs_to} should be None if 'var_type': {var_type} is not a signature hyperparameter.\")\n",
        "        if var_type in sig_var_types and (var_type, belongs_to) not in config_keys:\n",
        "            raise KeyError(f\"var_type: {var_type} is not one of the signature hyperparameters of action hyperparameter: {belongs_to}\")\n",
        "        formatted_var_type = var_type+'--action' if var_type in action_var_types else var_type\n",
        "        if (formatted_var_type, belongs_to) not in config_keys:\n",
        "            raise KeyError(f\"Requested ({var_type}, {belongs_to}) does not exist in the set of all pairs stored in ConfigSpace: {[(p[:-len('--action')] if '--action' in p else p,bt) for p,bt in config_keys]}\")\n",
        "\n",
        "        return self._config[(formatted_var_type, belongs_to)]\n",
        "\n",
        "    def get_sources(self, list_of_keys) -> list:\n",
        "        \"\"\"\n",
        "        Get the configurations for a list of hyperparameters.\n",
        "\n",
        "        Parameters:\n",
        "        - list_of_keys (list or set): List of (var_type, belongs_to) tuples.\n",
        "        \"\"\"\n",
        "        if not isinstance(list_of_keys, (list, set, tuple)):\n",
        "            raise ValueError(f\"Input 'list_of_keys' must be a list,set or a tuple but found: {type(list_of_keys)}\")\n",
        "        if any(not isinstance(key, tuple) for key in list_of_keys):\n",
        "            raise KeyError(f\"Non-tuple elements found in input 'list_of_keys': {list_of_keys}\")\n",
        "        return [self.get_source(var_type=key[0], belongs_to=key[1]) for key in list_of_keys]\n",
        "\n",
        "    def get_var(self, var_type: str, belongs_to: str = None) -> Var:\n",
        "        \"\"\"\n",
        "        Get the configuration for a specific hyperparameter.\n",
        "\n",
        "        Parameters:\n",
        "        - var_type (str): The hyperparameter name.\n",
        "        - belongs_to (str, optional): The action hyperparameter this hyperparameter belongs to.\n",
        "        \"\"\"\n",
        "        source = self.get_source(var_type, belongs_to)\n",
        "        if var_type not in self.action_var_types():\n",
        "            return Var(var_type, source, belongs_to)\n",
        "\n",
        "        #Assemble 'sig_dict' to initialize Action/Model objects\n",
        "        action_var_type = var_type\n",
        "        choices = source['params']['choices']\n",
        "        action_choices = []\n",
        "        for action_func in choices:\n",
        "            sig = set(signature(action_func).parameters.keys()) - {'device'}\n",
        "            existing_var_types = self.sig_var_types(action_var_type)\n",
        "            missing_var_types = sig - existing_var_types\n",
        "            if missing_var_types:\n",
        "                raise ValueError(f\"Requested action hyperparameter: {action_var_type} cannot be assembled due to missing hyperparameters: {missing_var_types}\")\n",
        "            sig_dict = {\n",
        "                sig_var_type: Var(sig_var_type, self.get_source(sig_var_type, action_var_type), action_var_type)\n",
        "                for sig_var_type in sig\n",
        "            }\n",
        "            #Add to 'action_choices' the initialized Action.\n",
        "            action_choices.append(ModelAction(action_func, sig_dict) if 'model' in action_var_type else Action(action_func, sig_dict))\n",
        "        source = {\n",
        "            'params': {'choices': action_choices},\n",
        "            'sample': 'categorical'\n",
        "        }\n",
        "        return ActionVar(action_var_type, source)\n",
        "\n",
        "    def get_action_vars(self, action_var_types=None):\n",
        "        self.verify_syntax(False)\n",
        "        if isinstance(action_var_types, str):\n",
        "            action_var_types = [action_var_types]\n",
        "        if action_var_types is not None:\n",
        "            return [self.get_var(action_var_type) for action_var_type in action_var_types]\n",
        "        return [self.get_var(action_var_type) for action_var_type in self.action_var_types()]\n",
        "\n",
        "    def get_non_action_vars(self, non_action_var_types=None):\n",
        "        self.verify_syntax(False)\n",
        "        if isinstance(non_action_var_types, str):\n",
        "            non_action_var_types = [non_action_var_types]\n",
        "        if non_action_var_types is not None:\n",
        "            return [self.get_var(var_type) for var_type in non_action_var_types]\n",
        "        return [self.get_var(var_type) for var_type in self.non_action_var_types()]\n",
        "\n",
        "    def get_sig_vars(self, list_keys: list=None):\n",
        "        if list_keys is not None and not isinstance(list_keys, list):\n",
        "            raise ValueError(f\"Input 'list_keys' must be None or a list but found: {list_keys}\")\n",
        "        invaid_keys = set() if list_keys is None else {key not in self._config.keys() for key in list_keys}\n",
        "        if invaid_keys:\n",
        "            raise KeyError(f\"Non-existent keys found in input 'list_keys': {list_keys}\")\n",
        "\n",
        "        if list_keys is None:\n",
        "            return [self.get_var(*key) for key in self._config.keys() if key[1] is not None]\n",
        "        return [self.get_var(*key) for key in list_keys]\n",
        "\n",
        "    def set_float(self, var_type: str, low: float, high: float, step: float = None, log: bool = False, belongs_to: str = None):\n",
        "        \"\"\"\n",
        "        Define a float hyperparameter.\n",
        "\n",
        "        Parameters:\n",
        "        - var_type (str): The name of the hyperparameter.\n",
        "        - low (float): The lower bound.\n",
        "        - high (float): The upper bound.\n",
        "        - step (float, optional): Step size.\n",
        "        - log (bool, optional): Whether to sample logarithmically.\n",
        "        - belongs_to (str, optional): The action hyperparameter this hyperparameter belongs to.\n",
        "        \"\"\"\n",
        "        #ValueError(s).\n",
        "        if any([\n",
        "            not isinstance(var_type, str),\n",
        "            not (belongs_to is None or isinstance(belongs_to, str)),\n",
        "            not isinstance(low, (float, int)),\n",
        "            not isinstance(high, (float, int)),\n",
        "            not (step is None or isinstance(step, (float, int))),\n",
        "            not isinstance(log, bool)\n",
        "        ]):\n",
        "            types = [type(x) for x in [var_type, str(belongs_to), low, high, step, log]]\n",
        "            raise ValueError(f\"Input var_type, belongs_to, low, high, step, log should be of types str, str/None, float/int, float/int, float/int or None, bool respectively but found {types}\")\n",
        "        if low > high:\n",
        "            raise ValueError(f\"low: {low} > high: {high}\")\n",
        "\n",
        "        #KeyError(s).\n",
        "        if any('-' in x or '>' in x for x in [var_type, str(belongs_to)]):\n",
        "            raise KeyError(f\"Input (var_type: {var_type}, belongs_to: {belongs_to}) contains at least one of the following characters ['-', '>']\")\n",
        "        if belongs_to is not None and belongs_to not in self.action_var_types():\n",
        "            raise KeyError(f\"'belongs_to': {belongs_to} is not in the set of registered action hyperparameters: {self.action_var_types()}\")\n",
        "        if var_type in self.action_var_types():\n",
        "            raise KeyError(f\"Input (var_type: {var_type}, belongs_to: {belongs_to}) defines a non-action parameter but var_type: {var_type} collides with the var_type of a registered action parameter. Overlapping names between non-action parameters and action parameters is not allowed.\")\n",
        "        belongs_to_choices = self.get_source(belongs_to)['params']['choices'] if belongs_to is not None else []\n",
        "        partitioned = [set(signature(action_func).parameters.keys()) for action_func in belongs_to_choices]\n",
        "        potential_sig_var_types = set.union(*partitioned) if partitioned else set()\n",
        "        if belongs_to is not None and var_type not in potential_sig_var_types:\n",
        "            raise KeyError(f\"Input (var_type: {var_type}, belongs_to: {belongs_to}) cannot define a signature parameter since var_type: {var_type} does not belong to the {belongs_to}'s signature: {potential_sig_var_types}\")\n",
        "\n",
        "        params = {'low': low, 'high': high, 'log': log}\n",
        "        if step is not None:\n",
        "            params['step'] = step\n",
        "\n",
        "        self._config[(var_type, belongs_to)] = {\n",
        "            'params': params,\n",
        "            'sample': 'float'\n",
        "        }\n",
        "\n",
        "        self.verify_syntax(True)\n",
        "\n",
        "    def set_int(self, var_type: str, low: int, high: int, step: int = 1, log: bool = False, belongs_to: str = None):\n",
        "        \"\"\"\n",
        "        Define an integer hyperparameter.\n",
        "\n",
        "        Parameters:\n",
        "        - var_type (str): The name of the hyperparameter.\n",
        "        - low (int): The lower bound.\n",
        "        - high (int): The upper bound.\n",
        "        - step (int, optional): Step size.\n",
        "        - log (bool, optional): Whether to sample logarithmically.\n",
        "        - belongs_to (str, optional): The action hyperparameter this hyperparameter belongs to.\n",
        "        \"\"\"\n",
        "        #ValueError(s).\n",
        "        if any([\n",
        "            not isinstance(var_type, str),\n",
        "            not (belongs_to is None or isinstance(belongs_to, str)),\n",
        "            not isinstance(low, int),\n",
        "            not isinstance(high, int),\n",
        "            not (step is None or isinstance(step, int)),\n",
        "            not isinstance(log, bool)\n",
        "        ]):\n",
        "            types = [type(x) for x in [var_type, str(belongs_to), low, high, step, log]]\n",
        "            raise ValueError(f\"Input var_type, belongs_to, low, high, step, log should be of types str, str/None, float/int, float/int, float/int or None, bool respectively but found {types}\")\n",
        "        if low > high:\n",
        "            raise ValueError(f\"low: {low} > high: {high}\")\n",
        "\n",
        "        #KeyError(s).\n",
        "        if any('-' in x or '>' in x for x in [var_type, str(belongs_to)]):\n",
        "            raise KeyError(f\"Input (var_type: {var_type}, belongs_to: {belongs_to}) contains at least one of the following characters ['-', '>']\")\n",
        "        if belongs_to is not None and belongs_to not in self.action_var_types():\n",
        "            raise KeyError(f\"'belongs_to': {belongs_to} is not in the set of registered action hyperparameters: {self.action_var_types()}\")\n",
        "        if var_type in self.action_var_types():\n",
        "            raise KeyError(f\"Input (var_type: {var_type}, belongs_to: {belongs_to}) defines a non-action parameter but var_type: {var_type} collides with the var_type of a registered action parameter. Overlapping names between non-action parameter and action parameters is not allowed.\")\n",
        "        belongs_to_choices = self.get_source(belongs_to)['params']['choices'] if belongs_to is not None else []\n",
        "        partitioned = [set(signature(action_func).parameters.keys()) for action_func in belongs_to_choices]\n",
        "        potential_sig_var_types = set.union(*partitioned) if partitioned else set()\n",
        "        if belongs_to is not None and var_type not in potential_sig_var_types:\n",
        "            raise KeyError(f\"Input (var_type: {var_type}, belongs_to: {belongs_to}) cannot define a signature parameter since var_type: {var_type} does not belong to the {belongs_to}'s signature: {potential_sig_var_types}\")\n",
        "\n",
        "        params = {'low': low, 'high': high, 'log': log, 'step': step}\n",
        "\n",
        "        self._config[(var_type, belongs_to)] = {\n",
        "            'params': params,\n",
        "            'sample': 'int'\n",
        "        }\n",
        "\n",
        "        self.verify_syntax(True)\n",
        "\n",
        "    def set_categorical(self, var_type: str, choices: list, action: bool = False, belongs_to: str = None):\n",
        "        \"\"\"\n",
        "        Define a categorical hyperparameter.\n",
        "\n",
        "        Parameters:\n",
        "        - var_type (str): The name of the hyperparameter.\n",
        "        - choices (list): List of possible values.\n",
        "        - action (bool, optional): Whether this hyperparameter is an action hyperparameter.\n",
        "        - belongs_to (str, optional): The action hyperparameter this hyperparameter belongs to.\n",
        "        \"\"\"\n",
        "        #ValueError(s).\n",
        "        if any([\n",
        "            not isinstance(var_type, str),\n",
        "            not (belongs_to is None or isinstance(belongs_to, str)),\n",
        "            not isinstance(action, bool)\n",
        "        ]):\n",
        "            types = [type(x) for x in [var_type, str(belongs_to)]]\n",
        "            raise ValueError(f\"Input var_type, belongs_to should be of types str, str/None respectively but found {types}\")\n",
        "        if not isinstance(choices, list):\n",
        "            raise ValueError(f\"Input 'choices' must be a list but found: {type(choices)}\")\n",
        "        if action and any(not callable(choice) for choice in choices):\n",
        "            raise ValueError(f\"Non-callable elements found in input choices: {choices}\")\n",
        "\n",
        "        #KeyError(s).\n",
        "        if any('-' in x or '>' in x for x in [var_type, str(belongs_to)]):\n",
        "            raise KeyError(f\"Input (var_type: {var_type}, belongs_to: {belongs_to}) contains at least one of the following characters ['-', '>']\")\n",
        "        if belongs_to is not None and belongs_to not in self.action_var_types():\n",
        "            raise KeyError(f\"'belongs_to': {belongs_to} is not in the set of registered action hyperparameters: {self.action_var_types()}\")\n",
        "        if not action and var_type in self.action_var_types() or action and var_type in self.non_action_var_types():\n",
        "            target, opposite = ('action','non-action') if action else ('non-action', 'action')\n",
        "            raise KeyError(f\"Input (var_type: {var_type}, belongs_to: {belongs_to}) defines a {target} parameter but var_type: {var_type} collides with the var_type of a registered {opposite} parameter. Overlapping names between {target} parameters and {opposite} parameters is not allowed.\")\n",
        "        if action and belongs_to is not None:\n",
        "            raise KeyError(f\"Input 'belongs_to' must be None if 'action' is True but found: {belongs_to}\")\n",
        "        belongs_to_choices = self.get_source(belongs_to)['params']['choices'] if belongs_to is not None else []\n",
        "        partitioned = [set(signature(action_func).parameters.keys()) for action_func in belongs_to_choices]\n",
        "        potential_sig_var_types = set.union(*partitioned) if partitioned else set()\n",
        "        if belongs_to is not None and var_type not in potential_sig_var_types:\n",
        "            raise KeyError(f\"Input (var_type: {var_type}, belongs_to: {belongs_to}) cannot define a signature parameter since var_type: {var_type} does not belong to the {belongs_to}'s signature: {potential_sig_var_types}\")\n",
        "\n",
        "        if not action or (var_type+'--action', belongs_to) not in self._config.keys():\n",
        "            formatted_var_type = var_type+'--action' if action else var_type\n",
        "            self._config[(formatted_var_type, belongs_to)] = {\n",
        "                'params': {'choices': choices},\n",
        "                'sample': 'categorical'\n",
        "            }\n",
        "        else: #Overriding the existing (action_var_type,belongs_to) has been requested:\n",
        "            #STEP1: POP all the related parameters\n",
        "            popped = self.pop(var_type)\n",
        "            #STEP2: Now, given that we're guaranteed we DO NOT have (var_type+'--action',None) in self._config,\n",
        "            #       we .set_categorical(var_type,..rest)\n",
        "            self.set_categorical(var_type=var_type, choices=choices, action=True)\n",
        "            #STEP3: Last but not least, selectively 'inherit' the information by looking at the potential sig_var_types needed for the new action parameter\n",
        "            partitioned = [set(signature(action_func).parameters.keys()) for action_func in choices]\n",
        "            sig_var_types = set.union(*partitioned) if partitioned else set()\n",
        "            inherit_sig_var_keys = {\n",
        "                sig_var_type for sig_var_type in sig_var_types\n",
        "                if (sig_var_type,var_type) in popped.keys()\n",
        "            }\n",
        "            for key in inherit_sig_var_keys:\n",
        "                source = popped[key]\n",
        "                sig_var_type, action_var_type = key\n",
        "                if source['sample']=='categorical':\n",
        "                    self.set_categorical(\n",
        "                        var_type=sig_var_type,\n",
        "                        belongs_to=action_var_type,\n",
        "                        choices=source['params']['choices']\n",
        "                    )\n",
        "                else:\n",
        "                    cls = self.set_float if source['sample']=='float' else self.set_int\n",
        "                    cls(var_type=sig_var_type, belongs_to=action_var_type, **source['params'])\n",
        "\n",
        "        self.verify_syntax(True)\n",
        "\n",
        "    def pop(self, var_type: str, belongs_to: str=None):\n",
        "        \"\"\"\n",
        "        Delete a hyperparameter and return its source(s). Multiple sources may be returned if a deleted hyperparameter is an action parameter.\n",
        "\n",
        "        Parameters:\n",
        "        - var_type (str): The name of the hyperparameter.\n",
        "        - belongs_to (str, optional): The action hyperparameter this hyperparameter belongs to.\n",
        "\n",
        "        Returns:\n",
        "        Dictionary(or dictionaries if what you deleted is an action_var_type) of mapping (var_type->source) for all deleted var_types.\n",
        "\n",
        "        NOTE: If you delete an Action parameter, Then ALL of its signature parameters will also be deleted.\n",
        "        \"\"\"\n",
        "        #If you can successfully retrieve a source from the provided pair, it means we have a valid pair\n",
        "        source = self.get_source(var_type, belongs_to)\n",
        "        is_action_var_type = var_type in self.action_var_types()\n",
        "\n",
        "        popped = dict()\n",
        "        #(1)If action_var_type with none of the signature parameters, Then just delete it.\n",
        "        #(2)If action_var_type with signature parameters, Then pop all the signature parameters first, then combine it with the popped action parameter(now action parameter with no signature parameters)\n",
        "        #(3)If not action_var_type, just pop it out.\n",
        "        if is_action_var_type and self.sig_var_types(var_type):\n",
        "            sig_var_types = self.sig_var_types(var_type)\n",
        "            #Note that we avoid recursion by directly popping both sig_var_types, and the action_var_type.\n",
        "            popped |= {(sig_var_type, var_type):self._config.pop((sig_var_type, var_type)) for sig_var_type in self.sig_var_types(var_type)}\n",
        "            return {(var_type,belongs_to):self._config.pop((var_type+'--action',belongs_to))} | popped\n",
        "\n",
        "        formatted_var_type = var_type+'--action' if is_action_var_type else var_type\n",
        "        popped = self._config.pop((formatted_var_type, belongs_to))\n",
        "        #If what was popped was a signature var_type, then run self.verify_syntax() to print out message about what the owner action_var_type will now be missing.\n",
        "        if belongs_to is not None:\n",
        "            self.verify_syntax(True)\n",
        "        #Return the resultant value.\n",
        "        return {(var_type, belongs_to): popped}\n",
        "\n",
        "    @abstractmethod\n",
        "    def objective(self):\n",
        "        return NotImplementedError(\".objective not implemented\")"
      ],
      "metadata": {
        "id": "1QtUPldITAQx"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UserConfigSpace(ConfigSpace):\n",
        "    def objective(self):\n",
        "        #dls_action <- Action object encapsulating DataLoaders construction(and involved parameters)\n",
        "        dls_action = self.sample('dls_action')\n",
        "        #dls <- Executed dls_action(device=self.device)\n",
        "        dls = dls_action(device=self.device)\n",
        "        #model_action <- ModelAction object encapsulating model construction(and involved parameters).\n",
        "        model_action = self.sample('model_action')\n",
        "        #model_action_func <- model_action's action_func\n",
        "        model_action_func = model_action.action_func\n",
        "        #model_action's saves state <- model_action.action_state.\n",
        "        model_action_state = model_action.action_state\n",
        "        #model <- Model yielded from action.\n",
        "        model = model_action(device=self.device)\n",
        "        #loss_func <- Sampled loss function.\n",
        "        loss_func = self.sample('loss_func')\n",
        "        #If the loss_func is a subclass of nn.Module, Then initialize.\n",
        "        if isinstance(loss_func, type) and issubclass(loss_func, nn.Module):\n",
        "            loss_func = loss_func()\n",
        "\n",
        "        #model_container <- Fast.AI's model_container object constructed from it.\n",
        "        model_container = Learner(dls=dls,\n",
        "                        model=model,\n",
        "                        loss_func=loss_func() if isinstance(loss_func, nn.Module) else loss_func,\n",
        "                        #***opt_func=self.sample('opt_func'), NOT IMPLEMENTED YET!!!***\n",
        "                        metrics=self.sample('metric')\n",
        "                        )\n",
        "\n",
        "        #Interaction with ModelAction.action_state during runtime to make a decision on 'freeze'.\n",
        "        if model_action_state is None and self.sample('freeze', False):\n",
        "            model_container.freeze()\n",
        "        else: #IF (action_state is not None) OR (action_state is None but sampled 'freeze' states False, THEN .unfreeze().\n",
        "            model_container.unfreeze()\n",
        "\n",
        "        #Barebone cbs list:\n",
        "        cbs = []\n",
        "        #Fill the list:\n",
        "        if self.sample('gradient_clip',False): #Apply Gradient clipping, if exists and is True.\n",
        "            cbs.append(GradientClip(max_norm=self.sample('max_norm', 0.1))) #If exists, then 'max_norm' MUST exist, hence no safety guarantee.\n",
        "        if self.trial is None:\n",
        "            cbs.append(EarlyStoppingCallback(monitor='valid_loss',\n",
        "                                            min_delta=self.sample('min_delta', 0.1),\n",
        "                                            patience=self.sample('patience', 15)))\n",
        "        else: #IF trial is a type of optuna.Trail, then apply PruningCallback.\n",
        "            cbs.append(FastAIPruningCallback(trial=self.trial, monitor='valid_loss'))\n",
        "\n",
        "        #Make the action and return it.\n",
        "        trainer = None\n",
        "        if self.sample('one_cycle', True):\n",
        "            lr_low = self.sample('lr_low', 9e-4)\n",
        "            lr_high = self.sample('lr_high', lr_low)\n",
        "            trainer = partial(model_container.fit_one_cycle,\n",
        "                            n_epoch=self.sample('n_epoch', 10),\n",
        "                            lr_max=slice(lr_low, lr_high),\n",
        "                            wd=self.sample('wd', 0.01), pct_start=self.sample('pct_start',0.3),\n",
        "                            cbs=cbs)\n",
        "        else:\n",
        "            trainer = partial(model_container.fit,\n",
        "                            n_epoch=self.sample('n_epoch', 10),\n",
        "                            lr=self.sample('lr', 1e-3),\n",
        "                            wd=self.sample('wd', 0.01),\n",
        "                            cbs=cbs)\n",
        "\n",
        "        #Train the model.\n",
        "        with model_container.no_bar(), model_container.no_logging():\n",
        "                trainer()\n",
        "\n",
        "        #Return (objective loss, model container)\n",
        "        return (model_container.validate()[0], model_container)"
      ],
      "metadata": {
        "id": "0gzaYCOni3eD"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Test: ConfigSpace, UserConfigSpace"
      ],
      "metadata": {
        "id": "Jero3rT9TJ8H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dummy_action1(param1, param2):\n",
        "    pass\n",
        "\n",
        "def dummy_action2(param3, param4, device=None):\n",
        "    pass\n",
        "\n",
        "def dummy_action3(param5, param6):\n",
        "    pass\n",
        "\n",
        "def dummy_action_evens(param2, param4, param6, device=None):\n",
        "    pass\n",
        "\n",
        "def dummy_action_odds(param1, param3, param5, device=None):\n",
        "    pass\n",
        "\n",
        "def dummy_action_combined(param1, param2, param3, param4, param5, param6):\n",
        "    pass\n",
        "\n",
        "def non_callable():\n",
        "    pass\n",
        "\n",
        "class TestUserConfigSpace(unittest.TestCase):\n",
        "    def setUp(self):\n",
        "        self.cs = UserConfigSpace()\n",
        "\n",
        "    def test_set_float(self):\n",
        "        self.cs.set_float('learning_rate', low=0.001, high=0.01)\n",
        "        expected = {\n",
        "            ('learning_rate', None): {\n",
        "                'params': {'low': 0.001, 'high': 0.01, 'log': False},\n",
        "                'sample': 'float'\n",
        "            }\n",
        "        }\n",
        "        self.assertEqual(self.cs._config, expected)\n",
        "\n",
        "        self.assertEqual(\n",
        "            self.cs.get_source('learning_rate'),\n",
        "            expected[('learning_rate', None)]\n",
        "        )\n",
        "\n",
        "    def test_set_int(self):\n",
        "        self.cs.set_int('num_layers', low=1, high=5)\n",
        "        expected = {\n",
        "            ('num_layers', None): {\n",
        "                'params': {'low': 1, 'high': 5, 'log': False, 'step': 1},\n",
        "                'sample': 'int'\n",
        "            }\n",
        "        }\n",
        "        self.assertEqual(self.cs._config, expected)\n",
        "\n",
        "        self.assertEqual(\n",
        "            self.cs.get_source('num_layers'),\n",
        "            expected[('num_layers', None)]\n",
        "        )\n",
        "\n",
        "    def test_set_categorical(self):\n",
        "        self.cs.set_categorical('activation', choices=[nn.ReLU, nn.Tanh, nn.Sigmoid])\n",
        "        expected = {\n",
        "            ('activation', None): {\n",
        "                'params': {'choices': [nn.ReLU, nn.Tanh, nn.Sigmoid]},\n",
        "                'sample': 'categorical'\n",
        "            }\n",
        "        }\n",
        "        self.assertEqual(self.cs._config, expected)\n",
        "\n",
        "        self.assertEqual(\n",
        "            self.cs.get_source('activation'),\n",
        "            expected[('activation', None)]\n",
        "        )\n",
        "\n",
        "    def test_set_action(self):\n",
        "        self.cs.set_categorical('optimizer', choices=[dummy_action1, dummy_action2], action=True)\n",
        "        expected = {\n",
        "            ('optimizer--action', None): {\n",
        "                'params': {'choices': [dummy_action1, dummy_action2]},\n",
        "                'sample': 'categorical'\n",
        "            }\n",
        "        }\n",
        "        self.assertEqual(self.cs._config, expected)\n",
        "\n",
        "        self.assertEqual(\n",
        "            self.cs.get_source('optimizer'),\n",
        "            expected[('optimizer--action', None)]\n",
        "        )\n",
        "\n",
        "        for i in range(1,5):\n",
        "            self.cs.set_float(f'param{i}', low=0.0, high=1.0, belongs_to='optimizer')\n",
        "        sig_dict_dummy_action1 = {f'param{i}': Var(f'param{i}', {'params': {'low': 0.0, 'high': 1.0, 'log': False}, 'sample': 'float'}, 'optimizer') for i in range(1,3)}\n",
        "        sig_dict_dummy_action2 = {f'param{i}': Var(f'param{i}', {'params': {'low': 0.0, 'high': 1.0, 'log': False}, 'sample': 'float'}, 'optimizer') for i in range(3,5)}\n",
        "        self.assertEqual(\n",
        "            self.cs.get_var('optimizer'),\n",
        "            ActionVar(\n",
        "                'optimizer',\n",
        "                 { 'params': {'choices': [Action(dummy_action1, sig_dict_dummy_action1), Action(dummy_action2, sig_dict_dummy_action2)]},\n",
        "                   'sample': 'categorical'}\n",
        "            )\n",
        "        )\n",
        "\n",
        "    def test_categorization(self):\n",
        "        #action_var_types\n",
        "        self.cs.set_categorical('optimizer', choices=[dummy_action1, dummy_action2], action=True)\n",
        "\n",
        "        #non_action_var_types\n",
        "        self.cs.set_int('num_layers', low=1, high=5)\n",
        "        self.cs.set_float('learning_rate', low=0.001, high=0.01)\n",
        "        self.cs.set_categorical('activation', choices=[nn.ReLU, nn.Tanh, nn.SiLU])\n",
        "\n",
        "        #sig_var_types\n",
        "        self.cs.set_float('param1', low=0.0, high=1.0, belongs_to='optimizer')\n",
        "        self.cs.set_float('param2', low=0.0, high=1.0, belongs_to='optimizer')\n",
        "        self.cs.set_float('param3', low=0.0, high=1.0, belongs_to='optimizer')\n",
        "        self.cs.set_float('param4', low=0.0, high=1.0, belongs_to='optimizer')\n",
        "\n",
        "        self.assertEqual(\n",
        "            self.cs.action_var_types(),\n",
        "            {'optimizer'}\n",
        "        )\n",
        "        self.assertEqual(\n",
        "            self.cs.non_action_var_types(),\n",
        "            {'num_layers', 'learning_rate', 'activation'}\n",
        "        )\n",
        "        self.assertEqual(\n",
        "            self.cs.sig_var_types('optimizer'),\n",
        "            {'param1', 'param2', 'param3', 'param4'}\n",
        "        )\n",
        "\n",
        "        with self.assertRaises(KeyError):\n",
        "            self.cs.set_float('param5', low=0.0, high=1.0, belongs_to='non_existent_action')\n",
        "        with self.assertRaises(KeyError):\n",
        "            #Should raise KeyError since 'non_existent_param' does NOT exist in signature(s) of ActionVar category 'optimizer'\n",
        "            self.cs.set_int('non_existent_param', low=0, high=1, belongs_to='optimizer')\n",
        "\n",
        "        #Get rid of the 'param1' from 'optimizer'\n",
        "        popped = self.cs.pop('param1', 'optimizer')\n",
        "        self.assertEqual(\n",
        "            popped,\n",
        "            {\n",
        "                ('param1', 'optimizer'): {\n",
        "                    'params': {'low': 0.0, 'high': 1.0, 'log': False},\n",
        "                    'sample': 'float'\n",
        "                }\n",
        "            }\n",
        "        )\n",
        "\n",
        "        #Verify that the action parameter requires signature parameters now.(The missing one; param1)\n",
        "        with self.assertRaises(ValueError):\n",
        "            self.cs.verify_syntax(False)\n",
        "\n",
        "    def test_add_signature_parameters(self):\n",
        "        self.cs.set_categorical('optimizer', choices=[dummy_action1, dummy_action2], action=True)\n",
        "        self.cs.set_float('param1', low=0.0, high=1.0, belongs_to='optimizer')\n",
        "        self.cs.set_float('param2', low=0.0, high=1.0, belongs_to='optimizer')\n",
        "        self.cs.set_float('param3', low=0.0, high=1.0, belongs_to='optimizer')\n",
        "        self.cs.set_float('param4', low=0.0, high=1.0, belongs_to='optimizer')\n",
        "        #Now verify should pass\n",
        "        self.cs.verify_syntax(False)\n",
        "\n",
        "        #Delete the 'optimizer'. This should delete all the other 'param1',...,'param4' as well.\n",
        "        popped = self.cs.pop('optimizer')\n",
        "        expected = {\n",
        "                ('optimizer', None): {\n",
        "                    'params': {'choices': [dummy_action1, dummy_action2]},\n",
        "                    'sample': 'categorical'\n",
        "                },\n",
        "                ('param1', 'optimizer'): {\n",
        "                    'params': {'low': 0.0, 'high': 1.0, 'log': False},\n",
        "                    'sample': 'float'\n",
        "                },\n",
        "                ('param2', 'optimizer'): {\n",
        "                    'params': {'low': 0.0, 'high': 1.0, 'log': False},\n",
        "                    'sample': 'float'\n",
        "                },\n",
        "                ('param3', 'optimizer'): {\n",
        "                    'params': {'low': 0.0, 'high': 1.0, 'log': False},\n",
        "                    'sample': 'float'\n",
        "                },\n",
        "                ('param4', 'optimizer'): {\n",
        "                    'params': {'low': 0.0, 'high': 1.0, 'log': False},\n",
        "                    'sample': 'float'\n",
        "                }\n",
        "        }\n",
        "        self.assertEqual(\n",
        "            popped,\n",
        "            expected,\n",
        "            msg = f\"popped: {popped} != \\nexpected: {expected}\"\n",
        "        )\n",
        "\n",
        "    def test_invalid_var_type_name(self):\n",
        "        with self.assertRaises(KeyError):\n",
        "            self.cs.set_float('learning>rate', low=0.001, high=0.01)\n",
        "\n",
        "    def test_invalid_belongs_to(self):\n",
        "        with self.assertRaises(KeyError):\n",
        "            self.cs.set_float('param', low=0.0, high=1.0, belongs_to='non_existent_action')\n",
        "\n",
        "    def test_non_callable_action_choice(self):\n",
        "        with self.assertRaises(ValueError): #action=True means that all inputs in 'choices' must be callable.\n",
        "            self.cs.set_categorical('optimizer', choices=[1,2], action=True)\n",
        "\n",
        "    def test_set_categorical_with_invalid_choices(self):\n",
        "        with self.assertRaises(ValueError):\n",
        "            self.cs.set_categorical('optimizer', choices='not_a_list')\n",
        "\n",
        "    def test_set_float_with_invalid_types(self):\n",
        "        with self.assertRaises(ValueError):\n",
        "            self.cs.set_float('learning_rate', low='0.001', high=0.01)\n",
        "\n",
        "    def test_set_int_with_invalid_types(self):\n",
        "        with self.assertRaises(ValueError):\n",
        "            self.cs.set_int('num_layers', low=1, high='5')\n",
        "\n",
        "    def test_get_non_action_var(self):\n",
        "        self.cs.set_float('learning_rate', low=0.001, high=0.01)\n",
        "        var = self.cs.get_var('learning_rate')\n",
        "        self.assertIsInstance(var, Var)\n",
        "        expected_var = Var('learning_rate', {'params': {'low': 0.001, 'high': 0.01, 'log': False}, 'sample': 'float'}, None)\n",
        "        self.assertEqual(var, expected_var)\n",
        "\n",
        "    def test_get_action_var(self):\n",
        "        self.cs.set_categorical('optimizer', choices=[dummy_action1], action=True)\n",
        "        self.cs.set_float('param1', low=0.0, high=1.0, belongs_to='optimizer')\n",
        "        self.cs.set_float('param2', low=0.0, high=1.0, belongs_to='optimizer')\n",
        "        action_var = self.cs.get_var('optimizer')\n",
        "        self.assertIsInstance(action_var, ActionVar)\n",
        "        expected_param1_var = Var('param1', {'params': {'low': 0.0, 'high': 1.0, 'log': False}, 'sample': 'float'}, 'optimizer')\n",
        "        expected_param2_var = Var('param2', {'params': {'low': 0.0, 'high': 1.0, 'log': False}, 'sample': 'float'}, 'optimizer')\n",
        "        expected_action_var = ActionVar(\n",
        "            'optimizer',\n",
        "             {\n",
        "                 'params': {'choices': [Action(dummy_action1, {expected_param1_var.var_type: expected_param1_var, expected_param2_var.var_type: expected_param2_var})]},\n",
        "                 'sample': 'categorical'\n",
        "             }\n",
        "        )\n",
        "        self.assertEqual(\n",
        "            action_var,\n",
        "            expected_action_var,\n",
        "            msg=f\"action_var: {action_var} != \\nexpected_action_var: {expected_action_var}\"\n",
        "        )\n",
        "\n",
        "    def test_get_action_vars(self):\n",
        "        self.cs.set_categorical('optimizer', choices=[dummy_action1], action=True)\n",
        "        self.cs.set_float('param1', low=0.0, high=1.0, belongs_to='optimizer')\n",
        "        #Attempt to retrieve optimizer w/o filling in all the required signature parameters\n",
        "        with self.assertRaises(ValueError): #Will internally run self._verify(), NOT self._verify(True,True)\n",
        "            self.cs.get_action_vars()\n",
        "\n",
        "        self.cs.set_float('param2', low=0.0, high=1.0, belongs_to='optimizer')\n",
        "        self.cs.verify_syntax(False)\n",
        "\n",
        "        #This time, you'll be missing param1\n",
        "        self.cs.pop('param1', 'optimizer')\n",
        "        with self.assertRaises(ValueError):\n",
        "            self.cs.get_action_vars()\n",
        "\n",
        "        #Add it again.\n",
        "        self.cs.set_float('param1', low=0.0, high=1.0, belongs_to='optimizer')\n",
        "        action_vars = self.cs.get_action_vars() #Should work this time.\n",
        "\n",
        "        #Verify the number of action_vars.\n",
        "        self.assertEqual(len(action_vars), 1)\n",
        "\n",
        "        #Verify that it's as we expected.\n",
        "        action_var = action_vars[0]\n",
        "        self.assertIsInstance(action_var, ActionVar)\n",
        "        expected_param1_var = Var('param1', {'params': {'low': 0.0, 'high': 1.0, 'log': False}, 'sample': 'float'}, 'optimizer')\n",
        "        expected_param2_var = Var('param2', {'params': {'low': 0.0, 'high': 1.0, 'log': False}, 'sample': 'float'}, 'optimizer')\n",
        "        expected_action_var = ActionVar(\n",
        "            'optimizer',\n",
        "             {\n",
        "                 'params': {'choices': [Action(dummy_action1, {expected_param1_var.var_type: expected_param1_var, expected_param2_var.var_type: expected_param2_var})]},\n",
        "                 'sample': 'categorical'\n",
        "             }\n",
        "        )\n",
        "        self.assertEqual(\n",
        "            action_var,\n",
        "            expected_action_var,\n",
        "            msg=f\"action_var: {action_var} != \\nexpected_action_var: {expected_action_var}\"\n",
        "        )\n",
        "\n",
        "    def test_get_non_action_vars(self):\n",
        "        self.cs.set_float('learning_rate', low=0.001, high=0.01)\n",
        "        self.cs.set_int('num_layers', low=1, high=5)\n",
        "        vars = self.cs.get_non_action_vars()\n",
        "        self.assertEqual(len(vars), 2)\n",
        "\n",
        "        #Check that created Var objects are as we expected.\n",
        "        expected_lr_var = Var('learning_rate', {'params': {'low': 0.001, 'high': 0.01, 'log': False}, 'sample': 'float'}, None)\n",
        "        expected_num_layers_var = Var('num_layers', {'params': {'low': 1, 'high': 5, 'log': False, 'step': 1}, 'sample': 'int'}, None)\n",
        "        expected_dict = {expected_lr_var.var_type: expected_lr_var, expected_num_layers_var.var_type: expected_num_layers_var}\n",
        "        for var in vars:\n",
        "            var_type = var.var_type\n",
        "            expected = expected_dict[var_type]\n",
        "            self.assertEqual(\n",
        "                var,\n",
        "                expected,\n",
        "                msg=f\"var_type: {var_type} maps to --> var: {var} != /nexpected: {expected}\"\n",
        "            )\n",
        "        #Do the same, but for signature parameters. Don't forget to add the 'optimizer' first.\n",
        "        self.cs.set_categorical('optimizer', choices=[dummy_action1], action=True)\n",
        "        self.cs.set_float('param1', low=0.0, high=1.0, belongs_to='optimizer')\n",
        "\n",
        "        with self.assertRaises(KeyError): #'param4' not in optimizer signatures\n",
        "            self.cs.set_float('param4', low=0.0, high=1.0, belongs_to='optimizer')\n",
        "\n",
        "        self.cs.set_float('param2', low=0.0, high=1.0, belongs_to='optimizer')\n",
        "        sig_vars = self.cs.get_sig_vars()\n",
        "        self.assertEqual(len(sig_vars), 2)\n",
        "        expected_param1_var = Var('param1', {'params': {'low': 0.0, 'high': 1.0, 'log': False}, 'sample': 'float'}, 'optimizer')\n",
        "        expected_param2_var = Var('param2', {'params': {'low': 0.0, 'high': 1.0, 'log': False}, 'sample': 'float'}, 'optimizer')\n",
        "        expected_dict = {expected_param1_var.var_type:expected_param1_var,\n",
        "                         expected_param2_var.var_type:expected_param2_var}\n",
        "        for sig_var in sig_vars:\n",
        "            sig_var_type = sig_var.var_type\n",
        "            expected = expected_dict[sig_var_type]\n",
        "            self.assertEqual(\n",
        "                sig_var,\n",
        "                expected,\n",
        "                msg=f\"sig_var_type: {sig_var_type} maps to --> sig_var: {sig_var} != /nexpected: {expected}\"\n",
        "            )\n",
        "\n",
        "    def test_post_set_verify(self):\n",
        "        #Just print what you need out.\n",
        "        self.cs.set_categorical('optimizer', choices=[dummy_action1], action=True)\n",
        "\n",
        "        #Should NOT raise KeyError, overlap between non-action_var_types and sig_var_types is ALLOWED.\n",
        "        self.cs.set_float('param1', low=0.0, high=1.0)\n",
        "        self.cs.set_float('param1', low=0.0, high=1.0, belongs_to='optimizer')\n",
        "        self.cs.set_float('param2', low=0.0, high=1.0, belongs_to='optimizer')\n",
        "        self.cs.verify_syntax(False)\n",
        "\n",
        "        #Check sig_var_types\n",
        "        sig_var_types = self.cs.sig_var_types()\n",
        "        self.assertEqual(\n",
        "            sig_var_types,\n",
        "            {'param1', 'param2'}\n",
        "        )\n",
        "\n",
        "        #Check for action_var_types\n",
        "        action_var_types = self.cs.action_var_types()\n",
        "        self.assertEqual(\n",
        "            action_var_types,\n",
        "            {'optimizer'}\n",
        "        )\n",
        "\n",
        "        #Check for non_action_var_types\n",
        "        non_action_var_types = self.cs.non_action_var_types()\n",
        "        self.assertEqual(\n",
        "            non_action_var_types,\n",
        "            {'param1'}\n",
        "        )\n",
        "\n",
        "    def test_duplicate_hyperparameter(self):\n",
        "        self.cs.set_float('learning_rate', low=0.001, high=0.01)\n",
        "        self.cs.set_float('learning_rate', low=0.0001, high=0.1) #overriding is ok!\n",
        "        self.assertTrue(True)\n",
        "\n",
        "    def test_sig_var_types_method(self):\n",
        "        self.cs.set_categorical('optimizer', choices=[dummy_action1], action=True)\n",
        "        self.cs.set_float('param1', low=0.0, high=1.0, belongs_to='optimizer')\n",
        "        self.cs.set_float('param2', low=0.0, high=1.0, belongs_to='optimizer')\n",
        "        self.cs.set_categorical('optimizer2', choices=[dummy_action2], action=True)\n",
        "\n",
        "        with self.assertRaises(KeyError):\n",
        "            self.cs.set_float('param3', low=0.0, high=1.0, belongs_to='optimizer') #ONLY param1,param2 allowed\n",
        "        with self.assertRaises(KeyError):\n",
        "            self.cs.set_float('param4', low=0.0, high=1.0, belongs_to='optimizer') #ONLY param1,param2 allowed\n",
        "\n",
        "        self.cs.set_float('param3', low=0.0, high=1.0, belongs_to='optimizer2')\n",
        "        self.cs.set_float('param4', low=0.0, high=1.0, belongs_to='optimizer2')\n",
        "\n",
        "        sig_var_types = self.cs.sig_var_types('optimizer')\n",
        "        self.assertEqual(sig_var_types, {'param1', 'param2'})\n",
        "\n",
        "        sig_var_types = self.cs.sig_var_types('optimizer2')\n",
        "        self.assertEqual(sig_var_types, {'param3', 'param4'})\n",
        "\n",
        "        sig_var_types = self.cs.sig_var_types()\n",
        "        self.assertEqual(sig_var_types, {'param1', 'param2','param3', 'param4'})\n",
        "\n",
        "    def test_get_source_invalid_var_type(self):\n",
        "        with self.assertRaises(KeyError):\n",
        "            self.cs.get_source('nonexistent_var_type')\n",
        "\n",
        "    def test_get_source_invalid_belongs_to(self):\n",
        "        self.cs.set_categorical('optimizer', choices=[dummy_action1], action=True)\n",
        "        with self.assertRaises(KeyError):\n",
        "            self.cs.get_source('param1', belongs_to='invalid_action')\n",
        "\n",
        "    def test_set_action_with_belongs_to(self):\n",
        "        self.cs.set_categorical('optimizer', choices=[dummy_action1], action=True)\n",
        "        with self.assertRaises(KeyError):\n",
        "            self.cs.set_categorical('optimizer2', choices=[dummy_action2], action=True, belongs_to='optimizer')\n",
        "\n",
        "    def test_set_float_invalid_var_type_name(self):\n",
        "        with self.assertRaises(KeyError):  # Corrected from ValueError to KeyError\n",
        "            self.cs.set_float('learning-rate', low=0.001, high=0.01)\n",
        "        with self.assertRaises(KeyError):  # Corrected from ValueError to KeyError\n",
        "            self.cs.set_float('learning>rate', low=0.001, high=0.01)\n",
        "\n",
        "    def test_set_int_invalid_belongs_to_name(self):\n",
        "        with self.assertRaises(KeyError):\n",
        "            self.cs.set_int('num_layers', low=1, high=5, belongs_to='invalid>action')\n",
        "\n",
        "    def test_set_categorical_invalid_action_choice(self):\n",
        "        with self.assertRaises(ValueError):\n",
        "            self.cs.set_categorical('optimizer', choices=[123], action=True)\n",
        "\n",
        "    def test_set_float_invalid_range(self):\n",
        "        with self.assertRaises(ValueError):\n",
        "            self.cs.set_float('learning_rate', low=0.1, high=0.001)\n",
        "\n",
        "    def test_get_var_invalid_var_type(self):\n",
        "        with self.assertRaises(KeyError):\n",
        "            self.cs.get_var('nonexistent_var_type')\n",
        "\n",
        "    def test_get_var_invalid_belongs_to(self):\n",
        "        with self.assertRaises(KeyError):\n",
        "            self.cs.get_var('param1', belongs_to='nonexistent_action')\n",
        "\n",
        "    def test_set_categorical_with_invalid_action_flag(self):\n",
        "        with self.assertRaises(ValueError):\n",
        "            self.cs.set_categorical('optimizer', choices=[dummy_action1], action='not_bool')\n",
        "\n",
        "    def test_set_float_with_invalid_step(self):\n",
        "        with self.assertRaises(ValueError):\n",
        "            self.cs.set_float('learning_rate', low=0.001, high=0.01, step='not_float')\n",
        "\n",
        "    def test_set_int_with_invalid_step(self):\n",
        "        with self.assertRaises(ValueError):\n",
        "            self.cs.set_int('num_layers', low=1, high=5, step='not_int')\n",
        "\n",
        "    def test_set_categorical_with_invalid_choices_type(self):\n",
        "        with self.assertRaises(ValueError):\n",
        "            self.cs.set_categorical('activation', choices='not_a_list')\n",
        "\n",
        "    def test_action_param_with_invalid_choices(self):\n",
        "        with self.assertRaises(ValueError):\n",
        "            self.cs.set_categorical('optimizer', choices=[123, 'string'], action=True)\n",
        "\n",
        "    def test_verify_missing_signature_parameters(self):\n",
        "        self.cs.set_categorical('optimizer', choices=[dummy_action1], action=True)\n",
        "        # Missing signature parameters\n",
        "        with self.assertRaises(ValueError):\n",
        "            self.cs.verify_syntax(False)\n",
        "\n",
        "    def test_sig_var_types_invalid_action_var_type(self):\n",
        "        with self.assertRaises(KeyError):\n",
        "            self.cs.sig_var_types('nonexistent_action')\n",
        "\n",
        "    def test_get_var_missing_signature_parameters(self):\n",
        "        self.cs.set_categorical('optimizer', choices=[dummy_action1], action=True)\n",
        "        with self.assertRaises(ValueError):\n",
        "            self.cs.get_var('optimizer')\n",
        "\n",
        "    def test_get_sources_with_invalid_input(self):\n",
        "        with self.assertRaises(ValueError):\n",
        "            self.cs.get_sources('not_a_list')\n",
        "\n",
        "    def test_get_sources_with_non_tuple_elements(self):\n",
        "        with self.assertRaises(KeyError):\n",
        "            self.cs.get_sources(['not_a_tuple'])\n",
        "\n",
        "    def test_set_float_with_invalid_log_type(self):\n",
        "        with self.assertRaises(ValueError):\n",
        "            self.cs.set_float('learning_rate', low=0.001, high=0.01, log='not_bool')\n",
        "\n",
        "    def test_set_int_with_invalid_log_type(self):\n",
        "        with self.assertRaises(ValueError):\n",
        "            self.cs.set_int('num_layers', low=1, high=5, log='not_bool')\n",
        "\n",
        "    def test_set_float_with_invalid_param_types(self):\n",
        "        with self.assertRaises(ValueError):\n",
        "            self.cs.set_float(123, low=0.001, high=0.01)\n",
        "\n",
        "    def test_set_int_with_invalid_param_types(self):\n",
        "        with self.assertRaises(ValueError):\n",
        "            self.cs.set_int(123, low=1, high=5)\n",
        "\n",
        "    def test_set_categorical_with_invalid_param_types(self):\n",
        "        with self.assertRaises(ValueError):\n",
        "            self.cs.set_categorical(123, choices=[1, 2, 3])\n",
        "\n",
        "    def test_set_float_with_belongs_to_none_when_required(self):\n",
        "        self.cs.set_categorical('optimizer', choices=[dummy_action1], action=True)\n",
        "        #Overlap is allowed between sig_var_types and the rest of the spaces\n",
        "        #(It only needs to be prevented between action, non-action name spaces)\n",
        "        self.cs.set_float('param1', low=0.0, high=1.0, belongs_to=None)\n",
        "\n",
        "    def test_set_float_with_belongs_to_not_none_when_not_required(self):\n",
        "        with self.assertRaises(KeyError):\n",
        "            self.cs.set_float('learning_rate', low=0.001, high=0.01, belongs_to='optimizer')\n",
        "\n",
        "    def test_set_categorical_action_with_belongs_to_not_none(self):\n",
        "        with self.assertRaises(KeyError):\n",
        "            self.cs.set_categorical('optimizer', choices=[dummy_action1], action=True, belongs_to='something')\n",
        "        self.cs.set_categorical('optimizer', choices=[dummy_action1], action=True)\n",
        "        #Should raise KeyError because 'some_action' is not a registered action\n",
        "        with self.assertRaises(KeyError):\n",
        "            self.cs.set_categorical('optimizer2', choices=['adam', 'sgd'], belongs_to='some_action')\n",
        "\n",
        "    def test_get_var_with_invalid_belongs_to(self):\n",
        "        self.cs.set_float('learning_rate', low=0.001, high=0.01)\n",
        "        with self.assertRaises(KeyError):\n",
        "            self.cs.get_var('learning_rate', belongs_to='some_action')\n",
        "\n",
        "    def test_get_action_vars_with_invalid_input(self):\n",
        "        with self.assertRaises(KeyError):\n",
        "            self.cs.get_action_vars(action_var_types='nonexistent_action')\n",
        "\n",
        "    def test_get_non_action_vars_with_invalid_input(self):\n",
        "        with self.assertRaises(KeyError):\n",
        "            self.cs.get_non_action_vars(non_action_var_types='nonexistent_var')\n",
        "\n",
        "    #****STILL UNDER CONSTRUCTION*****\n",
        "    def test_overriding_action_parameter(self):\n",
        "        #Add 2,6th params to 'optimizer_evens'\n",
        "        self.cs.set_categorical('optimizer_evens', choices=[dummy_action_evens], action=True)\n",
        "        self.cs.set_float('param2', low=0.0, high=1.0, belongs_to='optimizer_evens')\n",
        "        with self.assertRaises(KeyError): #since 1 is an odd number\n",
        "            self.cs.set_float('param1', low=0.0, high=1.0, belongs_to='optimizer_evens')\n",
        "        self.cs.set_float('param6', low=0.0, high=1.0, belongs_to='optimizer_evens')\n",
        "        self.assertEqual(\n",
        "            self.cs.sig_var_types('optimizer_evens'),\n",
        "            {'param2', 'param6'}\n",
        "        )\n",
        "        self.assertEqual(\n",
        "            self.cs.sig_var_types(),\n",
        "            {'param2', 'param6'}\n",
        "        )\n",
        "\n",
        "        #Override 'optimizer_evens', with dummy_action_odds.\n",
        "        #Note that this will cause ALL the sig_var_types to be deleted, with none inherited resultantly.\n",
        "        self.cs.set_categorical('optimizer_evens', choices=[dummy_action_odds], action=True)\n",
        "        self.assertEqual(\n",
        "            self.cs.sig_var_types('optimizer_evens'),\n",
        "            set() #since none of the potential_sig_var_types(={'param1','param3','param5'}) were in the 'cs'.\n",
        "        )\n",
        "        self.assertEqual(\n",
        "            self.cs.sig_var_types(),\n",
        "            set()\n",
        "        )\n",
        "\n",
        "    def test_verify_objective(self):\n",
        "        with self.assertRaises(ValueError):\n",
        "            self.cs.verify_objective(warn_missing_var_types=False)\n",
        "        with self.assertWarns(Warning):\n",
        "            self.cs.verify_objective(warn_missing_var_types=True)\n",
        "    \"\"\"\n",
        "    def test_sample_call_visitor(self):\n",
        "        #Dedent the source code\n",
        "        source = textwrap.dedent(source)\n",
        "        #Parse the source code into an AST\n",
        "        tree = ast.parse(source)\n",
        "        #Create an instance of the visitor and walk the tree\n",
        "        visitor = SampleCallVisitor()\n",
        "        visitor.visit(tree)\n",
        "    \"\"\"\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    unittest.main(argv=['first-arg-is-ignored'], exit=False)"
      ],
      "metadata": {
        "id": "Wszp-6JsjX6m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Class: ActionSampler"
      ],
      "metadata": {
        "id": "NpxMBQTBeH4_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ActionSampler:\n",
        "    def __init__(self, INIT_ACTION: ConfigSpace):\n",
        "        if not isinstance(INIT_ACTION, ConfigSpace):\n",
        "            raise ValueError(f\"Input 'INIT_ACTION' must be an instance of ConfigSpace but found: {type(INIT_ACTION)}\")\n",
        "\n",
        "        self.action_vars = {var.var_type: var for var in INIT_ACTION.get_action_vars()}\n",
        "        self.model_var_types = {action_var_type for action_var_type in self.action_vars.keys() if 'model' in action_var_type}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.action_vars)\n",
        "\n",
        "    def __eq__(self, other: ActionSampler):\n",
        "        return all([\n",
        "            isinstance(other, ActionSampler),\n",
        "            self.action_vars == other.action_vars,\n",
        "            self.model_var_types == other.model_var_types\n",
        "        ])\n",
        "\n",
        "    def __str__(self):\n",
        "        output = \"**********ActionSampler**********\\n\"\n",
        "        output += \"\".join([f\"{action_var}\\n\" for action_var in self.action_vars.values()])\n",
        "        return output\n",
        "\n",
        "    def get_var(self, var_type: str, action_idx_or_action_func=None, sig_var_type: str=None) -> ActionVar:\n",
        "        if var_type not in self.keys():\n",
        "            raise KeyError(f\"ActionSampler cannot retrieve Requested ActionVar: {var_type}, since it does NOT belong to: {self.keys()}\")\n",
        "        if action_idx_or_action_func != sig_var_type and (action_idx_or_action_func is None or sig_var_type is None):\n",
        "            raise KeyError(f\"Either both action_idx_or_action_func,sig_var_type should be None or both of them must be properly defined but found {(action_idx_or_action_func,sig_var_type)}\")\n",
        "        if sig_var_type is None:\n",
        "            return self.action_vars[var_type]\n",
        "        return self.get_action(var_type, action_idx_or_action_func).get_var(sig_var_type)\n",
        "\n",
        "    def get_action(self, action_var_type: str, action_idx_or_action_func) -> Action:\n",
        "        action_var = self.get_var(action_var_type)\n",
        "        assert isinstance(action_var, ActionVar), f\"type(action_var): {type(action_var)} != ActionVar\"\n",
        "        return action_var.get_action(action_idx_or_action_func)\n",
        "\n",
        "    def is_fixed(self):\n",
        "        return all(action_var.is_fixed() for action_var in self.action_vars.values())\n",
        "\n",
        "    def keys(self):\n",
        "        return self.action_vars.keys()\n",
        "\n",
        "    def values(self):\n",
        "        return self.action_vars.values()\n",
        "\n",
        "    def items(self):\n",
        "        return self.action_vars.items()\n",
        "\n",
        "    def sample(self, action_var_type: str, trial: Trial):\n",
        "        if trial is not None and not isinstance(trial, (optuna.trial.Trial, optuna.trial.FrozenTrial)):\n",
        "            raise ValueError(f\"Input 'trial' should be an object optuna::Trial or optuna::FrozenTrial but found: {type(trial)}\")\n",
        "        if trial is None:\n",
        "            raise ValueError(\"Input 'trial' cannot be None when sampling is requested through ActionSampler.\")\n",
        "\n",
        "        action_var = self.get_var(action_var_type)\n",
        "        assert isinstance(action_var, ActionVar), f\"type(action_var): {type(action_var)} != ActionVar\"\n",
        "\n",
        "        sampled_action = action_var.sample(trial=trial)\n",
        "        sampled_action.parameterize(trial=trial)\n",
        "\n",
        "        return sampled_action\n",
        "\n",
        "    def get_evaluated_params(self, source, exclude):\n",
        "        if not isinstance(source, (Trial, FrozenTrial, dict)):\n",
        "            raise ValueError(f\"Input 'source' must be of type in (optuna::Trial, optuna::FrozenTrial, dict) but found: {source}\")\n",
        "        if not isinstance(exclude, (list, tuple, str)):\n",
        "            raise ValueError(f\"Input 'exclude' must be one of (list, tuple, str) but found type(exclude): {type(exclude)}\")\n",
        "\n",
        "        source = source if isinstance(source, dict) else source.params\n",
        "        params = dict()\n",
        "\n",
        "        if isinstance(exclude, (list, tuple)):\n",
        "            exclude_set = set(exclude)\n",
        "        else:\n",
        "            exclude_set = {exclude}\n",
        "        action_var_types = set(self.keys()) - exclude_set\n",
        "\n",
        "        action_vars_to_assemble = {\n",
        "            key: val for key, val in source.items()\n",
        "            if key[0] in action_var_types and key[1] == 'None'\n",
        "        }\n",
        "\n",
        "        for key, val in action_vars_to_assemble.items():\n",
        "            action_var = self.get_var(key[0])\n",
        "            params[key] = action_var.get_loadable_action(val, source)\n",
        "\n",
        "        return params\n",
        "\n",
        "    def update(self, list_params: list, update_state_window: int, device: torch.device):\n",
        "        #IF action_var_type == 'model_action'\n",
        "        for action_var_type, action_var in self.items():\n",
        "            relevant_params = [params for params in list_params if (action_var_type, 'None') in params]\n",
        "            if not relevant_params:\n",
        "                continue\n",
        "\n",
        "            action_idxs = [params[(action_var_type,'None')] for params in relevant_params]\n",
        "            action_var.update(vals=action_idxs)\n",
        "\n",
        "            unique_action_idxs = list(dict.fromkeys(action_idxs).keys())\n",
        "\n",
        "            if action_var_type in self.model_var_types:\n",
        "                relevant_params = [\n",
        "                    params|self.get_evaluated_params(source=params,exclude=[action_var_type])\n",
        "                    for params in relevant_params\n",
        "                ]\n",
        "\n",
        "            action_var.update_actions(\n",
        "                update_action_idxs=unique_action_idxs,\n",
        "                list_params=relevant_params,\n",
        "                update_state_window=update_state_window,\n",
        "                device=device\n",
        "            )"
      ],
      "metadata": {
        "id": "3Fhz1hiPfHKJ"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Test: ActionSampler"
      ],
      "metadata": {
        "id": "Ihwh3f14Wl_K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Assuming Var, Action, ModelAction, and ActionVar are defined as per the improved code.\n",
        "\n",
        "#Fake Objective class\n",
        "class Objective:\n",
        "    def __init__(self, trial, combined_sampler, device, default_dict):\n",
        "        self.trial = trial\n",
        "        self.combined_sampler = combined_sampler\n",
        "        self.device = device\n",
        "        self.default_dict = default_dict\n",
        "\n",
        "    def execute_objective(self):\n",
        "        model = self.default_dict['model_action'](device=self.device)\n",
        "\n",
        "        for param in model.parameters():\n",
        "            param.data.fill_(0)\n",
        "\n",
        "        model_container = Mock(spec=Learner)\n",
        "        model_container.model = model\n",
        "\n",
        "        return (0, model)\n",
        "\n",
        "#Sample model classes\n",
        "class ModelDefault(nn.Module):\n",
        "    def __init__(self, rand_number: int):\n",
        "        super().__init__()\n",
        "        self.net = nn.Linear(16, 10)\n",
        "        self.rand_number = torch.tensor(rand_number)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x) + self.rand_number\n",
        "\n",
        "class ModelDropout(nn.Module):\n",
        "    def __init__(self, activation: nn.Module, dropout_p: float):\n",
        "        super().__init__()\n",
        "        self.activation = activation\n",
        "        self.dropout_p = dropout_p\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(16, 10),\n",
        "            activation(),\n",
        "            nn.Dropout(p=dropout_p),\n",
        "            nn.Linear(10, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "#Action functions\n",
        "def model_action_default(rand_number: int, device: torch.device):\n",
        "    return ModelDefault(rand_number=rand_number).to(device)\n",
        "\n",
        "def model_action_dropout(activation, dropout_p, device: torch.device):\n",
        "    return ModelDropout(activation=activation, dropout_p=dropout_p).to(device)\n",
        "\n",
        "def dls_action_default(batch_size: int, device: torch.device):\n",
        "    return {'batch_size': batch_size, 'device': device}\n",
        "\n",
        "def dls_action_normalize(batch_size: int, normalize: bool):\n",
        "    return {'batch_size': batch_size, 'normalize': normalize}\n",
        "\n",
        "def dls_action_activation(batch_size: int, normalize: bool, activation: nn.Module):\n",
        "    return {'batch_size': batch_size, 'normalize': normalize, 'activation': activation}\n",
        "\n",
        "class TestActionSampler(unittest.TestCase):\n",
        "    def setUp(self):\n",
        "        # Initialize UserConfigSpace\n",
        "        self.cs = UserConfigSpace()\n",
        "\n",
        "        # Define action hyperparameters\n",
        "        self.cs.set_categorical(\n",
        "            var_type='model_action',\n",
        "            choices=[model_action_default, model_action_dropout],\n",
        "            action=True\n",
        "        )\n",
        "        self.cs.set_categorical(\n",
        "            var_type='dls_action',\n",
        "            choices=[dls_action_default, dls_action_normalize, dls_action_activation],\n",
        "            action=True\n",
        "        )\n",
        "\n",
        "        #Define signature parameters for model_action\n",
        "        self.cs.set_int(\n",
        "            var_type='rand_number',\n",
        "            low=1,\n",
        "            high=10,\n",
        "            belongs_to='model_action'\n",
        "        )\n",
        "        self.cs.set_categorical(\n",
        "            var_type='activation',\n",
        "            choices=[nn.ReLU, nn.PReLU, nn.SiLU],\n",
        "            belongs_to='model_action'\n",
        "        )\n",
        "        self.cs.set_float(\n",
        "            var_type='dropout_p',\n",
        "            low=0.0,\n",
        "            high=1.0,\n",
        "            belongs_to='model_action'\n",
        "        )\n",
        "\n",
        "        # Define signature parameters for dls_action\n",
        "        self.cs.set_int(\n",
        "            var_type='batch_size',\n",
        "            low=16,\n",
        "            high=64,\n",
        "            belongs_to='dls_action'\n",
        "        )\n",
        "        self.cs.set_categorical(\n",
        "            var_type='normalize',\n",
        "            choices=[True, False],\n",
        "            belongs_to='dls_action'\n",
        "        )\n",
        "        self.cs.set_categorical(\n",
        "            var_type='activation',\n",
        "            choices=[nn.ReLU, nn.SiLU],\n",
        "            belongs_to='dls_action'\n",
        "        )\n",
        "\n",
        "        # Initialize ActionSampler with UserConfigSpace\n",
        "        self.action_sampler = ActionSampler(self.cs)\n",
        "\n",
        "    def test_initialization(self):\n",
        "        #Test that ActionSampler initializes correctly\n",
        "        self.assertEqual(len(self.action_sampler), 2)\n",
        "        self.assertIn('model_action', self.action_sampler.keys())\n",
        "        self.assertIn('dls_action', self.action_sampler.keys())\n",
        "        self.assertIn('model_action', self.action_sampler.model_var_types)\n",
        "        self.assertNotIn('dls_action', self.action_sampler.model_var_types)\n",
        "    def test_initialization(self):\n",
        "        # Test that ActionSampler initializes correctly\n",
        "        self.assertEqual(len(self.action_sampler), 2)\n",
        "        self.assertEqual(\n",
        "            self.action_sampler.keys(),\n",
        "            {'model_action', 'dls_action'}\n",
        "        )\n",
        "        self.assertEqual(\n",
        "            self.action_sampler.model_var_types,\n",
        "            {'model_action'}\n",
        "        )\n",
        "\n",
        "    def test_sample_actions(self):\n",
        "        #Test sampling model_action\n",
        "        trial = optuna.create_study(direction='minimize').ask()\n",
        "        model_action = self.action_sampler.sample('model_action', trial)\n",
        "        self.assertIsInstance(model_action, ModelAction)\n",
        "        self.assertIsNotNone(model_action.params)\n",
        "        self.assertIsNotNone(model_action._model_loader)\n",
        "\n",
        "        #Test sampling dls_action\n",
        "        trial = optuna.create_study(direction='minimize').ask()\n",
        "        dls_action = self.action_sampler.sample('dls_action', trial)\n",
        "        self.assertIsInstance(dls_action, Action)\n",
        "        self.assertIsNotNone(dls_action.params)\n",
        "        self.assertIsNotNone(dls_action._model_loader)\n",
        "\n",
        "    def test_get_evaluated_params(self):\n",
        "        #Create a source with both model_action and dls_action parameters\n",
        "        source_params = {\n",
        "            ('model_action', 'None'): 0,\n",
        "            ('rand_number', 'model_action'): 5,\n",
        "            ('dls_action', 'None'): 2,\n",
        "            ('batch_size', 'dls_action'): 32,\n",
        "            ('normalize', 'dls_action'): 1, #False\n",
        "            ('activation', 'dls_action'): 0  # nn.ReLU\n",
        "        }\n",
        "        source_params_two = {\n",
        "            ('model_action', 'None'): 1,\n",
        "            ('activation', 'model_action'): 2, #nn.SiLU\n",
        "            ('dropout_p', 'model_action'): 0.5,\n",
        "            ('dls_action', 'None'): 1,\n",
        "            ('batch_size', 'dls_action'): 48,\n",
        "            ('normalize', 'dls_action'): 0, #True\n",
        "        }\n",
        "        list_params = [source_params, source_params_two]\n",
        "\n",
        "        for count,source_params in enumerate(list_params):\n",
        "            #Exclude 'model_action' and get evaluated params\n",
        "            evaluated_params = self.action_sampler.get_evaluated_params(source_params, exclude='model_action')\n",
        "            self.assertIn(('dls_action', 'None'), evaluated_params, msg=f\"action_sampler.var_types: {self.action_sampler.keys()}\")\n",
        "            dls_action = evaluated_params[('dls_action', 'None')]\n",
        "            self.assertIsInstance(dls_action, Action)\n",
        "            self.assertNotIn(('model_action', 'None'), evaluated_params)\n",
        "            if count==0:\n",
        "                d = {'batch_size': 32, 'normalize': False, 'activation': nn.ReLU}\n",
        "                self.assertEqual(\n",
        "                    dls_action(device=None),\n",
        "                    d,\n",
        "                    msg=f\"dls_action(): {dls_action(device=None)} != \\nExpected: {d}\"\n",
        "                )\n",
        "            else:\n",
        "                d = {'batch_size': 48, 'normalize': True}\n",
        "                self.assertEqual(\n",
        "                    dls_action(device=None),\n",
        "                    d,\n",
        "                    msg=f\"dls_action(): {dls_action(device=None)} != \\nExpected: {d}\"\n",
        "                )\n",
        "\n",
        "            #Exclude 'dls_action' and get evaluated params\n",
        "            evaluated_params = self.action_sampler.get_evaluated_params(\n",
        "                source_params,\n",
        "                exclude='dls_action'\n",
        "            )\n",
        "            self.assertIn(('model_action', 'None'), evaluated_params)\n",
        "            model_action = evaluated_params[('model_action', 'None')]\n",
        "            self.assertIsInstance(model_action, ModelAction)\n",
        "            self.assertNotIn(('dls_action', 'None'), evaluated_params)\n",
        "            if count==0:\n",
        "                d = {'rand_number': 5}\n",
        "                self.assertEqual(\n",
        "                    model_action._model_loader.keywords,\n",
        "                    d,\n",
        "                    msg=f\"model_action._model_loader.keywords: {model_action._model_loader.keywords} != \\nExpected: {d}\"\n",
        "                )\n",
        "            else:\n",
        "                d = {'dropout_p': 0.5, 'activation':nn.SiLU}\n",
        "                self.assertEqual(\n",
        "                    model_action._model_loader.keywords,\n",
        "                    d,\n",
        "                    msg = f\"model_action._model_loader.keywords: {model_action._model_loader.keywords} != \\nExpected: {d}\"\n",
        "                )\n",
        "\n",
        "    def test_update_method(self):\n",
        "        #Prepare list_params with mixed model_action and dls_action parameters\n",
        "        list_params = [\n",
        "            # Entry 1:\n",
        "            {\n",
        "                ('model_action', 'None'): 0,  # model_action_default\n",
        "                ('rand_number', 'model_action'): 5,\n",
        "\n",
        "                ('dls_action', 'None'): 1,  # dls_action_normalize\n",
        "                ('batch_size', 'dls_action'): 32,\n",
        "                ('normalize', 'dls_action'): 1,  # False\n",
        "\n",
        "                ('lr', 'None'): 0.01  # non_action_var_type\n",
        "            },\n",
        "            # Entry 2:\n",
        "            {\n",
        "                ('model_action', 'None'): 1,  # model_action_dropout\n",
        "                ('activation', 'model_action'): 1,  # nn.PReLU\n",
        "                ('dropout_p', 'model_action'): 0.5,\n",
        "\n",
        "                ('dls_action', 'None'): 2,  # dls_action_activation\n",
        "                ('batch_size', 'dls_action'): 64,\n",
        "                ('normalize', 'dls_action'): 1,  # False\n",
        "                ('activation', 'dls_action'): 1,  # nn.SiLU\n",
        "\n",
        "                ('lr', 'None'): 0.001  # non_action_var_type\n",
        "            },\n",
        "            # Entry 3:\n",
        "            {\n",
        "                ('model_action', 'None'): 1,  # model_action_dropout\n",
        "                ('activation', 'model_action'): 0,  # nn.ReLU\n",
        "                ('dropout_p', 'model_action'): 0.23,\n",
        "\n",
        "                ('dls_action', 'None'): 0,  # dls_action_default\n",
        "                ('batch_size', 'dls_action'): 32,\n",
        "\n",
        "                ('lr', 'None'): 0.988  # non_action_var_type\n",
        "            },\n",
        "            # Entry 4:\n",
        "            {\n",
        "                ('dls_action', 'None'): 2,  # dls_action_activation\n",
        "                ('batch_size', 'dls_action'): 64,\n",
        "                ('normalize', 'dls_action'): 1,  # False\n",
        "                ('activation', 'dls_action'): 0  # nn.ReLU\n",
        "            },\n",
        "            # Entry 5:\n",
        "            {\n",
        "                ('model_action', 'None'): 0,  # model_action_default\n",
        "                ('rand_number', 'model_action'): 8,\n",
        "                ('lr', 'None'): 0.02\n",
        "            }\n",
        "        ]\n",
        "\n",
        "        #Call update with update_state_window=1\n",
        "        self.action_sampler.update(list_params, update_state_window=1, device=torch.device('cpu'))\n",
        "\n",
        "        #Verify that the actions have been updated correctly\n",
        "        #For model_action\n",
        "        model_action_var = self.action_sampler.get_var('model_action')\n",
        "        self.assertEqual(model_action_var.distribution(indices=True), [0,0,1,1])\n",
        "\n",
        "        #For dls_action\n",
        "        dls_action_var = self.action_sampler.get_var('dls_action')\n",
        "        self.assertEqual(dls_action_var.distribution(indices=True), [0,1,2,2])\n",
        "\n",
        "        #Determine which model action should have updated state\n",
        "        #The unique action indices in the order they appear are [0, 1]\n",
        "        #Since update_state_window=1, only the first action (index 0) should have its state updated\n",
        "\n",
        "        model_action_default = model_action_var.get_action(0)\n",
        "        model_action_dropout = model_action_var.get_action(1)\n",
        "\n",
        "        #Verify that only the first model action has updated action_state\n",
        "        self.assertIsNotNone(model_action_default.action_state)\n",
        "        self.assertIsNone(model_action_dropout.action_state)\n",
        "\n",
        "        #Verify that dls actions do not have action_state updated\n",
        "        for idx in [0, 1, 2]:\n",
        "            dls_action = dls_action_var.get_action(idx)\n",
        "            self.assertIsInstance(dls_action, Action)\n",
        "            self.assertIsNone(dls_action.action_state)\n",
        "\n",
        "        #Additionally, we can verify the updated variables\n",
        "        #For model_action_default\n",
        "        rand_number_var = model_action_var.get_var(0, 'rand_number')\n",
        "        self.assertEqual(rand_number_var.distribution(), {'low':5, 'high':8, 'step':1, 'log':False})\n",
        "\n",
        "        #For model_action_dropout\n",
        "        activation_var = model_action_var.get_var(1, 'activation')\n",
        "        self.assertEqual(activation_var.distribution(), [nn.ReLU, nn.PReLU])\n",
        "\n",
        "        dropout_p_var = model_action_var.get_var(1, 'dropout_p')\n",
        "        self.assertEqual(dropout_p_var._dist['low'], 0.23)\n",
        "        self.assertEqual(dropout_p_var._dist['high'], 0.5)\n",
        "\n",
        "        for i in range(0,3):\n",
        "            batch_size_var = dls_action_var.get_var(i, 'batch_size')\n",
        "            self.assertEqual(batch_size_var._dist['low'], 32)\n",
        "            self.assertEqual(batch_size_var._dist['high'], 64)\n",
        "\n",
        "        for i in range(1,3):\n",
        "            normalize_var = dls_action_var.get_var(i, 'normalize')\n",
        "            self.assertEqual(normalize_var.distribution(), [False])\n",
        "\n",
        "        for i in range(2,3):\n",
        "            activation_var = dls_action_var.get_var(i, 'activation')\n",
        "            self.assertEqual(activation_var.distribution(), [nn.ReLU, nn.SiLU])\n",
        "\n",
        "    def test_update_dls_action(self):\n",
        "        #Prepare list_params with only dls_action parameters\n",
        "        list_params = [\n",
        "            # Entry 1: dls_action_default, batch_size=32\n",
        "            {\n",
        "                ('dls_action', 'None'): 0,\n",
        "                ('batch_size', 'dls_action'): 32\n",
        "            },\n",
        "            # Entry 2: dls_action_normalize, batch_size=48, normalize=True\n",
        "            {\n",
        "                ('dls_action', 'None'): 1,\n",
        "                ('batch_size', 'dls_action'): 48,\n",
        "                ('normalize', 'dls_action'): 0  # True\n",
        "            },\n",
        "            # Entry 3: dls_action_activation, batch_size=64, normalize=False, activation=nn.SiLU\n",
        "            {\n",
        "                ('dls_action', 'None'): 2,\n",
        "                ('batch_size', 'dls_action'): 64,\n",
        "                ('normalize', 'dls_action'): 1,  # False\n",
        "                ('activation', 'dls_action'): 1  # nn.SiLU\n",
        "            }\n",
        "        ]\n",
        "\n",
        "        #Call update with update_state_window=1 (should be ignored for dls_action)\n",
        "        self.action_sampler.update(list_params, update_state_window=1, device=torch.device('cpu'))\n",
        "\n",
        "        #Verify that the dls actions have updated configuration spaces\n",
        "        dls_action_var = self.action_sampler.get_var('dls_action')\n",
        "\n",
        "        batch_size_var = dls_action_var.get_var(0, 'batch_size')\n",
        "        self.assertEqual(batch_size_var._dist['low'], 32)\n",
        "        self.assertEqual(batch_size_var._dist['high'], 64)\n",
        "\n",
        "        normalize_var = dls_action_var.get_var(1, 'normalize')\n",
        "        self.assertEqual(normalize_var.distribution(), [True, False])\n",
        "\n",
        "        activation_var = dls_action_var.get_var(2, 'activation')\n",
        "        self.assertEqual(activation_var.distribution(), [nn.SiLU])\n",
        "\n",
        "        # Verify that action_state is not updated for dls actions\n",
        "        for idx in [0, 1, 2]:\n",
        "            action = dls_action_var.get_action(idx)\n",
        "            self.assertIsNone(action.action_state)\n",
        "\n",
        "    def test_update_model_action(self):\n",
        "        # Prepare list_params with only model_action parameters\n",
        "        list_params = [\n",
        "            # Entry 1: model_action_default, rand_number=5\n",
        "            {\n",
        "                ('model_action', 'None'): 0,\n",
        "                ('rand_number', 'model_action'): 5,\n",
        "                ('lr', 'None'): 0.01\n",
        "            },\n",
        "            # Entry 2: model_action_dropout, activation=nn.SiLU, dropout_p=0.3\n",
        "            {\n",
        "                ('model_action', 'None'): 1,\n",
        "                ('activation', 'model_action'): 2,  # nn.SiLU\n",
        "                ('dropout_p', 'model_action'): 0.3,\n",
        "                ('lr', 'None'): 0.001\n",
        "            },\n",
        "            # Entry 3: model_action_default, rand_number=8\n",
        "            {\n",
        "                ('model_action', 'None'): 0,\n",
        "                ('rand_number', 'model_action'): 8,\n",
        "                ('lr', 'None'): 0.02\n",
        "            }\n",
        "        ]\n",
        "\n",
        "        # Call update with update_state_window=2\n",
        "        self.action_sampler.update(list_params, update_state_window=2, device=torch.device('cpu'))\n",
        "\n",
        "        # Verify that the model actions have updated configuration spaces\n",
        "        model_action_var = self.action_sampler.get_var('model_action')\n",
        "\n",
        "        rand_number_var = model_action_var.get_var(0, 'rand_number')\n",
        "        self.assertEqual(rand_number_var.distribution(), {'low': 5, 'high': 8, 'log': False, 'step': 1})\n",
        "\n",
        "        activation_var = model_action_var.get_var(1, 'activation')\n",
        "        self.assertEqual(activation_var.distribution(), [nn.SiLU])\n",
        "\n",
        "        dropout_p_var = model_action_var.get_var(1, 'dropout_p')\n",
        "        self.assertEqual(dropout_p_var._dist['low'], 0.3)\n",
        "        self.assertEqual(dropout_p_var._dist['high'], 0.3)\n",
        "\n",
        "        # Verify that action_state is updated for model actions\n",
        "        model_action_default = model_action_var.get_action(0)\n",
        "        model_action_dropout = model_action_var.get_action(1)\n",
        "\n",
        "        self.assertIsNotNone(model_action_default.action_state)\n",
        "        self.assertIsNotNone(model_action_dropout.action_state)\n",
        "\n",
        "    def test_update_with_mixed_actions(self):\n",
        "        # Prepare list_params with both model_action and dls_action parameters\n",
        "        list_params = [\n",
        "            # Entry 1: model_action_default, rand_number=5\n",
        "            {\n",
        "                ('model_action', 'None'): 0,\n",
        "                ('rand_number', 'model_action'): 5,\n",
        "                ('lr', 'None'): 0.01,\n",
        "                ('dls_action', 'None'): 1,\n",
        "                ('batch_size', 'dls_action'): 48,\n",
        "                ('normalize', 'dls_action'): 0  # True\n",
        "            },\n",
        "            # Entry 2: model_action_dropout, activation=nn.SiLU, dropout_p=0.3\n",
        "            {\n",
        "                ('model_action', 'None'): 1,\n",
        "                ('activation', 'model_action'): 2,  # nn.SiLU\n",
        "                ('dropout_p', 'model_action'): 0.3,\n",
        "                ('lr', 'None'): 0.001,\n",
        "                ('dls_action', 'None'): 2,\n",
        "                ('batch_size', 'dls_action'): 64,\n",
        "                ('normalize', 'dls_action'): 1,  # False\n",
        "                ('activation', 'dls_action'): 1  # nn.SiLU\n",
        "            }\n",
        "        ]\n",
        "\n",
        "        # Call update with update_state_window=1\n",
        "        self.action_sampler.update(list_params, update_state_window=1, device=torch.device('cpu'))\n",
        "\n",
        "        # Verify that the model actions have updated states\n",
        "        model_action_var = self.action_sampler.get_var('model_action')\n",
        "        model_action_default = model_action_var.get_action(0)\n",
        "        model_action_dropout = model_action_var.get_action(1)\n",
        "\n",
        "        self.assertIsNotNone(model_action_default.action_state)\n",
        "        self.assertIsNone(model_action_dropout.action_state)  # Since update_state_window=1\n",
        "\n",
        "        # Verify that dls actions do not have action_state updated\n",
        "        dls_action_var = self.action_sampler.get_var('dls_action')\n",
        "        for idx in [0, 1, 2]:\n",
        "            action = dls_action_var.get_action(idx)\n",
        "            self.assertIsNone(action.action_state)\n",
        "\n",
        "        #Verify that get_evaluated_params correctly assembles actions\n",
        "        #When updating model_action, dls_action should be prepared\n",
        "        #This is tested by checking if the dls_action is included in the parameters passed to Objective\n",
        "        #(In a real implementation, the Objective function would use these actions)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    unittest.main(argv=['first-arg-is-ignored'], exit=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MhawB6SNkofc",
        "outputId": "a5cee49b-13aa-4903-e8b5-dc532bdce4a1"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-36-2e3739082d59>:193: UserWarning: Action hyperparameter 'model_action' requires additional hyperparameters: [('dropout_p', 'model_action'), ('rand_number', 'model_action'), ('activation', 'model_action')]\n",
            "\n",
            "  warnings.warn(msg)\n",
            "<ipython-input-36-2e3739082d59>:193: UserWarning: Action hyperparameter 'model_action' requires additional hyperparameters: [('dropout_p', 'model_action'), ('rand_number', 'model_action'), ('activation', 'model_action')]\n",
            "Action hyperparameter 'dls_action' requires additional hyperparameters: [('batch_size', 'dls_action'), ('activation', 'dls_action'), ('normalize', 'dls_action')]\n",
            "\n",
            "  warnings.warn(msg)\n",
            "<ipython-input-36-2e3739082d59>:193: UserWarning: Action hyperparameter 'model_action' requires additional hyperparameters: [('dropout_p', 'model_action'), ('activation', 'model_action')]\n",
            "Action hyperparameter 'dls_action' requires additional hyperparameters: [('batch_size', 'dls_action'), ('activation', 'dls_action'), ('normalize', 'dls_action')]\n",
            "\n",
            "  warnings.warn(msg)\n",
            "<ipython-input-36-2e3739082d59>:193: UserWarning: Action hyperparameter 'model_action' requires additional hyperparameters: [('dropout_p', 'model_action')]\n",
            "Action hyperparameter 'dls_action' requires additional hyperparameters: [('batch_size', 'dls_action'), ('activation', 'dls_action'), ('normalize', 'dls_action')]\n",
            "\n",
            "  warnings.warn(msg)\n",
            "<ipython-input-36-2e3739082d59>:193: UserWarning: Action hyperparameter 'dls_action' requires additional hyperparameters: [('batch_size', 'dls_action'), ('activation', 'dls_action'), ('normalize', 'dls_action')]\n",
            "\n",
            "  warnings.warn(msg)\n",
            "<ipython-input-36-2e3739082d59>:193: UserWarning: Action hyperparameter 'dls_action' requires additional hyperparameters: [('activation', 'dls_action'), ('normalize', 'dls_action')]\n",
            "\n",
            "  warnings.warn(msg)\n",
            "<ipython-input-36-2e3739082d59>:193: UserWarning: Action hyperparameter 'dls_action' requires additional hyperparameters: [('activation', 'dls_action')]\n",
            "\n",
            "  warnings.warn(msg)\n",
            "..[I 2024-10-07 01:09:06,191] A new study created in memory with name: no-name-58cfa11a-1468-45e5-ac93-e7869d7a9cef\n",
            "[I 2024-10-07 01:09:06,194] A new study created in memory with name: no-name-4daad62c-9d7f-47b2-9a58-a406b242a649\n",
            ".................[I 2024-10-07 01:09:06,250] A new study created in memory with name: no-name-7e84dcc8-a002-44de-a3e3-9c0af4403cab\n",
            ".[I 2024-10-07 01:09:06,255] A new study created in memory with name: no-name-abccd944-aaf4-4860-b3cf-664ad726d5a9\n",
            ".[I 2024-10-07 01:09:06,260] A new study created in memory with name: no-name-0c7b2892-66cf-4c0f-8847-773effd9adb6\n",
            ".[I 2024-10-07 01:09:06,266] A new study created in memory with name: no-name-dc794842-6046-4724-8518-62141bef8c3f\n",
            ".[I 2024-10-07 01:09:06,274] A new study created in memory with name: no-name-30df2f59-cc36-4195-b55c-1370736f195d\n",
            ".[I 2024-10-07 01:09:06,278] A new study created in memory with name: no-name-8940063f-e394-4bcd-81e0-e15a5c224975\n",
            ".[I 2024-10-07 01:09:06,283] A new study created in memory with name: no-name-5e360105-0ac2-4bb4-b59c-b316be174881\n",
            "..<ipython-input-36-2e3739082d59>:193: UserWarning: Action hyperparameter 'optimizer' requires additional hyperparameters: [('param4', 'optimizer'), ('param2', 'optimizer'), ('param3', 'optimizer'), ('param1', 'optimizer')]\n",
            "\n",
            "  warnings.warn(msg)\n",
            "<ipython-input-36-2e3739082d59>:193: UserWarning: Action hyperparameter 'optimizer' requires additional hyperparameters: [('param4', 'optimizer'), ('param2', 'optimizer'), ('param3', 'optimizer')]\n",
            "\n",
            "  warnings.warn(msg)\n",
            "<ipython-input-36-2e3739082d59>:193: UserWarning: Action hyperparameter 'optimizer' requires additional hyperparameters: [('param4', 'optimizer'), ('param3', 'optimizer')]\n",
            "\n",
            "  warnings.warn(msg)\n",
            "<ipython-input-36-2e3739082d59>:193: UserWarning: Action hyperparameter 'optimizer' requires additional hyperparameters: [('param4', 'optimizer')]\n",
            "\n",
            "  warnings.warn(msg)\n",
            ".<ipython-input-36-2e3739082d59>:193: UserWarning: Action hyperparameter 'optimizer' requires additional hyperparameters: [('param1', 'optimizer')]\n",
            "\n",
            "  warnings.warn(msg)\n",
            "..<ipython-input-36-2e3739082d59>:193: UserWarning: Action hyperparameter 'optimizer' requires additional hyperparameters: [('param2', 'optimizer'), ('param1', 'optimizer')]\n",
            "\n",
            "  warnings.warn(msg)\n",
            "<ipython-input-36-2e3739082d59>:193: UserWarning: Action hyperparameter 'optimizer' requires additional hyperparameters: [('param2', 'optimizer')]\n",
            "\n",
            "  warnings.warn(msg)\n",
            ".................<ipython-input-36-2e3739082d59>:193: UserWarning: Action hyperparameter 'optimizer_evens' requires additional hyperparameters: [('param4', 'optimizer_evens'), ('param2', 'optimizer_evens'), ('param6', 'optimizer_evens')]\n",
            "\n",
            "  warnings.warn(msg)\n",
            "<ipython-input-36-2e3739082d59>:193: UserWarning: Action hyperparameter 'optimizer_evens' requires additional hyperparameters: [('param4', 'optimizer_evens'), ('param6', 'optimizer_evens')]\n",
            "\n",
            "  warnings.warn(msg)\n",
            "<ipython-input-36-2e3739082d59>:193: UserWarning: Action hyperparameter 'optimizer_evens' requires additional hyperparameters: [('param4', 'optimizer_evens')]\n",
            "\n",
            "  warnings.warn(msg)\n",
            "<ipython-input-36-2e3739082d59>:193: UserWarning: Action hyperparameter 'optimizer_evens' requires additional hyperparameters: [('param3', 'optimizer_evens'), ('param1', 'optimizer_evens'), ('param5', 'optimizer_evens')]\n",
            "\n",
            "  warnings.warn(msg)\n",
            "...........................<ipython-input-36-2e3739082d59>:193: UserWarning: Action hyperparameter 'optimizer2' requires additional hyperparameters: [('param4', 'optimizer2'), ('param3', 'optimizer2')]\n",
            "\n",
            "  warnings.warn(msg)\n",
            "<ipython-input-36-2e3739082d59>:193: UserWarning: Action hyperparameter 'optimizer2' requires additional hyperparameters: [('param4', 'optimizer2')]\n",
            "\n",
            "  warnings.warn(msg)\n",
            ".............[I 2024-10-07 01:09:06,432] A new study created in memory with name: no-name-ffa0531a-af91-4f28-ad4a-a38f67a08091\n",
            "..[I 2024-10-07 01:09:06,436] A new study created in memory with name: no-name-35df8e1a-264d-454d-a836-4e43127307f9\n",
            ".....\n",
            "----------------------------------------------------------------------\n",
            "Ran 94 tests in 0.264s\n",
            "\n",
            "OK\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Class: PrimitiveSampler"
      ],
      "metadata": {
        "id": "EqVpt_tceM-g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PrimitiveSampler:\n",
        "    def __init__(self, INIT_PRIM: ConfigSpace):\n",
        "        if not isinstance(INIT_PRIM, ConfigSpace):\n",
        "            raise ValueError(f\"Input 'INIT_PRIM' must be an instance of ConfigSpace but found type(INIT_PRIM): {type(INIT_PRIM)}\")\n",
        "        self.vars = {var.var_type: var for var in INIT_PRIM.get_non_action_vars()}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.vars)\n",
        "\n",
        "    def __eq__(self, other: PrimitiveSampler):\n",
        "        return isinstance(other, PrimitiveSampler) and self.vars == other.vars\n",
        "\n",
        "    def __str__(self):\n",
        "        output = \"**********PrimitiveSampler**********\\n\"\n",
        "        output += \"\".join([f\"{var}\\n\" for var in self.vars.values()])\n",
        "        return output\n",
        "\n",
        "    def get_var(self, var_type):\n",
        "        if var_type not in self.keys():\n",
        "            raise KeyError(f\"Requested 'var_type': {var_type} is not in PrimitiveSampler.keys(): {self.keys()}.\")\n",
        "        return self.vars[var_type]\n",
        "\n",
        "    def keys(self):\n",
        "        return self.vars.keys()\n",
        "\n",
        "    def values(self):\n",
        "        return self.vars.values()\n",
        "\n",
        "    def items(self):\n",
        "        return self.vars.items()\n",
        "\n",
        "    def is_fixed(self):\n",
        "        return all(var.is_fixed() for var in self.values())\n",
        "\n",
        "    def get_evaluated_params(self, source):\n",
        "        if not isinstance(source, (Trial, FrozenTrial, dict)):\n",
        "            raise ValueError(f\"Input 'source' must be of type in (optuna::Trial, optuna::FrozenTrial, dict) but found: {source}\")\n",
        "\n",
        "        source = source if isinstance(source, dict) else source.params\n",
        "\n",
        "        params = {\n",
        "            (var_type,belongs_to):val for (var_type,belongs_to),val in source.items()\n",
        "            if var_type in self.keys() and belongs_to=='None'\n",
        "        }\n",
        "\n",
        "        for key,val in params.items():\n",
        "            var = self.get_var(key[0])\n",
        "            if var.contain_idx(val):\n",
        "                val = var.convert_idx_to_val(val)\n",
        "            params[key] = val\n",
        "\n",
        "        return params\n",
        "\n",
        "    def sample(self, var_type: str, trial):\n",
        "        if trial is not None and not isinstance(trial, (Trial, FrozenTrial)):\n",
        "            raise ValueError(f\"Input 'trial' should be an instance of optuna.Trial or optuna.FrozenTrial but found: {trial}\")\n",
        "        if trial is None:\n",
        "            raise ValueError(\"Input 'trial' cannot be 'None'. Please provide an optuna.Trial or optuna.FrozenTrial instance.\")\n",
        "        if var_type not in self.keys():\n",
        "            raise KeyError(f\"Requested var_type: {var_type} does not exist in the PrimitiveSampler.\")\n",
        "\n",
        "        #sample value from the variable of type 'var_type'.\n",
        "        sampled_val = self.get_var(var_type).sample(trial)\n",
        "\n",
        "        return sampled_val\n",
        "\n",
        "    def update_vars(self, list_params: list):\n",
        "        #Update variables based on the list of parameters\n",
        "        for var_type, var in self.items():\n",
        "            var.update(vals=[params[(var_type, 'None')] for params in list_params if (var_type, 'None') in params.keys()])"
      ],
      "metadata": {
        "id": "rIdylV13-qhH"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Class: CombinedSampler"
      ],
      "metadata": {
        "id": "aT-F4CO5z0CH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CombinedSampler:\n",
        "    def __init__(self, INIT_CONFIG: ConfigSpace):\n",
        "        if not isinstance(INIT_CONFIG, ConfigSpace):\n",
        "            raise ValueError(f\"Input 'INIT_CONFIG' must be of type in ConfigSpace but found type(INIT_CONFIG): {type(INIT_CONFIG)}\")\n",
        "        self.action_sampler = ActionSampler(INIT_CONFIG)\n",
        "        self.primitive_sampler = PrimitiveSampler(INIT_CONFIG)\n",
        "        self._sig_var_types = INIT_CONFIG.sig_var_types()\n",
        "\n",
        "    def __str__(self):\n",
        "        output = \"CombinedSampler:\\n\"\n",
        "        output += f\"{self.action_sampler}\\n\"\n",
        "        return output + f\"{self.primitive_sampler}\\n\"\n",
        "\n",
        "    def is_fixed(self):\n",
        "        return self.primitive_sampler.is_fixed() and self.action_sampler.is_fixed()\n",
        "\n",
        "    def get_var(self, var_type: str, *args, **kwargs) -> Var:\n",
        "        if var_type in self.non_action_var_types():\n",
        "            return self.primitive_sampler.get_var(var_type)\n",
        "        return self.action_sampler.get_var(var_type, *args, **kwargs)\n",
        "\n",
        "    def get_action(self, action_var_type: str, action_idx_or_action_func) -> Action:\n",
        "        return self.action_sampler.get_action(action_var_type, action_idx_or_action_func)\n",
        "\n",
        "    def non_action_var_types(self):\n",
        "        return set(self.primitive_sampler.keys())\n",
        "\n",
        "    def action_var_types(self):\n",
        "        return set(self.action_sampler.keys())\n",
        "\n",
        "    def model_action_var_types(self):\n",
        "        return self.action_sampler.model_var_types\n",
        "\n",
        "    def sig_var_types(self, action_var_type: str=None, action_idx_or_action_func=None):\n",
        "        if action_var_type is None and action_idx_or_action_func is None:\n",
        "            return self._sig_var_types\n",
        "        elif action_var_type is None or action_idx_or_action_func is None:\n",
        "            raise KeyError(\"Either 'action_var_type' and 'action_idx_or_action_func' must be provided or both must be None.\")\n",
        "        return set(self.get_action(action_var_type, action_idx_or_action_func).keys())\n",
        "\n",
        "    def var_types(self):\n",
        "        return set(self.primitive_sampler.keys()) | set(self.action_sampler.keys())\n",
        "\n",
        "    def contain_idx(self, *args, idx):\n",
        "        return self.get_var(*args).contain_idx(idx)\n",
        "\n",
        "    def convert_idx_to_val(self, *args, idx):\n",
        "        return self.get_var(*args).convert_idx_to_val(idx)\n",
        "\n",
        "    def contain_val(self, *args, val):\n",
        "        return self.get_var(*args).contain_val(val)\n",
        "\n",
        "    def convert_val_to_idx(self, *args, val):\n",
        "        return self.get_var(*args).convert_val_to_idx(val)\n",
        "\n",
        "    def sample(self, var_type: str, trial: Trial, default_val = None):\n",
        "        \"\"\"\n",
        "        For proper sampling, 'trial' must be an instance of optuna.Trial or optuna.FrozenTrial.\n",
        "        If 'trial' is None, then 'default_val' will be returned.\n",
        "        \"\"\"\n",
        "        if var_type in self._sig_var_types:\n",
        "            raise KeyError(f\"Requested var_type: {var_type} is a signature var_type, which is not allowed to be sampled directly.\")\n",
        "        if var_type not in self.var_types():\n",
        "            raise KeyError(f\"Requested var_type: {var_type} does not exist in the CombinedSampler.\")\n",
        "        var = self.get_var(var_type)\n",
        "        valid_trial = trial is not None and isinstance(trial, (Trial, FrozenTrial))\n",
        "\n",
        "        #Any 'default_val' must be in the current distribution of the var_type\n",
        "        if not valid_trial and not var.contain_val(default_val):\n",
        "            raise ValueError(f\"Var of type: {var_type} cannot produce the given 'default_val': {default_val} since its current distribution is: {var.distribution()}\")\n",
        "        if not valid_trial:\n",
        "            #Return 'default_val' if 'trial' is invalid but 'default_val' is valid\n",
        "            return default_val\n",
        "\n",
        "        sample_from = self.action_sampler if var_type in self.action_var_types() else self.primitive_sampler\n",
        "        return sample_from.sample(var_type, trial)\n",
        "\n",
        "    def update(self, top_k_trials: list, update_state_window: int, device: torch.device):\n",
        "        #Empty list for top_k_trials is not allowed. (Auto-HPO should have been finished before any empty-update is justifiable).\n",
        "        if not len(top_k_trials):\n",
        "            raise ValueError(\"Input 'top_k_trials' cannot be empty\")\n",
        "\n",
        "        #Convert trial params to the required format\n",
        "        list_params = [{tuple(key.split('>')): val for key, val in trial.params.items()} for trial in top_k_trials]\n",
        "\n",
        "        #Update primitive_sampler\n",
        "        self.primitive_sampler.update_vars(list_params=list_params)\n",
        "\n",
        "        #Update list_params by evaluating non-action sampled idx(s) to value(s).\n",
        "        list_params = [params|self.primitive_sampler.get_evaluated_params(source=params) for params in list_params]\n",
        "\n",
        "        #Update action_sampler\n",
        "        self.action_sampler.update(\n",
        "            list_params=list_params,\n",
        "            update_state_window=update_state_window,\n",
        "            device=device\n",
        "        )"
      ],
      "metadata": {
        "id": "ZSmG7u4m0l6C"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Test: CombinedSampler\n",
        "\n"
      ],
      "metadata": {
        "id": "YbHQetChF_yV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Assuming Var, Action, ModelAction, and ActionVar are defined and imported\n",
        "#Also assuming that CombinedSampler class is defined as per the improved code\n",
        "\n",
        "#Fake Objective class\n",
        "class Objective:\n",
        "    def __init__(self, trial, combined_sampler, device, default_dict):\n",
        "        self.trial = trial\n",
        "        self.combined_sampler = combined_sampler\n",
        "        self.device = device\n",
        "        self.default_dict = default_dict\n",
        "\n",
        "    def execute_objective(self):\n",
        "        model = self.default_dict['model_action'](device=self.device)\n",
        "\n",
        "        for param in model.parameters():\n",
        "            param.data.fill_(0)\n",
        "\n",
        "        model_container = Mock(spec=Learner)\n",
        "        model_container.model = model\n",
        "\n",
        "        return (0, model)\n",
        "\n",
        "#Sample model classes\n",
        "class ModelDefault(nn.Module):\n",
        "    def __init__(self, rand_number: int):\n",
        "        super().__init__()\n",
        "        self.net = nn.Linear(16, 10)\n",
        "        self.rand_number = torch.tensor(rand_number)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x) + self.rand_number\n",
        "\n",
        "class ModelDropout(nn.Module):\n",
        "    def __init__(self, activation: nn.Module, dropout_p: float):\n",
        "        super().__init__()\n",
        "        self.activation = activation\n",
        "        self.dropout_p = dropout_p\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(16, 10),\n",
        "            activation(),\n",
        "            nn.Dropout(p=dropout_p),\n",
        "            nn.Linear(10, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class ModelDropoutVariant(nn.Module):\n",
        "    def __init__(self, rand_number: int, activation: nn.Module, dropout_p: int):\n",
        "        super().__init__()\n",
        "        self.rand_number = rand_number\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_features=16, out_features=10),\n",
        "            activation(),\n",
        "            nn.Dropout(p=dropout_p),\n",
        "            nn.Linear(in_features=10, out_features=10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# Action functions\n",
        "def model_action_default(rand_number: int, device: torch.device):\n",
        "    return ModelDefault(rand_number=rand_number).to(device)\n",
        "\n",
        "def model_action_dropout(activation, dropout_p, device: torch.device):\n",
        "    return ModelDropout(activation=activation, dropout_p=dropout_p).to(device)\n",
        "\n",
        "def model_action_dropout_variant(rand_number: int, activation, dropout_p, device: torch.device):\n",
        "    return ModelDropoutVariant(rand_number=rand_number, activation=activation, dropout_p=dropout_p).to(device)\n",
        "\n",
        "def dls_action_default(batch_size: int, device: torch.device):\n",
        "    return {'batch_size': batch_size, 'device': device}\n",
        "\n",
        "def dls_action_normalize(batch_size: int, normalize: bool):\n",
        "    return {'batch_size': batch_size, 'normalize': normalize}\n",
        "\n",
        "def dls_action_activation(batch_size: int, normalize: bool, activation: nn.Module):\n",
        "    return {'batch_size': batch_size, 'normalize': normalize, 'activation': activation}\n",
        "\n",
        "class TestCombinedSampler(unittest.TestCase):\n",
        "    def setUp(self):\n",
        "        # Initialize UserConfigSpace\n",
        "        self.cs = UserConfigSpace()\n",
        "\n",
        "        # Define action hyperparameters\n",
        "        self.cs.set_categorical(\n",
        "            var_type='model_action',\n",
        "            choices=[model_action_default, model_action_dropout, model_action_dropout_variant],\n",
        "            action=True\n",
        "        )\n",
        "        self.cs.set_categorical(\n",
        "            var_type='dls_action',\n",
        "            choices=[dls_action_default, dls_action_normalize, dls_action_activation],\n",
        "            action=True\n",
        "        )\n",
        "\n",
        "        # Define signature parameters for 'model_action'\n",
        "        self.cs.set_categorical(\n",
        "            var_type='rand_number',\n",
        "            choices=list(range(1, 11)),\n",
        "            belongs_to='model_action'\n",
        "        )\n",
        "        self.cs.set_categorical(\n",
        "            var_type='activation',\n",
        "            choices=[nn.ReLU, nn.PReLU, nn.SiLU],\n",
        "            belongs_to='model_action'\n",
        "        )\n",
        "        self.cs.set_float(\n",
        "            var_type='dropout_p',\n",
        "            low=0.0,\n",
        "            high=1.0,\n",
        "            belongs_to='model_action'\n",
        "        )\n",
        "\n",
        "        # Define signature parameters for 'dls_action'\n",
        "        self.cs.set_int(\n",
        "            var_type='batch_size',\n",
        "            low=16,\n",
        "            high=64,\n",
        "            belongs_to='dls_action'\n",
        "        )\n",
        "        self.cs.set_categorical(\n",
        "            var_type='normalize',\n",
        "            choices=[True, False],\n",
        "            belongs_to='dls_action'\n",
        "        )\n",
        "        self.cs.set_categorical(\n",
        "            var_type='activation',\n",
        "            choices=[nn.ReLU, nn.SiLU],\n",
        "            belongs_to='dls_action'\n",
        "        )\n",
        "\n",
        "        # Define non-action variables\n",
        "        self.cs.set_float(\n",
        "            var_type='lr',\n",
        "            low=0.0001,\n",
        "            high=0.01,\n",
        "            log=True\n",
        "        )\n",
        "        self.cs.set_float(\n",
        "            var_type='wd',\n",
        "            low=0.0001,\n",
        "            high=0.1,\n",
        "            log=True\n",
        "        )\n",
        "\n",
        "        # Initialize CombinedSampler with UserConfigSpace\n",
        "        self.combined_sampler = CombinedSampler(self.cs)\n",
        "\n",
        "    def create_mock_trial(self, params):\n",
        "        trial = Mock(spec=optuna.trial.FrozenTrial)\n",
        "        trial.params = params\n",
        "        return trial\n",
        "\n",
        "    def test_initialization(self):\n",
        "        #Test that CombinedSampler initializes correctly\n",
        "        self.assertIsInstance(self.combined_sampler.action_sampler, ActionSampler)\n",
        "        self.assertIsInstance(self.combined_sampler.primitive_sampler, PrimitiveSampler)\n",
        "\n",
        "        #Check var_types retrieval/categorization.\n",
        "        self.assertEqual(self.combined_sampler.action_var_types(), {'model_action', 'dls_action'})\n",
        "        self.assertEqual(self.combined_sampler.non_action_var_types(), {'lr', 'wd'})\n",
        "        self.assertEqual(self.combined_sampler.sig_var_types(), {'rand_number', 'batch_size', 'normalize', 'activation', 'dropout_p'})\n",
        "        self.assertEqual(self.combined_sampler.var_types(), {'model_action', 'dls_action', 'lr', 'wd'})\n",
        "\n",
        "    def test_sample_non_action_vars(self):\n",
        "        trial = optuna.create_study(direction='minimize').ask()\n",
        "        lr = self.combined_sampler.sample('lr', trial)\n",
        "        wd = self.combined_sampler.sample('wd', trial)\n",
        "        #Assert that the sampled values are within the expected ranges\n",
        "        self.assertTrue(0.0001 <= lr <= 0.01)\n",
        "        self.assertTrue(0.0001 <= wd <= 0.1)\n",
        "\n",
        "    def test_sample_model_actions(self):\n",
        "        trial = optuna.create_study(direction='minimize').ask()\n",
        "        model_action = self.combined_sampler.sample('model_action', trial)\n",
        "        self.assertIsInstance(model_action, ModelAction)\n",
        "        self.assertIsNotNone(model_action.params)\n",
        "        self.assertIsNotNone(model_action._model_loader)\n",
        "        self.assertEqual(\n",
        "            model_action.params,\n",
        "            model_action._model_loader.keywords,\n",
        "            msg=f\"model_action.params: {model_action.params} != model_action._model_loader.keywores: {model_action._model_loader.keywords }\"\n",
        "        )\n",
        "        #Now check which action_func was sampled and assert accordingly\n",
        "        if model_action.action_func == model_action_default:\n",
        "            rand_number = model_action.params['rand_number']\n",
        "            self.assertIn(rand_number, list(range(1, 11)))\n",
        "        elif model_action.action_func == model_action_dropout:\n",
        "            activation = model_action.params['activation']\n",
        "            dropout_p = model_action.params['dropout_p']\n",
        "            self.assertIn(activation, [nn.ReLU, nn.PReLU, nn.SiLU])\n",
        "            self.assertTrue(0.0 <= dropout_p <= 1.0)\n",
        "        elif model_action.action_func == model_action_dropout_variant:\n",
        "            rand_number = model_action.params['rand_number']\n",
        "            activation = model_action.params['activation']\n",
        "            dropout_p = model_action.params['dropout_p']\n",
        "            self.assertIn(rand_number, list(range(1, 11)))\n",
        "            self.assertIn(activation, [nn.ReLU, nn.PReLU, nn.SiLU])\n",
        "            self.assertTrue(0.0 <= dropout_p <= 1.0)\n",
        "        else:\n",
        "            raise ValueError(f\"Unexpected model_action action_func: {model_action.action_func}\")\n",
        "\n",
        "    def test_sample_dls_actions(self):\n",
        "        trial = optuna.create_study(direction='minimize').ask()\n",
        "        dls_action = self.combined_sampler.sample('dls_action', trial)\n",
        "        self.assertIsInstance(dls_action, Action)\n",
        "        self.assertFalse(isinstance(dls_action, ModelAction))\n",
        "        self.assertIsNotNone(dls_action.params)\n",
        "        self.assertIsNotNone(dls_action._model_loader)\n",
        "        self.assertEqual(\n",
        "            dls_action.params,\n",
        "            dls_action._model_loader.keywords\n",
        "        )\n",
        "        #Now check which action_func was sampled and assert accordingly\n",
        "        if dls_action.action_func == dls_action_default:\n",
        "            batch_size = dls_action.params['batch_size']\n",
        "            self.assertTrue(16 <= batch_size <= 64)\n",
        "        elif dls_action.action_func == dls_action_normalize:\n",
        "            batch_size = dls_action.params['batch_size']\n",
        "            normalize = dls_action.params['normalize']\n",
        "            self.assertTrue(16 <= batch_size <= 64)\n",
        "            self.assertIn(normalize, [True, False])\n",
        "        elif dls_action.action_func == dls_action_activation:\n",
        "            batch_size = dls_action.params['batch_size']\n",
        "            normalize = dls_action.params['normalize']\n",
        "            activation = dls_action.params['activation']\n",
        "            self.assertTrue(16 <= batch_size <= 64)\n",
        "            self.assertIn(normalize, [True, False])\n",
        "            self.assertIn(activation, [nn.ReLU, nn.SiLU])\n",
        "        else:\n",
        "            raise ValueError(f\"Unexpected dls_action action_func: {dls_action.action_func}\")\n",
        "\n",
        "    def test_update(self):\n",
        "        #Prepare mock trials with various combinations\n",
        "        trial1_params = {\n",
        "            'model_action>None': 0,  #model_action_default\n",
        "            'rand_number>model_action': 5, #6\n",
        "\n",
        "            'dls_action>None': 1,  #dls_action_normalize\n",
        "            'batch_size>dls_action': 32,\n",
        "            'normalize>dls_action': 0,  #True\n",
        "\n",
        "            'lr>None': 0.005,\n",
        "            'wd>None': 0.001\n",
        "        }\n",
        "        trial2_params = {\n",
        "            'model_action>None': 1,  #model_action_dropout\n",
        "            'activation>model_action': 2,  #nn.SiLU\n",
        "            'dropout_p>model_action': 0.3,\n",
        "\n",
        "            'dls_action>None': 2,  #dls_action_activation\n",
        "            'batch_size>dls_action': 48,\n",
        "            'normalize>dls_action': 1,  #False\n",
        "            'activation>dls_action': 1,  #nn.SiLU\n",
        "\n",
        "            'lr>None': 0.002,\n",
        "            'wd>None': 0.01\n",
        "        }\n",
        "        trial3_params = {\n",
        "            'model_action>None': 2,  #model_action_dropout_variant\n",
        "            'rand_number>model_action': 7, #8\n",
        "            'activation>model_action': 0,  #nn.ReLU\n",
        "            'dropout_p>model_action': 0.5,\n",
        "\n",
        "            'dls_action>None': 0,  #dls_action_default\n",
        "            'batch_size>dls_action': 64,\n",
        "\n",
        "            'lr>None': 0.001,\n",
        "            'wd>None': 0.005\n",
        "        }\n",
        "        #Create trials\n",
        "        trial1 = self.create_mock_trial(trial1_params)\n",
        "        trial2 = self.create_mock_trial(trial2_params)\n",
        "        trial3 = self.create_mock_trial(trial3_params)\n",
        "        top_k_trials = [trial1, trial2, trial3]\n",
        "        #Call update with update_state_window=2\n",
        "        self.combined_sampler.update(top_k_trials=top_k_trials, update_state_window=2, device=torch.device('cpu'))\n",
        "        # Now verify that the variables and actions have been updated correctly\n",
        "        # For non-action variables\n",
        "        lr_var = self.combined_sampler.get_var('lr')\n",
        "        wd_var = self.combined_sampler.get_var('wd')\n",
        "        #Note that it reflects ALL the 'k' number of params in top_k_trials\n",
        "        self.assertEqual(lr_var._dist['low'], 0.001)\n",
        "        self.assertEqual(lr_var._dist['high'], 0.005)\n",
        "        self.assertEqual(wd_var._dist['low'], 0.001)\n",
        "        self.assertEqual(wd_var._dist['high'], 0.01)\n",
        "        #For model_action\n",
        "        model_action_var = self.combined_sampler.action_sampler.get_var('model_action')\n",
        "        self.assertEqual(model_action_var.distribution(indices=True), [0, 1, 2])\n",
        "        #Check action_states\n",
        "        model_action_default = model_action_var.get_action(0)\n",
        "        model_action_dropout = model_action_var.get_action(1)\n",
        "        model_action_dropout_variant = model_action_var.get_action(2)\n",
        "        self.assertIsNotNone(model_action_default.action_state)\n",
        "        self.assertIsNotNone(model_action_dropout.action_state)\n",
        "        self.assertIsNone(model_action_dropout_variant.action_state)\n",
        "        #For dls_action(action_state should not be updated)\n",
        "        dls_action_var = self.combined_sampler.action_sampler.get_var('dls_action')\n",
        "        for idx in [0, 1, 2]:\n",
        "            action = dls_action_var.get_action(idx)\n",
        "            self.assertIsNone(action.action_state)\n",
        "\n",
        "    def test_sampling_after_update(self):\n",
        "        #Prepare and update as before\n",
        "        self.test_update()\n",
        "        self.assertEqual(self.combined_sampler.get_var('model_action').distribution(True), [0,1,2])\n",
        "        self.assertEqual(self.combined_sampler.get_var('dls_action').distribution(True), [0,1,2])\n",
        "        #After updating, sample variables and check that distributions are updated\n",
        "        trial = optuna.create_study(direction='minimize').ask()\n",
        "        for var_type in self.combined_sampler.var_types():\n",
        "            val = self.combined_sampler.sample(var_type, trial)\n",
        "            if var_type=='model_action':\n",
        "                self.assertIsInstance(val, ModelAction)\n",
        "                self.assertIn(val.action_func, [model_action_default, model_action_dropout, model_action_dropout_variant])\n",
        "                self.assertEqual(val(device=None)(torch.randn(16,16)).shape, torch.Size([16,10]))\n",
        "            elif var_type=='dls_action':\n",
        "                self.assertIsInstance(val, Action)\n",
        "                self.assertIn(val.action_func, [dls_action_default, dls_action_normalize, dls_action_activation])\n",
        "            elif var_type=='lr':\n",
        "                self.assertTrue(0.001 <= val <= 0.005)\n",
        "            elif var_type=='wd':\n",
        "                self.assertTrue(0.001 <= val <= 0.01)\n",
        "\n",
        "    def test_get_evaluated_params(self):\n",
        "        #Prepare a source with both action and non-action parameters\n",
        "        source_params = {\n",
        "            ('model_action', 'None'): 2,  # model_action_dropout_variant\n",
        "            ('rand_number', 'model_action'): 7, #8\n",
        "            ('activation', 'model_action'): 1,  # nn.PReLU\n",
        "            ('dropout_p', 'model_action'): 0.4,\n",
        "\n",
        "            ('dls_action', 'None'): 2,  # dls_action_activation\n",
        "            ('batch_size', 'dls_action'): 48,\n",
        "            ('normalize', 'dls_action'): 1,  # False\n",
        "            ('activation', 'dls_action'): 0,  # nn.ReLU\n",
        "\n",
        "            ('lr', 'None'): 0.001,\n",
        "            ('wd', 'None'): 0.005\n",
        "        }\n",
        "        #Get evaluated params from primitive_sampler\n",
        "        evaluated_params = self.combined_sampler.primitive_sampler.get_evaluated_params(source_params)\n",
        "        expected_params = {\n",
        "            ('lr', 'None'): 0.001,\n",
        "            ('wd', 'None'): 0.005\n",
        "        }\n",
        "        self.assertEqual(evaluated_params, expected_params)\n",
        "        #Get evaluated actions from action_sampler\n",
        "        evaluated_actions = self.combined_sampler.action_sampler.get_evaluated_params(source_params, exclude=[])\n",
        "        #Check that both model_action and dls_action are in evaluated_actions\n",
        "        self.assertIn(('model_action', 'None'), evaluated_actions)\n",
        "        self.assertIn(('dls_action', 'None'), evaluated_actions)\n",
        "        model_action = evaluated_actions[('model_action', 'None')]\n",
        "        dls_action = evaluated_actions[('dls_action', 'None')]\n",
        "        self.assertIsInstance(model_action, ModelAction)\n",
        "        self.assertIsInstance(dls_action, Action)\n",
        "        #Check model_action._model_loader parameters\n",
        "        model_action_loader_params = model_action._model_loader.keywords\n",
        "        self.assertEqual(model_action_loader_params['rand_number'], 8)\n",
        "        self.assertEqual(\n",
        "            model_action_loader_params['activation'],\n",
        "            nn.PReLU,\n",
        "            msg=f\"model_action: {model_action.action_func}, model_action_loader_params: {model_action_loader_params}\")\n",
        "        self.assertEqual(model_action_loader_params['dropout_p'], 0.4)\n",
        "        #Check dls_action parameters\n",
        "        dls_model_loader_params = dls_action._model_loader.keywords\n",
        "        self.assertEqual(dls_model_loader_params['batch_size'], 48)\n",
        "        self.assertEqual(dls_model_loader_params['normalize'], False)\n",
        "        self.assertEqual(\n",
        "            dls_model_loader_params['activation'],\n",
        "            nn.ReLU,\n",
        "            msg=f\"dls_action: {dls_action.action_func}, dls_model_loader_params: {dls_model_loader_params}\")\n",
        "\n",
        "    def test_combination_of_actions(self):\n",
        "        #Test sampling multiple times to cover combinations\n",
        "        for _ in range(10):\n",
        "            trial = optuna.create_study(direction='minimize').ask()\n",
        "            model_action = self.combined_sampler.sample('model_action', trial)\n",
        "            dls_action = self.combined_sampler.sample('dls_action', trial)\n",
        "            lr = self.combined_sampler.sample('lr', trial)\n",
        "            wd = self.combined_sampler.sample('wd', trial)\n",
        "\n",
        "            #Check types\n",
        "            self.assertIsInstance(model_action, ModelAction)\n",
        "            self.assertIsInstance(dls_action, Action)\n",
        "            self.assertTrue(0.0001 <= lr <= 0.01)\n",
        "            self.assertTrue(0.0001 <= wd <= 0.1)\n",
        "\n",
        "            #Check model_action parameters\n",
        "            if model_action.action_func == model_action_default:\n",
        "                rand_number = model_action.params['rand_number']\n",
        "                self.assertIn(rand_number, list(range(1, 11)))\n",
        "            elif model_action.action_func == model_action_dropout:\n",
        "                activation = model_action.params['activation']\n",
        "                dropout_p = model_action.params['dropout_p']\n",
        "                self.assertIn(activation, [nn.ReLU, nn.PReLU, nn.SiLU])\n",
        "                self.assertTrue(0.0 <= dropout_p <= 1.0)\n",
        "            elif model_action.action_func == model_action_dropout_variant:\n",
        "                rand_number = model_action.params['rand_number']\n",
        "                activation = model_action.params['activation']\n",
        "                dropout_p = model_action.params['dropout_p']\n",
        "                self.assertIn(rand_number, list(range(1, 11)))\n",
        "                self.assertIn(activation, [nn.ReLU, nn.PReLU, nn.SiLU])\n",
        "                self.assertTrue(0.0 <= dropout_p <= 1.0)\n",
        "            else:\n",
        "                raise ValueError(f\"Unexpected model_action action_func: {model_action.action_func}\")\n",
        "\n",
        "            #Check dls_action parameters similarly\n",
        "            if dls_action.action_func == dls_action_default:\n",
        "                batch_size = dls_action.params['batch_size']\n",
        "                self.assertTrue(16 <= batch_size <= 64)\n",
        "            elif dls_action.action_func == dls_action_normalize:\n",
        "                batch_size = dls_action.params['batch_size']\n",
        "                normalize = dls_action.params['normalize']\n",
        "                self.assertTrue(16 <= batch_size <= 64)\n",
        "                self.assertIn(normalize, [True, False])\n",
        "            elif dls_action.action_func == dls_action_activation:\n",
        "                batch_size = dls_action.params['batch_size']\n",
        "                normalize = dls_action.params['normalize']\n",
        "                activation = dls_action.params['activation']\n",
        "                self.assertTrue(16 <= batch_size <= 64)\n",
        "                self.assertIn(normalize, [True, False])\n",
        "                self.assertIn(activation, [nn.ReLU, nn.SiLU])\n",
        "            else:\n",
        "                raise ValueError(f\"Unexpected dls_action action_func: {dls_action.action_func}\")\n",
        "\n",
        "    def test_invalid_variable_type(self):\n",
        "        trial = optuna.create_study(direction='minimize').ask()\n",
        "        with self.assertRaises(KeyError):\n",
        "            self.combined_sampler.sample('invalid_var_type', trial)\n",
        "\n",
        "    def test_invalid_default_value(self):\n",
        "        with self.assertRaises(ValueError):\n",
        "            self.combined_sampler.sample(\n",
        "                var_type='lr',\n",
        "                trial=None,\n",
        "                #Throw in a value out of range\n",
        "                default_val=self.combined_sampler.get_var('lr').distribution()['high']+10.0\n",
        "            )\n",
        "\n",
        "    def test_update_with_no_trials(self):\n",
        "        with self.assertRaises(ValueError):\n",
        "            self.combined_sampler.update(top_k_trials=[], update_state_window=1, device=torch.device('cpu'))\n",
        "\n",
        "    def test_update_with_large_window(self):\n",
        "        #Prepare one trial\n",
        "        trial_params = {\n",
        "            'model_action>None': 0,  #model_action_default\n",
        "            'rand_number>model_action': 5, #6\n",
        "            'lr>None': 1e-4,\n",
        "            'wd>None': 1e-3\n",
        "        }\n",
        "        trial = self.create_mock_trial(trial_params)\n",
        "        #Call update with update_state_window larger than number of trials\n",
        "        #Ensure that update_state_window does not cause errors\n",
        "        self.combined_sampler.update(top_k_trials=[trial], update_state_window=5, device=torch.device('cpu'))\n",
        "\n",
        "        #Check that the (primitive)variables are updated based on the trial\n",
        "        lr_var = self.combined_sampler.get_var('lr')\n",
        "        self.assertEqual(lr_var._dist['low'], 1e-4)\n",
        "        self.assertEqual(lr_var._dist['high'], 1e-4)\n",
        "        wd_var = self.combined_sampler.get_var('wd')\n",
        "        self.assertEqual(wd_var._dist['low'], 1e-3)\n",
        "        self.assertEqual(wd_var._dist['high'], 1e-3)\n",
        "\n",
        "        #Check that the ModelAction(and its signature Vars) are updated properly\n",
        "        model_action_var = self.combined_sampler.action_sampler.get_var('model_action')\n",
        "        self.assertEqual(model_action_var.distribution(indices=True), [0])\n",
        "        rand_number_var = model_action_var.get_var(model_action_default,'rand_number')\n",
        "        self.assertEqual(rand_number_var.distribution(), [6])\n",
        "\n",
        "    def test_missing_action_variable_in_trial(self):\n",
        "        trial_params = {\n",
        "            'lr>None': 0.005,\n",
        "            'wd>None': 0.001\n",
        "            # 'model_action-None' is missing\n",
        "        }\n",
        "        trial = self.create_mock_trial(trial_params)\n",
        "\n",
        "        #Call update and expect that it handles missing action vars\n",
        "        self.combined_sampler.update(top_k_trials=[trial], update_state_window=2, device=torch.device('cpu'))\n",
        "\n",
        "        #Check that non-action vars are updated\n",
        "        lr_var = self.combined_sampler.get_var('lr')\n",
        "        self.assertEqual(lr_var._dist['low'], 0.005)\n",
        "        self.assertEqual(lr_var._dist['high'], 0.005)\n",
        "        wd_var = self.combined_sampler.get_var('wd')\n",
        "        self.assertEqual(wd_var._dist['low'], 0.001)\n",
        "        self.assertEqual(wd_var._dist['high'], 0.001)\n",
        "\n",
        "        #Check that action vars are not updated\n",
        "        model_action_var = self.combined_sampler.action_sampler.get_var('model_action')\n",
        "        #Since no action indices were provided, the distribution should remain the same\n",
        "        self.assertEqual(model_action_var.distribution(indices=True), [0, 1, 2])\n",
        "\n",
        "    def test_trial_with_unknown_variables(self):\n",
        "        trial_params = {\n",
        "            'model_action>None': 0,\n",
        "            'rand_number>model_action': 5,\n",
        "            'unknown_var>None': 42,\n",
        "            'lr>None': 0.005,\n",
        "            'wd>None': 0.001\n",
        "        }\n",
        "        trial = self.create_mock_trial(trial_params)\n",
        "\n",
        "        #Call update and expect that it ignores unknown variables\n",
        "        self.combined_sampler.update(top_k_trials=[trial], update_state_window=1, device=torch.device('cpu'))\n",
        "\n",
        "        #Ensure that known variables are updated\n",
        "        lr_var = self.combined_sampler.get_var('lr')\n",
        "        self.assertEqual(lr_var._dist['low'], 0.005)\n",
        "        self.assertEqual(lr_var._dist['high'], 0.005)\n",
        "        wd_var = self.combined_sampler.get_var('wd')\n",
        "        self.assertEqual(wd_var._dist['low'], 0.001)\n",
        "        self.assertEqual(wd_var._dist['high'], 0.001)\n",
        "\n",
        "        #Check that the ModelAction(and its signature Vars) are updated properly\n",
        "        model_action_var = self.combined_sampler.action_sampler.get_var('model_action')\n",
        "        self.assertEqual(model_action_var.distribution(indices=True), [0])\n",
        "        rand_number_var = model_action_var.get_var(model_action_default,'rand_number')\n",
        "        self.assertEqual(rand_number_var.distribution(), [6])\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    unittest.main(argv=['first-arg-is-ignored'], exit=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9SIRVvjGlAIW",
        "outputId": "03ce1396-22a9-4e3b-cf7b-9bc5e261bfd2"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-36-2e3739082d59>:193: UserWarning: Action hyperparameter 'model_action' requires additional hyperparameters: [('dropout_p', 'model_action'), ('rand_number', 'model_action'), ('activation', 'model_action')]\n",
            "\n",
            "  warnings.warn(msg)\n",
            "<ipython-input-36-2e3739082d59>:193: UserWarning: Action hyperparameter 'model_action' requires additional hyperparameters: [('dropout_p', 'model_action'), ('rand_number', 'model_action'), ('activation', 'model_action')]\n",
            "Action hyperparameter 'dls_action' requires additional hyperparameters: [('batch_size', 'dls_action'), ('activation', 'dls_action'), ('normalize', 'dls_action')]\n",
            "\n",
            "  warnings.warn(msg)\n",
            "<ipython-input-36-2e3739082d59>:193: UserWarning: Action hyperparameter 'model_action' requires additional hyperparameters: [('dropout_p', 'model_action'), ('activation', 'model_action')]\n",
            "Action hyperparameter 'dls_action' requires additional hyperparameters: [('batch_size', 'dls_action'), ('activation', 'dls_action'), ('normalize', 'dls_action')]\n",
            "\n",
            "  warnings.warn(msg)\n",
            "<ipython-input-36-2e3739082d59>:193: UserWarning: Action hyperparameter 'model_action' requires additional hyperparameters: [('dropout_p', 'model_action')]\n",
            "Action hyperparameter 'dls_action' requires additional hyperparameters: [('batch_size', 'dls_action'), ('activation', 'dls_action'), ('normalize', 'dls_action')]\n",
            "\n",
            "  warnings.warn(msg)\n",
            "<ipython-input-36-2e3739082d59>:193: UserWarning: Action hyperparameter 'dls_action' requires additional hyperparameters: [('batch_size', 'dls_action'), ('activation', 'dls_action'), ('normalize', 'dls_action')]\n",
            "\n",
            "  warnings.warn(msg)\n",
            "<ipython-input-36-2e3739082d59>:193: UserWarning: Action hyperparameter 'dls_action' requires additional hyperparameters: [('activation', 'dls_action'), ('normalize', 'dls_action')]\n",
            "\n",
            "  warnings.warn(msg)\n",
            "<ipython-input-36-2e3739082d59>:193: UserWarning: Action hyperparameter 'dls_action' requires additional hyperparameters: [('activation', 'dls_action')]\n",
            "\n",
            "  warnings.warn(msg)\n",
            "..[I 2024-10-07 01:09:11,479] A new study created in memory with name: no-name-3ec09e22-6381-4c28-b387-fdf85c68095f\n",
            "[I 2024-10-07 01:09:11,483] A new study created in memory with name: no-name-91dc2da5-309a-4335-b29e-d95c03d10b57\n",
            ".................[I 2024-10-07 01:09:11,546] A new study created in memory with name: no-name-b40ab843-4443-4337-9495-28362190b228\n",
            "[I 2024-10-07 01:09:11,550] A new study created in memory with name: no-name-9fdadb3e-5806-4274-8896-5981bf1c432c\n",
            "[I 2024-10-07 01:09:11,553] A new study created in memory with name: no-name-e0f78937-b3f5-46bd-bb70-19008dd3f8d7\n",
            "[I 2024-10-07 01:09:11,558] A new study created in memory with name: no-name-299779ca-4206-48f6-89ea-08ab2413de9d\n",
            "[I 2024-10-07 01:09:11,562] A new study created in memory with name: no-name-1b200415-6a43-4d95-8207-ede631a22fee\n",
            "[I 2024-10-07 01:09:11,569] A new study created in memory with name: no-name-16cd70c1-3474-4e12-b9a4-e85c4e5417bd\n",
            "[I 2024-10-07 01:09:11,573] A new study created in memory with name: no-name-cab83d83-b019-4680-bc03-71c718b49718\n",
            "[I 2024-10-07 01:09:11,577] A new study created in memory with name: no-name-dba3d34f-9441-4f81-a24c-40903c767dac\n",
            "[I 2024-10-07 01:09:11,580] A new study created in memory with name: no-name-0b5a28ff-52e1-469d-aa52-6310f4873b08\n",
            "[I 2024-10-07 01:09:11,586] A new study created in memory with name: no-name-a5b62946-4a6c-425b-b36e-3e084bc3a101\n",
            "....[I 2024-10-07 01:09:11,611] A new study created in memory with name: no-name-c35dd03e-f6ce-4dcd-8208-1e89ead0e752\n",
            "..[I 2024-10-07 01:09:11,623] A new study created in memory with name: no-name-98844ed6-f904-4bd4-a594-765cb6333242\n",
            ".[I 2024-10-07 01:09:11,631] A new study created in memory with name: no-name-178022dc-feaf-47d4-b420-5f0a41cef5da\n",
            ".[I 2024-10-07 01:09:11,642] A new study created in memory with name: no-name-e67e55bb-a4da-4884-9cb3-aeb68341882a\n",
            ".[I 2024-10-07 01:09:11,660] A new study created in memory with name: no-name-a0c03136-f63a-4ab6-b3cc-d227217bf922\n",
            ".....[I 2024-10-07 01:09:11,701] A new study created in memory with name: no-name-a2710fce-6f98-4409-8ce6-e0ab2ef7f4f0\n",
            ".[I 2024-10-07 01:09:11,708] A new study created in memory with name: no-name-908bf048-e62d-4af3-8134-bff840be235a\n",
            ".[I 2024-10-07 01:09:11,715] A new study created in memory with name: no-name-c62e65dd-073b-4564-ad21-95a33f09902a\n",
            ".[I 2024-10-07 01:09:11,722] A new study created in memory with name: no-name-5d212d30-73d0-41c4-aa56-7c65ba2f2cf0\n",
            ".[I 2024-10-07 01:09:11,729] A new study created in memory with name: no-name-d4d01e02-f876-4a2b-a6a3-c946162cdd76\n",
            ".[I 2024-10-07 01:09:11,733] A new study created in memory with name: no-name-7e44d761-339b-413b-a942-249fbcef06ea\n",
            ".[I 2024-10-07 01:09:11,738] A new study created in memory with name: no-name-9188ebb7-ce96-4896-889b-fa289bfcef05\n",
            "..<ipython-input-36-2e3739082d59>:193: UserWarning: Action hyperparameter 'optimizer' requires additional hyperparameters: [('param4', 'optimizer'), ('param2', 'optimizer'), ('param3', 'optimizer'), ('param1', 'optimizer')]\n",
            "\n",
            "  warnings.warn(msg)\n",
            "<ipython-input-36-2e3739082d59>:193: UserWarning: Action hyperparameter 'optimizer' requires additional hyperparameters: [('param4', 'optimizer'), ('param2', 'optimizer'), ('param3', 'optimizer')]\n",
            "\n",
            "  warnings.warn(msg)\n",
            "<ipython-input-36-2e3739082d59>:193: UserWarning: Action hyperparameter 'optimizer' requires additional hyperparameters: [('param4', 'optimizer'), ('param3', 'optimizer')]\n",
            "\n",
            "  warnings.warn(msg)\n",
            "<ipython-input-36-2e3739082d59>:193: UserWarning: Action hyperparameter 'optimizer' requires additional hyperparameters: [('param4', 'optimizer')]\n",
            "\n",
            "  warnings.warn(msg)\n",
            ".<ipython-input-36-2e3739082d59>:193: UserWarning: Action hyperparameter 'optimizer' requires additional hyperparameters: [('param1', 'optimizer')]\n",
            "\n",
            "  warnings.warn(msg)\n",
            "..<ipython-input-36-2e3739082d59>:193: UserWarning: Action hyperparameter 'optimizer' requires additional hyperparameters: [('param2', 'optimizer'), ('param1', 'optimizer')]\n",
            "\n",
            "  warnings.warn(msg)\n",
            "<ipython-input-36-2e3739082d59>:193: UserWarning: Action hyperparameter 'optimizer' requires additional hyperparameters: [('param2', 'optimizer')]\n",
            "\n",
            "  warnings.warn(msg)\n",
            ".................<ipython-input-36-2e3739082d59>:193: UserWarning: Action hyperparameter 'optimizer_evens' requires additional hyperparameters: [('param4', 'optimizer_evens'), ('param2', 'optimizer_evens'), ('param6', 'optimizer_evens')]\n",
            "\n",
            "  warnings.warn(msg)\n",
            "<ipython-input-36-2e3739082d59>:193: UserWarning: Action hyperparameter 'optimizer_evens' requires additional hyperparameters: [('param4', 'optimizer_evens'), ('param6', 'optimizer_evens')]\n",
            "\n",
            "  warnings.warn(msg)\n",
            "<ipython-input-36-2e3739082d59>:193: UserWarning: Action hyperparameter 'optimizer_evens' requires additional hyperparameters: [('param4', 'optimizer_evens')]\n",
            "\n",
            "  warnings.warn(msg)\n",
            "<ipython-input-36-2e3739082d59>:193: UserWarning: Action hyperparameter 'optimizer_evens' requires additional hyperparameters: [('param3', 'optimizer_evens'), ('param1', 'optimizer_evens'), ('param5', 'optimizer_evens')]\n",
            "\n",
            "  warnings.warn(msg)\n",
            "...........................<ipython-input-36-2e3739082d59>:193: UserWarning: Action hyperparameter 'optimizer2' requires additional hyperparameters: [('param4', 'optimizer2'), ('param3', 'optimizer2')]\n",
            "\n",
            "  warnings.warn(msg)\n",
            "<ipython-input-36-2e3739082d59>:193: UserWarning: Action hyperparameter 'optimizer2' requires additional hyperparameters: [('param4', 'optimizer2')]\n",
            "\n",
            "  warnings.warn(msg)\n",
            ".............[I 2024-10-07 01:09:11,928] A new study created in memory with name: no-name-853bfb0a-916c-477e-99c4-139f4b639a54\n",
            "..[I 2024-10-07 01:09:11,936] A new study created in memory with name: no-name-3a4005df-6a8a-472b-bf7d-f490a8e09e64\n",
            ".....\n",
            "----------------------------------------------------------------------\n",
            "Ran 108 tests in 0.484s\n",
            "\n",
            "OK\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Class: Objective"
      ],
      "metadata": {
        "id": "qHSHpR-E4QYh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Objective:\n",
        "    def __init__(self, trial, combined_sampler: CombinedSampler, device: torch.device, default_dict: dict = {}):\n",
        "        #Instance of Objective must have an attribute 'execute_objective' set BEFOREHAND\n",
        "        if not hasattr(self, 'execute_objective'):\n",
        "            raise RuntimeError(\"Class Objective must have an attribute 'execute_objective' set before any instance of it can be initialized. This is an internal issue.\")\n",
        "\n",
        "        #Perform general-error checking\n",
        "        self._validate_inputs(trial=trial, combined_sampler=combined_sampler, default_dict=default_dict)\n",
        "\n",
        "        #Initialize all the member variables.\n",
        "        self.trial = trial\n",
        "        self.default_dict = default_dict\n",
        "        self.combined_sampler = combined_sampler\n",
        "        self.var_types = self.combined_sampler.var_types() if self.combined_sampler else set(default_dict.keys())\n",
        "        self.action_var_types = self.combined_sampler.action_var_types() if self.combined_sampler else {key for key in self.default_dict.keys() if 'action' in key}\n",
        "        self.sampled_var_types = set()\n",
        "        self.device = device if device else torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    def _validate_inputs(self, trial: Trial, combined_sampler: CombinedSampler, default_dict: dict):\n",
        "        #trial type check:\n",
        "        if trial is not None and not isinstance(trial, (Trial, FrozenTrial)):\n",
        "            raise ValueError(f\"Input 'trial' must either be 'None' or an instance in (optuna::Trial, optuna::FrozenTrial) but found: {trial}\")\n",
        "\n",
        "        #combined_sampler type check:\n",
        "        if combined_sampler is not None and not isinstance(combined_sampler, CombinedSampler):\n",
        "            raise ValueError(f\"Input 'combined_sampler' must either be None or of type CombinedSampler but found: {combined_sampler}\")\n",
        "\n",
        "        #Make sure there's something to be sampled:\n",
        "        if not default_dict and not combined_sampler:\n",
        "            raise ValueError(f\"Both default_dict and combined_sampler are empty. Nothing to be sampled.\")\n",
        "\n",
        "    def sample(self, var_type: str, default_val = None):\n",
        "        #No double sampling allowed\n",
        "        if var_type in self.sampled_var_types:\n",
        "            raise ValueError(f\"Variable '{var_type}' has already been sampled. Variables can only be sampled once per trial.\")\n",
        "\n",
        "        #(1)If var_type in default_dict, sampled_val is retrieved directly from default_dict\n",
        "        #(2)If not (1) and var_type not in combined_sampler(or no combined_sampler was passed in), sampled_val = default_val\n",
        "        #(3)If not (1) and (2), then sample from combined_sampler.\n",
        "        if var_type in self.default_dict.keys():\n",
        "            sampled_val = self.default_dict[var_type]\n",
        "        elif self.combined_sampler is None or var_type not in self.var_types:\n",
        "            sampled_val = default_val\n",
        "        else: #All False, THEN sampled_val <- sampled value from self.combined_sampler\n",
        "            sampled_val = self.combined_sampler.sample(var_type, self.trial, default_val)\n",
        "\n",
        "        #Add var_type to the set of all sampled var_types.\n",
        "        self.sampled_var_types.add(var_type)\n",
        "\n",
        "        return sampled_val"
      ],
      "metadata": {
        "id": "FJE7gRBCagn5"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Test: Objective, UserObjective, SampleCallVisitor"
      ],
      "metadata": {
        "id": "xAvWWmAD0HvN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Dict, Any\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class PolynomialDataset(Dataset):\n",
        "    def __init__(self, coeffs: torch.tensor, input_min: float, input_max: float, input_size: int):\n",
        "        self.coeffs = coeffs.clone().detach()\n",
        "        self.inputs = torch.empty(size=(input_size,)).uniform_(input_min, input_max)\n",
        "        self.outputs = self._f(X=self.inputs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.inputs[idx], self.outputs[idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.inputs.size(0)\n",
        "\n",
        "    def _f(self, X):\n",
        "        powers = torch.arange(self.coeffs.size(0)).unsqueeze(0)\n",
        "        Y = self.inputs.unsqueeze(-1)**powers\n",
        "        return torch.matmul(Y, self.coeffs).squeeze(-1)\n",
        "\n",
        "degree = 6\n",
        "coeffs_min, coeffs_max = -10.0, 10.0\n",
        "coeffs = torch.empty(size=(degree+1,)).uniform_(coeffs_min, coeffs_max)\n",
        "\n",
        "train_dset = PolynomialDataset(coeffs=coeffs, input_min=-100.0, input_max=100.0, input_size=8000)\n",
        "valid_dset = PolynomialDataset(coeffs=coeffs, input_min=-300.12, input_max=500.78, input_size=2000)\n",
        "\n",
        "class ModelDefault(nn.Module):\n",
        "    def __init__(self, activation: nn.Module, preprocess: bool, start_neurons: int):\n",
        "        super().__init__()\n",
        "        self.preprocess = preprocess\n",
        "        if self.preprocess:\n",
        "            start_neurons = max(start_neurons, degree+1)\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_features=degree+1 if self.preprocess else 1, out_features=start_neurons),\n",
        "            nn.BatchNorm1d(num_features=start_neurons),\n",
        "            activation(),\n",
        "            nn.Linear(in_features=start_neurons, out_features=start_neurons*2),\n",
        "            nn.BatchNorm1d(num_features=start_neurons*2),\n",
        "            activation(),\n",
        "            nn.Linear(in_features=start_neurons*2, out_features=1)\n",
        "        )\n",
        "\n",
        "    def _preprocess(self, X):\n",
        "        N = X.size(0)\n",
        "        X = X.unsqueeze(-1)**torch.arange(degree+1).unsqueeze(0)\n",
        "        return X\n",
        "\n",
        "    def forward(self, X):\n",
        "        if X.dim() != 1:\n",
        "            X = X.reshape(-1)\n",
        "\n",
        "        if self.preprocess:\n",
        "            X = self._preprocess(X)\n",
        "        else:\n",
        "            X = X.unsqueeze(-1)\n",
        "        return self.net(X)\n",
        "\n",
        "class ModelDropout(nn.Module):\n",
        "    def __init__(self, activation: nn.Module, dropout_p: float, preprocess: bool, start_neurons: int):\n",
        "        super().__init__()\n",
        "        self.preprocess = preprocess\n",
        "        if self.preprocess:\n",
        "            start_neurons = max(start_neurons, degree+1)\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_features=degree+1 if self.preprocess else 1, out_features=start_neurons),\n",
        "            nn.BatchNorm1d(num_features=start_neurons),\n",
        "            activation(),\n",
        "            nn.Dropout(p=dropout_p),\n",
        "            nn.Linear(in_features=start_neurons, out_features=start_neurons*2),\n",
        "            nn.BatchNorm1d(num_features=start_neurons*2),\n",
        "            activation(),\n",
        "            nn.Linear(in_features=start_neurons*2, out_features=1)\n",
        "        )\n",
        "\n",
        "    def _preprocess(self, X):\n",
        "        N = X.size(0)\n",
        "        X = X.unsqueeze(-1)**torch.arange(degree+1).unsqueeze(0)\n",
        "        return X\n",
        "\n",
        "    def forward(self, X):\n",
        "        if X.dim() != 1:\n",
        "            X = X.reshape(-1)\n",
        "\n",
        "        if self.preprocess:\n",
        "            X = self._preprocess(X)\n",
        "        else:\n",
        "            X = X.unsqueeze(-1)\n",
        "        return self.net(X)\n",
        "\n",
        "def model_action_default(activation: nn.Module, preprocess: bool, start_neurons: int, device: torch.device):\n",
        "    if start_neurons < 1:\n",
        "        raise ValueError(f\"start_neurons MUST be a positive integer but found start_neurons: {start_neurons}\")\n",
        "    return ModelDefault(activation=activation,\n",
        "                        preprocess=preprocess,\n",
        "                        start_neurons=start_neurons).to(device)\n",
        "\n",
        "def model_action_dropout(activation: nn.Module, dropout_p: float, preprocess: bool, start_neurons: int, device: torch.device):\n",
        "    if start_neurons < 1:\n",
        "        raise ValueError(f\"start_neurons MUST be a positive integer but found start_neurons: {start_neurons}\")\n",
        "    if dropout_p < 0.0 or dropout_p >= 1.0:\n",
        "        raise ValueError(f\"0.0 <= dropout_p < 1.0 required but found dropout_p: {dropout_p}\")\n",
        "    return ModelDropout(activation=activation,\n",
        "                        dropout_p=dropout_p,\n",
        "                        preprocess=preprocess,\n",
        "                        start_neurons=start_neurons).to(device)\n",
        "\n",
        "def dls_action_default(batch_size: int, device=None):\n",
        "    train_dataloader = DataLoader(train_dset, batch_size=batch_size, shuffle=True)\n",
        "    valid_dataloader = DataLoader(valid_dset, batch_size=batch_size, shuffle=True)\n",
        "    dls = DataLoaders(train_dataloader, valid_dataloader)\n",
        "    return dls\n",
        "\n",
        "def custom_mse(Y_hat, Y):\n",
        "    Y_hat = Y_hat.reshape(-1)\n",
        "    Y = Y.reshape(-1)\n",
        "    return nn.MSELoss()(Y_hat, Y)\n",
        "\n",
        "class TestObjective(unittest.TestCase):\n",
        "    def setUp(self):\n",
        "        self.INIT_DICT = {\n",
        "            ('dls_action--action', None): {\n",
        "                'params': {'choices': [dls_action_default]},\n",
        "                'sample': 'categorical'\n",
        "            },\n",
        "            ('freeze', None): {\n",
        "                'params': {'choices': [True, False]},\n",
        "                'sample': 'categorical'\n",
        "            },\n",
        "            ('lr_low', None): {\n",
        "                'params': {'low': 1e-7, 'high': 9e-4, 'log': True},\n",
        "                'sample': 'float'\n",
        "            },\n",
        "            ('lr_high', None): {\n",
        "                'params': {'low': 9e-4, 'high': 1e-1, 'log': True},\n",
        "                'sample': 'float'\n",
        "            },\n",
        "            ('lr', None): {\n",
        "                'params': {'low': 1e-6, 'high': 1e-1, 'log': True},\n",
        "                'sample': 'float'\n",
        "            },\n",
        "            ('wd', None): {\n",
        "                'params': {'choices': [1e-4, 1e-3, 1e-2, 1e-1]},\n",
        "                'sample': 'categorical'\n",
        "            },\n",
        "            ('gradient_clip', None): {\n",
        "                'params': {'choices': [True, False]},\n",
        "                'sample': 'categorical'\n",
        "            },\n",
        "            ('max_norm', None): {\n",
        "                'params': {'low': 0.0, 'high': 15.0, 'log': False},\n",
        "                'sample': 'float'\n",
        "            },\n",
        "            ('one_cycle', None): {\n",
        "                'params': {'choices': [True, False]},\n",
        "                'sample': 'categorical'\n",
        "            },\n",
        "            ('pct_start', None): {\n",
        "                'params': {'low': 0.10, 'high': 0.95, 'log': False, 'step': 0.05},\n",
        "                'sample': 'float'\n",
        "            },\n",
        "            ('n_epoch', None): {\n",
        "                'params': {'choices': [5, 10, 15]},\n",
        "                'sample': 'categorical'\n",
        "            },\n",
        "            ('model_action--action', None): {\n",
        "                'params': {'choices': [model_action_default, model_action_dropout]},\n",
        "                'sample': 'categorical'\n",
        "            },\n",
        "            ('activation', 'model_action'): {\n",
        "                'params': {'choices': [nn.ReLU, nn.PReLU, nn.SiLU]},\n",
        "                'sample': 'categorical'\n",
        "            },\n",
        "            ('loss_func', None): {\n",
        "                'params': {'choices': [custom_mse]},\n",
        "                'sample': 'categorical'\n",
        "            },\n",
        "            ('metric', None): {\n",
        "                'params': {'choices': [custom_mse]},\n",
        "                'sample': 'categorical'\n",
        "            },\n",
        "            ('preprocess', 'model_action'): {\n",
        "                'params': {'choices': [True, False]},\n",
        "                'sample': 'categorical'\n",
        "            },\n",
        "            ('start_neurons', 'model_action'): {\n",
        "                'params': {'choices': [1, 2, 4, 8, 16, 32]},\n",
        "                'sample': 'categorical'\n",
        "            },\n",
        "            ('dropout_p', 'model_action'): {\n",
        "                'params': {'low': 0.01, 'high': 0.95, 'log': False, 'step': 0.05},\n",
        "                'sample': 'float'\n",
        "            },\n",
        "            ('batch_size', 'dls_action'): {\n",
        "                'params': {'choices': [2, 4, 8, 16, 32, 64]},\n",
        "                'sample': 'categorical'\n",
        "            }\n",
        "        }\n",
        "        self.cs = UserConfigSpace(init_config=self.INIT_DICT)\n",
        "        self.trial = optuna.create_study(direction='minimize').ask()\n",
        "        self.device = torch.device('cpu')\n",
        "\n",
        "    def test_objective_initialization(self):\n",
        "        #TEST: Initialziation of ConfigSpace via INIT_DICT\n",
        "        config_space = UserConfigSpace(init_config=self.INIT_DICT)\n",
        "\n",
        "        combined_sampler = CombinedSampler(config_space)\n",
        "        self.assertEqual(\n",
        "            combined_sampler.var_types(),\n",
        "            {\n",
        "            'dls_action', 'freeze', 'lr_low', 'lr_high', 'lr', 'wd',\n",
        "            'gradient_clip', 'max_norm', 'one_cycle', 'pct_start', 'n_epoch',\n",
        "            'model_action', 'loss_func', 'metric'\n",
        "            },\n",
        "            msg=f\"combined_sampler.var_types(): {combined_sampler.var_types()}\")\n",
        "        self.assertEqual(\n",
        "            combined_sampler.action_var_types(),\n",
        "            {'model_action', 'dls_action'}\n",
        "        )\n",
        "        self.assertEqual(\n",
        "            combined_sampler.model_action_var_types(),\n",
        "            {'model_action'}\n",
        "        )\n",
        "        self.assertEqual(\n",
        "            combined_sampler.sig_var_types(),\n",
        "             {'activation', 'batch_size', 'preprocess', 'dropout_p', 'start_neurons'},\n",
        "             msg=f\"combined_sampler.sig_var_types(): {combined_sampler.sig_var_types()}\"\n",
        "        )\n",
        "        sampled_dict = dict()\n",
        "        for key in combined_sampler.var_types():\n",
        "            sampled_dict[key] = combined_sampler.sample(key, self.trial, default_val=None)\n",
        "        #This builds shared ownership between the two.\n",
        "        setattr(Objective, 'execute_objective', self.cs.objective.__func__)\n",
        "        objective = Objective(trial=self.trial, combined_sampler=combined_sampler, device=self.device)\n",
        "        self.assertEqual(objective.default_dict, {})\n",
        "\n",
        "    def test_execute_objective_trial(self):\n",
        "        #This builds shared ownership between the two.\n",
        "        setattr(Objective, 'execute_objective', self.cs.objective.__func__)\n",
        "        combined_sampler = CombinedSampler(INIT_CONFIG=self.cs)\n",
        "        objective = Objective(trial=self.trial, combined_sampler=combined_sampler, device=self.device, default_dict={})\n",
        "        objective_loss, model_container = objective.execute_objective()\n",
        "        self.assertIsInstance(model_container, Learner)\n",
        "        model = model_container.model\n",
        "        self.assertIsInstance(model, nn.Module)\n",
        "        input_sample = torch.randn(size=[64, 1])\n",
        "        output = model(input_sample)\n",
        "        self.assertEqual(output.shape, torch.Size([64, 1]))\n",
        "\n",
        "    #Additional tests can be adapted similarly\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    unittest.main(argv=['first-arg-is-ignored'], exit=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 523
        },
        "id": "gLm2_QfwlZnD",
        "outputId": "d3f3ab43-6ef8-456b-fc7b-09ac82483e1e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-2e3739082d59>:193: UserWarning: Action hyperparameter 'model_action' requires additional hyperparameters: [('dropout_p', 'model_action'), ('start_neurons', 'model_action'), ('preprocess', 'model_action'), ('activation', 'model_action')]\n",
            "\n",
            "  warnings.warn(msg)\n",
            "<ipython-input-8-2e3739082d59>:193: UserWarning: Action hyperparameter 'model_action' requires additional hyperparameters: [('dropout_p', 'model_action'), ('start_neurons', 'model_action'), ('preprocess', 'model_action'), ('activation', 'model_action')]\n",
            "Action hyperparameter 'dls_action' requires additional hyperparameters: [('batch_size', 'dls_action')]\n",
            "\n",
            "  warnings.warn(msg)\n",
            "<ipython-input-8-2e3739082d59>:193: UserWarning: Action hyperparameter 'model_action' requires additional hyperparameters: [('dropout_p', 'model_action'), ('start_neurons', 'model_action'), ('preprocess', 'model_action')]\n",
            "Action hyperparameter 'dls_action' requires additional hyperparameters: [('batch_size', 'dls_action')]\n",
            "\n",
            "  warnings.warn(msg)\n",
            "<ipython-input-8-2e3739082d59>:193: UserWarning: Action hyperparameter 'model_action' requires additional hyperparameters: [('dropout_p', 'model_action'), ('start_neurons', 'model_action')]\n",
            "Action hyperparameter 'dls_action' requires additional hyperparameters: [('batch_size', 'dls_action')]\n",
            "\n",
            "  warnings.warn(msg)\n",
            "<ipython-input-8-2e3739082d59>:193: UserWarning: Action hyperparameter 'model_action' requires additional hyperparameters: [('dropout_p', 'model_action'), ('start_neurons', 'model_action')]\n",
            "\n",
            "  warnings.warn(msg)\n",
            "<ipython-input-8-2e3739082d59>:193: UserWarning: Action hyperparameter 'model_action' requires additional hyperparameters: [('dropout_p', 'model_action')]\n",
            "\n",
            "  warnings.warn(msg)\n",
            "[I 2024-10-07 01:12:25,186] A new study created in memory with name: no-name-28917656-82c1-4b11-8803-3ba2344f6bf0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ".[I 2024-10-07 01:15:06,486] A new study created in memory with name: no-name-f07f490c-358e-4370-8572-39e0cff29948\n",
            ".\n",
            "----------------------------------------------------------------------\n",
            "Ran 2 tests in 161.312s\n",
            "\n",
            "OK\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Class: HpoTrainer"
      ],
      "metadata": {
        "id": "Ebi8cz_iT8jS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class HpoTrainer:\n",
        "    def __init__(self, combined_sampler: CombinedSampler, n_trials: int, device: torch.device,\n",
        "                 prev_top_k_trials: list = [], pruner: optuna.pruners = None, timeout: int = 14400, eta: int = 2):\n",
        "        \"\"\"\n",
        "        Class Explanation: HpoTrainer is a TEMPORARY objective for running a single HPO process.\n",
        "        \"\"\"\n",
        "        self.combined_sampler = combined_sampler\n",
        "        self.device = device\n",
        "        self.eta = eta\n",
        "        self.timeout = timeout\n",
        "\n",
        "        #Enqueue ALL the previous_top_k_trials.\n",
        "        #NOTE: This allows the optimizer to efficiently search the new space.\n",
        "        self.prior_trials = prev_top_k_trials\n",
        "\n",
        "        #Use the user-defined n_trials for new trials (ignore the prior trial count)\n",
        "        self.n_trials = n_trials\n",
        "\n",
        "        #Select a pruner if not specified\n",
        "        choices = (optuna.pruners.HyperbandPruner, optuna.pruners.SuccessiveHalvingPruner)\n",
        "        self.pruner = random.choice(choices) if pruner is None else pruner\n",
        "\n",
        "    def run(self):\n",
        "        print(\"*****************RUNNING: HPO PROCESS*****************\")\n",
        "\n",
        "        #To get logging info on the HPO process:\n",
        "        optuna.logging.get_logger(\"optuna\").addHandler(logging.StreamHandler(sys.stdout))\n",
        "\n",
        "        #Create the study object with pruner\n",
        "        study = optuna.create_study(\n",
        "            direction='minimize',\n",
        "            pruner=self.pruner(reduction_factor=self.eta)\n",
        "        )\n",
        "\n",
        "        #Enqueue the prior trials (these will run first)\n",
        "        for trial in self.prior_trials:\n",
        "            study.enqueue_trial(trial.params)\n",
        "\n",
        "        #Run the optimization for the user-defined n_trials (new trials only)\n",
        "        study.optimize(func=self.hpo_objective, n_trials=self.n_trials, timeout=self.timeout)\n",
        "\n",
        "        #Return the optimized study\n",
        "        return study\n",
        "\n",
        "    def hpo_objective(self, trial):\n",
        "        #Sanity check\n",
        "        if not isinstance(trial, (Trial, FrozenTrial)):\n",
        "            raise ValueError(f\"Input 'trial' MUST be an instance of optuna::Trial OR optuna::FrozenTrial but found trial: {trial}.\")\n",
        "\n",
        "        #Retrieve & return the objective loss, produced by Objective.\n",
        "        objective_loss, _ = Objective(\n",
        "            trial=trial,\n",
        "            combined_sampler=self.combined_sampler,\n",
        "            device=self.device\n",
        "        ).execute_objective()\n",
        "\n",
        "        #Return the objective loss.\n",
        "        return objective_loss"
      ],
      "metadata": {
        "id": "6aLuEwSsLsO_"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Test: HpoTrainer"
      ],
      "metadata": {
        "id": "npIqWHWz127V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Now, adapt the test cases\n",
        "class TestHpoTrainer(unittest.TestCase):\n",
        "    def setUp(self):\n",
        "        self.INIT_DICT = {\n",
        "            ('dls_action--action', None): {\n",
        "                'params': {'choices': [dls_action_default]},\n",
        "                'sample': 'categorical'\n",
        "            },\n",
        "            ('freeze', None): {\n",
        "                'params': {'choices': [True, False]},\n",
        "                'sample': 'categorical'\n",
        "            },\n",
        "            ('lr_low', None): {\n",
        "                'params': {'low': 1e-7, 'high': 9e-4, 'log': True},\n",
        "                'sample': 'float'\n",
        "            },\n",
        "            ('lr_high', None): {\n",
        "                'params': {'low': 9e-4, 'high': 1e-1, 'log': True},\n",
        "                'sample': 'float'\n",
        "            },\n",
        "            ('lr', None): {\n",
        "                'params': {'low': 1e-6, 'high': 1e-1, 'log': True},\n",
        "                'sample': 'float'\n",
        "            },\n",
        "            ('wd', None): {\n",
        "                'params': {'choices': [1e-4, 1e-3, 1e-2, 1e-1]},\n",
        "                'sample': 'categorical'\n",
        "            },\n",
        "            ('gradient_clip', None): {\n",
        "                'params': {'choices': [True, False]},\n",
        "                'sample': 'categorical'\n",
        "            },\n",
        "            ('max_norm', None): {\n",
        "                'params': {'low': 0.0, 'high': 15.0, 'log': False},\n",
        "                'sample': 'float'\n",
        "            },\n",
        "            ('one_cycle', None): {\n",
        "                'params': {'choices': [True, False]},\n",
        "                'sample': 'categorical'\n",
        "            },\n",
        "            ('pct_start', None): {\n",
        "                'params': {'low': 0.10, 'high': 0.95,  'log': False, 'step': 0.05},\n",
        "                'sample': 'float'\n",
        "            },\n",
        "            ('n_epoch', None): {\n",
        "                'params': {'choices': list(range(1,11))},\n",
        "                'sample': 'categorical'\n",
        "            },\n",
        "            ('model_action--action', None): {\n",
        "                'params': {'choices': [model_action_default, model_action_dropout]},\n",
        "                'sample': 'categorical'\n",
        "            },\n",
        "            ('activation', 'model_action'): {\n",
        "                'params': {'choices': [nn.ReLU, nn.PReLU, nn.SiLU]},\n",
        "                'sample': 'categorical'\n",
        "            },\n",
        "            ('loss_func', None): {\n",
        "                'params': {'choices': [custom_mse]},\n",
        "                'sample': 'categorical'\n",
        "            },\n",
        "            ('metric', None): {\n",
        "                'params': {'choices': [custom_mse]},\n",
        "                'sample': 'categorical'\n",
        "            },\n",
        "            ('preprocess', 'model_action'): {\n",
        "                'params': {'choices': [True, False]},\n",
        "                'sample': 'categorical'\n",
        "            },\n",
        "            ('start_neurons', 'model_action'): {\n",
        "                'params': {'choices': [1, 2, 4, 8, 16, 32]},\n",
        "                'sample': 'categorical'\n",
        "            },\n",
        "            ('dropout_p', 'model_action'): {\n",
        "                'params': {'low': 0.01, 'high': 0.95,  'log': False, 'step': 0.05},\n",
        "                'sample': 'float'\n",
        "            },\n",
        "            ('batch_size', 'dls_action'): {\n",
        "                'params': {'choices': [2, 4, 8, 16, 32, 64]},\n",
        "                'sample': 'categorical'\n",
        "            }\n",
        "        }\n",
        "        self.cs = UserConfigSpace(init_config=self.INIT_DICT)\n",
        "        setattr(Objective, 'execute_objective', self.cs.objective.__func__)\n",
        "        self.device = torch.device('cpu')\n",
        "\n",
        "    def test_initialization(self):\n",
        "        combined_sampler = CombinedSampler(INIT_CONFIG=self.cs)\n",
        "\n",
        "        hpo_trainer = HpoTrainer(\n",
        "            combined_sampler=combined_sampler,\n",
        "            n_trials=2,\n",
        "            device=self.device\n",
        "        )\n",
        "        self.assertIsInstance(hpo_trainer, HpoTrainer)\n",
        "        self.assertIsInstance(hpo_trainer.combined_sampler, CombinedSampler)\n",
        "        self.assertEqual(hpo_trainer.n_trials, 2)\n",
        "        self.assertEqual(hpo_trainer.device, self.device)\n",
        "        self.assertEqual(hpo_trainer.eta, 2)\n",
        "        self.assertEqual(hpo_trainer.timeout, 14400)\n",
        "\n",
        "    def test_run(self):\n",
        "        n_trials = [4, 2, 1]\n",
        "        top_ks = random.choices(list(range(1, 10)), k=len(n_trials))\n",
        "        for n, k in zip(n_trials, top_ks):\n",
        "            combined_sampler = CombinedSampler(INIT_CONFIG=self.cs)\n",
        "\n",
        "            hpo_trainer = HpoTrainer(\n",
        "                combined_sampler=combined_sampler,\n",
        "                n_trials=n,\n",
        "                device=self.device\n",
        "            )\n",
        "\n",
        "            study = hpo_trainer.run()\n",
        "            prev_trials = study.trials\n",
        "            top_k = min(len(prev_trials), k)\n",
        "            top_k_trials = sorted(\n",
        "                prev_trials,\n",
        "                key=lambda trial: trial.value if trial.value is not None else float('inf')\n",
        "            )[:top_k]\n",
        "            combined_sampler.update(\n",
        "                top_k_trials=top_k_trials,\n",
        "                update_state_window=2,\n",
        "                device=self.device\n",
        "            )\n",
        "            #You can add assertions or checks here as needed\n",
        "            print(\"****************************************\")\n",
        "            print(combined_sampler)\n",
        "            print(\"****************************************\")\n",
        "            print()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    unittest.main(argv=['first-arg-is-ignored'], exit=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "E-Ps9JtQR12B",
        "outputId": "684c8e00-526e-4dce-c939-9241aed31914"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-2e3739082d59>:193: UserWarning: Action hyperparameter 'model_action' requires additional hyperparameters: [('dropout_p', 'model_action'), ('start_neurons', 'model_action'), ('preprocess', 'model_action'), ('activation', 'model_action')]\n",
            "\n",
            "  warnings.warn(msg)\n",
            "<ipython-input-8-2e3739082d59>:193: UserWarning: Action hyperparameter 'model_action' requires additional hyperparameters: [('dropout_p', 'model_action'), ('start_neurons', 'model_action'), ('preprocess', 'model_action'), ('activation', 'model_action')]\n",
            "Action hyperparameter 'dls_action' requires additional hyperparameters: [('batch_size', 'dls_action')]\n",
            "\n",
            "  warnings.warn(msg)\n",
            "<ipython-input-8-2e3739082d59>:193: UserWarning: Action hyperparameter 'model_action' requires additional hyperparameters: [('dropout_p', 'model_action'), ('start_neurons', 'model_action'), ('preprocess', 'model_action')]\n",
            "Action hyperparameter 'dls_action' requires additional hyperparameters: [('batch_size', 'dls_action')]\n",
            "\n",
            "  warnings.warn(msg)\n",
            "<ipython-input-8-2e3739082d59>:193: UserWarning: Action hyperparameter 'model_action' requires additional hyperparameters: [('dropout_p', 'model_action'), ('start_neurons', 'model_action')]\n",
            "Action hyperparameter 'dls_action' requires additional hyperparameters: [('batch_size', 'dls_action')]\n",
            "\n",
            "  warnings.warn(msg)\n",
            "<ipython-input-8-2e3739082d59>:193: UserWarning: Action hyperparameter 'model_action' requires additional hyperparameters: [('dropout_p', 'model_action'), ('start_neurons', 'model_action')]\n",
            "\n",
            "  warnings.warn(msg)\n",
            "<ipython-input-8-2e3739082d59>:193: UserWarning: Action hyperparameter 'model_action' requires additional hyperparameters: [('dropout_p', 'model_action')]\n",
            "\n",
            "  warnings.warn(msg)\n",
            ".[I 2024-10-07 01:15:06,537] A new study created in memory with name: no-name-efc75526-97de-4c57-9c67-dffea4a4396f\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*****************RUNNING: HPO PROCESS*****************\n",
            "A new study created in memory with name: no-name-efc75526-97de-4c57-9c67-dffea4a4396f\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-07 01:15:14,871] Trial 0 finished with value: 2.933249212057006e+32 and parameters: {'dls_action>None': 0, 'batch_size>dls_action': 3, 'model_action>None': 0, 'start_neurons>model_action': 3, 'preprocess>model_action': 0, 'activation>model_action': 2, 'loss_func>None': 0, 'metric>None': 0, 'freeze>None': 0, 'gradient_clip>None': 0, 'max_norm>None': 2.4066637230764645, 'one_cycle>None': 1, 'n_epoch>None': 3, 'lr>None': 3.5957951740910273e-06, 'wd>None': 3}. Best is trial 0 with value: 2.933249212057006e+32.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 0 finished with value: 2.933249212057006e+32 and parameters: {'dls_action>None': 0, 'batch_size>dls_action': 3, 'model_action>None': 0, 'start_neurons>model_action': 3, 'preprocess>model_action': 0, 'activation>model_action': 2, 'loss_func>None': 0, 'metric>None': 0, 'freeze>None': 0, 'gradient_clip>None': 0, 'max_norm>None': 2.4066637230764645, 'one_cycle>None': 1, 'n_epoch>None': 3, 'lr>None': 3.5957951740910273e-06, 'wd>None': 3}. Best is trial 0 with value: 2.933249212057006e+32.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/optuna/distributions.py:689: UserWarning: The distribution is specified by [0.01, 0.95] and step=0.05, but the range is not divisible by `step`. It will be replaced by [0.01, 0.91].\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-07 01:15:15,576] Trial 1 finished with value: 2.9332497923413994e+32 and parameters: {'dls_action>None': 0, 'batch_size>dls_action': 5, 'model_action>None': 1, 'dropout_p>model_action': 0.36000000000000004, 'start_neurons>model_action': 5, 'preprocess>model_action': 0, 'activation>model_action': 2, 'loss_func>None': 0, 'metric>None': 0, 'freeze>None': 1, 'gradient_clip>None': 1, 'one_cycle>None': 0, 'lr_low>None': 1.309786927349962e-07, 'lr_high>None': 0.040434500278020764, 'n_epoch>None': 0, 'wd>None': 0, 'pct_start>None': 0.85}. Best is trial 0 with value: 2.933249212057006e+32.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 1 finished with value: 2.9332497923413994e+32 and parameters: {'dls_action>None': 0, 'batch_size>dls_action': 5, 'model_action>None': 1, 'dropout_p>model_action': 0.36000000000000004, 'start_neurons>model_action': 5, 'preprocess>model_action': 0, 'activation>model_action': 2, 'loss_func>None': 0, 'metric>None': 0, 'freeze>None': 1, 'gradient_clip>None': 1, 'one_cycle>None': 0, 'lr_low>None': 1.309786927349962e-07, 'lr_high>None': 0.040434500278020764, 'n_epoch>None': 0, 'wd>None': 0, 'pct_start>None': 0.85}. Best is trial 0 with value: 2.933249212057006e+32.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-07 01:17:10,397] Trial 2 finished with value: 2.9332495989132683e+32 and parameters: {'dls_action>None': 0, 'batch_size>dls_action': 0, 'model_action>None': 0, 'start_neurons>model_action': 5, 'preprocess>model_action': 1, 'activation>model_action': 2, 'loss_func>None': 0, 'metric>None': 0, 'freeze>None': 1, 'gradient_clip>None': 1, 'one_cycle>None': 0, 'lr_low>None': 7.516429390595025e-06, 'lr_high>None': 0.009498687338470443, 'n_epoch>None': 6, 'wd>None': 2, 'pct_start>None': 0.2}. Best is trial 0 with value: 2.933249212057006e+32.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 2 finished with value: 2.9332495989132683e+32 and parameters: {'dls_action>None': 0, 'batch_size>dls_action': 0, 'model_action>None': 0, 'start_neurons>model_action': 5, 'preprocess>model_action': 1, 'activation>model_action': 2, 'loss_func>None': 0, 'metric>None': 0, 'freeze>None': 1, 'gradient_clip>None': 1, 'one_cycle>None': 0, 'lr_low>None': 7.516429390595025e-06, 'lr_high>None': 0.009498687338470443, 'n_epoch>None': 6, 'wd>None': 2, 'pct_start>None': 0.2}. Best is trial 0 with value: 2.933249212057006e+32.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-07 01:17:12,516] Trial 3 finished with value: 2.933249018628875e+32 and parameters: {'dls_action>None': 0, 'batch_size>dls_action': 4, 'model_action>None': 1, 'dropout_p>model_action': 0.6100000000000001, 'start_neurons>model_action': 1, 'preprocess>model_action': 0, 'activation>model_action': 2, 'loss_func>None': 0, 'metric>None': 0, 'freeze>None': 0, 'gradient_clip>None': 1, 'one_cycle>None': 1, 'n_epoch>None': 6, 'lr>None': 0.09434613213428941, 'wd>None': 2}. Best is trial 3 with value: 2.933249018628875e+32.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 3 finished with value: 2.933249018628875e+32 and parameters: {'dls_action>None': 0, 'batch_size>dls_action': 4, 'model_action>None': 1, 'dropout_p>model_action': 0.6100000000000001, 'start_neurons>model_action': 1, 'preprocess>model_action': 0, 'activation>model_action': 2, 'loss_func>None': 0, 'metric>None': 0, 'freeze>None': 0, 'gradient_clip>None': 1, 'one_cycle>None': 1, 'n_epoch>None': 6, 'lr>None': 0.09434613213428941, 'wd>None': 2}. Best is trial 3 with value: 2.933249018628875e+32.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-07 01:19:48,861] A new study created in memory with name: no-name-c74dfb69-f269-4580-b98e-efb9678c1bd7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "****************************************\n",
            "CombinedSampler:\n",
            "**********ActionSampler**********\n",
            "Var of type: model_action, signature of: None\n",
            "Configuration Space:\n",
            "'choices' = [<__main__.ModelAction object at 0x7f72bfc7de40>, <__main__.ModelAction object at 0x7f72bfc7de40>, <__main__.ModelAction object at 0x7f72bfc7d2a0>]\n",
            "\n",
            "Var of type: dls_action, signature of: None\n",
            "Configuration Space:\n",
            "'choices' = [<__main__.Action object at 0x7f72bfc7eb60>]\n",
            "\n",
            "\n",
            "**********PrimitiveSampler**********\n",
            "Var of type: pct_start, signature of: None\n",
            "Configuration Space:\n",
            "'low': 0.2, 'high': 0.2, 'log': False, 'step': 0.05\n",
            "\n",
            "Var of type: wd, signature of: None\n",
            "Configuration Space:\n",
            "'choices' = [0.01, 0.01, 0.1]\n",
            "\n",
            "Var of type: one_cycle, signature of: None\n",
            "Configuration Space:\n",
            "'choices' = [True, False, False]\n",
            "\n",
            "Var of type: n_epoch, signature of: None\n",
            "Configuration Space:\n",
            "'choices' = [4, 7, 7]\n",
            "\n",
            "Var of type: lr_low, signature of: None\n",
            "Configuration Space:\n",
            "'low': 7.516429390595025e-06, 'high': 7.516429390595025e-06, 'log': True, 'step': unspecified\n",
            "\n",
            "Var of type: lr, signature of: None\n",
            "Configuration Space:\n",
            "'low': 3.5957951740910273e-06, 'high': 0.09434613213428941, 'log': True, 'step': unspecified\n",
            "\n",
            "Var of type: metric, signature of: None\n",
            "Configuration Space:\n",
            "'choices' = [<function custom_mse at 0x7f72bfc51d80>]\n",
            "\n",
            "Var of type: gradient_clip, signature of: None\n",
            "Configuration Space:\n",
            "'choices' = [True, False, False]\n",
            "\n",
            "Var of type: loss_func, signature of: None\n",
            "Configuration Space:\n",
            "'choices' = [<function custom_mse at 0x7f72bfc51d80>]\n",
            "\n",
            "Var of type: lr_high, signature of: None\n",
            "Configuration Space:\n",
            "'low': 0.009498687338470443, 'high': 0.009498687338470443, 'log': True, 'step': unspecified\n",
            "\n",
            "Var of type: freeze, signature of: None\n",
            "Configuration Space:\n",
            "'choices' = [True, True, False]\n",
            "\n",
            "Var of type: max_norm, signature of: None\n",
            "Configuration Space:\n",
            "'low': 2.4066637230764645, 'high': 2.4066637230764645, 'log': False, 'step': unspecified\n",
            "\n",
            "\n",
            "\n",
            "****************************************\n",
            "\n",
            "*****************RUNNING: HPO PROCESS*****************\n",
            "A new study created in memory with name: no-name-c74dfb69-f269-4580-b98e-efb9678c1bd7\n",
            "A new study created in memory with name: no-name-c74dfb69-f269-4580-b98e-efb9678c1bd7\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-07 01:21:25,681] Trial 0 finished with value: 2.9332486317726126e+32 and parameters: {'dls_action>None': 0, 'batch_size>dls_action': 0, 'model_action>None': 1, 'dropout_p>model_action': 0.36000000000000004, 'start_neurons>model_action': 1, 'preprocess>model_action': 1, 'activation>model_action': 1, 'loss_func>None': 0, 'metric>None': 0, 'freeze>None': 1, 'gradient_clip>None': 1, 'one_cycle>None': 0, 'lr_low>None': 3.024013888998857e-05, 'lr_high>None': 0.0021211137048254103, 'n_epoch>None': 5, 'wd>None': 0, 'pct_start>None': 0.1}. Best is trial 0 with value: 2.9332486317726126e+32.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 0 finished with value: 2.9332486317726126e+32 and parameters: {'dls_action>None': 0, 'batch_size>dls_action': 0, 'model_action>None': 1, 'dropout_p>model_action': 0.36000000000000004, 'start_neurons>model_action': 1, 'preprocess>model_action': 1, 'activation>model_action': 1, 'loss_func>None': 0, 'metric>None': 0, 'freeze>None': 1, 'gradient_clip>None': 1, 'one_cycle>None': 0, 'lr_low>None': 3.024013888998857e-05, 'lr_high>None': 0.0021211137048254103, 'n_epoch>None': 5, 'wd>None': 0, 'pct_start>None': 0.1}. Best is trial 0 with value: 2.9332486317726126e+32.\n",
            "Trial 0 finished with value: 2.9332486317726126e+32 and parameters: {'dls_action>None': 0, 'batch_size>dls_action': 0, 'model_action>None': 1, 'dropout_p>model_action': 0.36000000000000004, 'start_neurons>model_action': 1, 'preprocess>model_action': 1, 'activation>model_action': 1, 'loss_func>None': 0, 'metric>None': 0, 'freeze>None': 1, 'gradient_clip>None': 1, 'one_cycle>None': 0, 'lr_low>None': 3.024013888998857e-05, 'lr_high>None': 0.0021211137048254103, 'n_epoch>None': 5, 'wd>None': 0, 'pct_start>None': 0.1}. Best is trial 0 with value: 2.9332486317726126e+32.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-07 01:21:38,872] Trial 1 finished with value: 2.933250759482055e+32 and parameters: {'dls_action>None': 0, 'batch_size>dls_action': 3, 'model_action>None': 1, 'dropout_p>model_action': 0.41000000000000003, 'start_neurons>model_action': 0, 'preprocess>model_action': 0, 'activation>model_action': 2, 'loss_func>None': 0, 'metric>None': 0, 'freeze>None': 1, 'gradient_clip>None': 1, 'one_cycle>None': 1, 'n_epoch>None': 6, 'lr>None': 0.0005581288399463272, 'wd>None': 3}. Best is trial 0 with value: 2.9332486317726126e+32.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 1 finished with value: 2.933250759482055e+32 and parameters: {'dls_action>None': 0, 'batch_size>dls_action': 3, 'model_action>None': 1, 'dropout_p>model_action': 0.41000000000000003, 'start_neurons>model_action': 0, 'preprocess>model_action': 0, 'activation>model_action': 2, 'loss_func>None': 0, 'metric>None': 0, 'freeze>None': 1, 'gradient_clip>None': 1, 'one_cycle>None': 1, 'n_epoch>None': 6, 'lr>None': 0.0005581288399463272, 'wd>None': 3}. Best is trial 0 with value: 2.9332486317726126e+32.\n",
            "Trial 1 finished with value: 2.933250759482055e+32 and parameters: {'dls_action>None': 0, 'batch_size>dls_action': 3, 'model_action>None': 1, 'dropout_p>model_action': 0.41000000000000003, 'start_neurons>model_action': 0, 'preprocess>model_action': 0, 'activation>model_action': 2, 'loss_func>None': 0, 'metric>None': 0, 'freeze>None': 1, 'gradient_clip>None': 1, 'one_cycle>None': 1, 'n_epoch>None': 6, 'lr>None': 0.0005581288399463272, 'wd>None': 3}. Best is trial 0 with value: 2.9332486317726126e+32.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-07 01:23:15,705] A new study created in memory with name: no-name-ddc14544-e444-466c-91b9-f19793a99e2a\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "****************************************\n",
            "CombinedSampler:\n",
            "**********ActionSampler**********\n",
            "Var of type: model_action, signature of: None\n",
            "Configuration Space:\n",
            "'choices' = [<__main__.ModelAction object at 0x7f72bfc7ec80>]\n",
            "\n",
            "Var of type: dls_action, signature of: None\n",
            "Configuration Space:\n",
            "'choices' = [<__main__.Action object at 0x7f72bfc7c4c0>]\n",
            "\n",
            "\n",
            "**********PrimitiveSampler**********\n",
            "Var of type: pct_start, signature of: None\n",
            "Configuration Space:\n",
            "'low': 0.1, 'high': 0.1, 'log': False, 'step': 0.05\n",
            "\n",
            "Var of type: wd, signature of: None\n",
            "Configuration Space:\n",
            "'choices' = [0.0001]\n",
            "\n",
            "Var of type: one_cycle, signature of: None\n",
            "Configuration Space:\n",
            "'choices' = [True]\n",
            "\n",
            "Var of type: n_epoch, signature of: None\n",
            "Configuration Space:\n",
            "'choices' = [6]\n",
            "\n",
            "Var of type: lr_low, signature of: None\n",
            "Configuration Space:\n",
            "'low': 3.024013888998857e-05, 'high': 3.024013888998857e-05, 'log': True, 'step': unspecified\n",
            "\n",
            "Var of type: lr, signature of: None\n",
            "Configuration Space:\n",
            "'low': 1e-06, 'high': 0.1, 'log': True, 'step': unspecified\n",
            "\n",
            "Var of type: metric, signature of: None\n",
            "Configuration Space:\n",
            "'choices' = [<function custom_mse at 0x7f72bfc51d80>]\n",
            "\n",
            "Var of type: gradient_clip, signature of: None\n",
            "Configuration Space:\n",
            "'choices' = [False]\n",
            "\n",
            "Var of type: loss_func, signature of: None\n",
            "Configuration Space:\n",
            "'choices' = [<function custom_mse at 0x7f72bfc51d80>]\n",
            "\n",
            "Var of type: lr_high, signature of: None\n",
            "Configuration Space:\n",
            "'low': 0.0021211137048254103, 'high': 0.0021211137048254103, 'log': True, 'step': unspecified\n",
            "\n",
            "Var of type: freeze, signature of: None\n",
            "Configuration Space:\n",
            "'choices' = [False]\n",
            "\n",
            "Var of type: max_norm, signature of: None\n",
            "Configuration Space:\n",
            "'low': 0.0, 'high': 15.0, 'log': False, 'step': unspecified\n",
            "\n",
            "\n",
            "\n",
            "****************************************\n",
            "\n",
            "*****************RUNNING: HPO PROCESS*****************\n",
            "A new study created in memory with name: no-name-ddc14544-e444-466c-91b9-f19793a99e2a\n",
            "A new study created in memory with name: no-name-ddc14544-e444-466c-91b9-f19793a99e2a\n",
            "A new study created in memory with name: no-name-ddc14544-e444-466c-91b9-f19793a99e2a\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-07 01:23:19,394] Trial 0 finished with value: 2.9332499857695306e+32 and parameters: {'dls_action>None': 0, 'batch_size>dls_action': 5, 'model_action>None': 1, 'dropout_p>model_action': 0.76, 'start_neurons>model_action': 5, 'preprocess>model_action': 1, 'activation>model_action': 2, 'loss_func>None': 0, 'metric>None': 0, 'freeze>None': 1, 'gradient_clip>None': 0, 'max_norm>None': 6.430835055871464, 'one_cycle>None': 1, 'n_epoch>None': 5, 'lr>None': 0.02147583062895726, 'wd>None': 0}. Best is trial 0 with value: 2.9332499857695306e+32.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 0 finished with value: 2.9332499857695306e+32 and parameters: {'dls_action>None': 0, 'batch_size>dls_action': 5, 'model_action>None': 1, 'dropout_p>model_action': 0.76, 'start_neurons>model_action': 5, 'preprocess>model_action': 1, 'activation>model_action': 2, 'loss_func>None': 0, 'metric>None': 0, 'freeze>None': 1, 'gradient_clip>None': 0, 'max_norm>None': 6.430835055871464, 'one_cycle>None': 1, 'n_epoch>None': 5, 'lr>None': 0.02147583062895726, 'wd>None': 0}. Best is trial 0 with value: 2.9332499857695306e+32.\n",
            "Trial 0 finished with value: 2.9332499857695306e+32 and parameters: {'dls_action>None': 0, 'batch_size>dls_action': 5, 'model_action>None': 1, 'dropout_p>model_action': 0.76, 'start_neurons>model_action': 5, 'preprocess>model_action': 1, 'activation>model_action': 2, 'loss_func>None': 0, 'metric>None': 0, 'freeze>None': 1, 'gradient_clip>None': 0, 'max_norm>None': 6.430835055871464, 'one_cycle>None': 1, 'n_epoch>None': 5, 'lr>None': 0.02147583062895726, 'wd>None': 0}. Best is trial 0 with value: 2.9332499857695306e+32.\n",
            "Trial 0 finished with value: 2.9332499857695306e+32 and parameters: {'dls_action>None': 0, 'batch_size>dls_action': 5, 'model_action>None': 1, 'dropout_p>model_action': 0.76, 'start_neurons>model_action': 5, 'preprocess>model_action': 1, 'activation>model_action': 2, 'loss_func>None': 0, 'metric>None': 0, 'freeze>None': 1, 'gradient_clip>None': 0, 'max_norm>None': 6.430835055871464, 'one_cycle>None': 1, 'n_epoch>None': 5, 'lr>None': 0.02147583062895726, 'wd>None': 0}. Best is trial 0 with value: 2.9332499857695306e+32.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ".[I 2024-10-07 01:23:23,058] A new study created in memory with name: no-name-1e17498a-9d99-4553-9d94-6459bed125f7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "****************************************\n",
            "CombinedSampler:\n",
            "**********ActionSampler**********\n",
            "Var of type: model_action, signature of: None\n",
            "Configuration Space:\n",
            "'choices' = [<__main__.ModelAction object at 0x7f72bfc9c2b0>]\n",
            "\n",
            "Var of type: dls_action, signature of: None\n",
            "Configuration Space:\n",
            "'choices' = [<__main__.Action object at 0x7f72bfc9fa60>]\n",
            "\n",
            "\n",
            "**********PrimitiveSampler**********\n",
            "Var of type: pct_start, signature of: None\n",
            "Configuration Space:\n",
            "'low': 0.1, 'high': 0.95, 'log': False, 'step': 0.05\n",
            "\n",
            "Var of type: wd, signature of: None\n",
            "Configuration Space:\n",
            "'choices' = [0.0001]\n",
            "\n",
            "Var of type: one_cycle, signature of: None\n",
            "Configuration Space:\n",
            "'choices' = [False]\n",
            "\n",
            "Var of type: n_epoch, signature of: None\n",
            "Configuration Space:\n",
            "'choices' = [6]\n",
            "\n",
            "Var of type: lr_low, signature of: None\n",
            "Configuration Space:\n",
            "'low': 1e-07, 'high': 0.0009, 'log': True, 'step': unspecified\n",
            "\n",
            "Var of type: lr, signature of: None\n",
            "Configuration Space:\n",
            "'low': 0.02147583062895726, 'high': 0.02147583062895726, 'log': True, 'step': unspecified\n",
            "\n",
            "Var of type: metric, signature of: None\n",
            "Configuration Space:\n",
            "'choices' = [<function custom_mse at 0x7f72bfc51d80>]\n",
            "\n",
            "Var of type: gradient_clip, signature of: None\n",
            "Configuration Space:\n",
            "'choices' = [True]\n",
            "\n",
            "Var of type: loss_func, signature of: None\n",
            "Configuration Space:\n",
            "'choices' = [<function custom_mse at 0x7f72bfc51d80>]\n",
            "\n",
            "Var of type: lr_high, signature of: None\n",
            "Configuration Space:\n",
            "'low': 0.0009, 'high': 0.1, 'log': True, 'step': unspecified\n",
            "\n",
            "Var of type: freeze, signature of: None\n",
            "Configuration Space:\n",
            "'choices' = [False]\n",
            "\n",
            "Var of type: max_norm, signature of: None\n",
            "Configuration Space:\n",
            "'low': 6.430835055871464, 'high': 6.430835055871464, 'log': False, 'step': unspecified\n",
            "\n",
            "\n",
            "\n",
            "****************************************\n",
            "\n",
            "A new study created in memory with name: no-name-1e17498a-9d99-4553-9d94-6459bed125f7\n",
            "A new study created in memory with name: no-name-1e17498a-9d99-4553-9d94-6459bed125f7\n",
            "A new study created in memory with name: no-name-1e17498a-9d99-4553-9d94-6459bed125f7\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ".[I 2024-10-07 01:24:49,021] A new study created in memory with name: no-name-42b5ecf9-eb5c-43e8-a5b1-d8bd9e40e596\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A new study created in memory with name: no-name-42b5ecf9-eb5c-43e8-a5b1-d8bd9e40e596\n",
            "A new study created in memory with name: no-name-42b5ecf9-eb5c-43e8-a5b1-d8bd9e40e596\n",
            "A new study created in memory with name: no-name-42b5ecf9-eb5c-43e8-a5b1-d8bd9e40e596\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ".\n",
            "----------------------------------------------------------------------\n",
            "Ran 4 tests in 582.504s\n",
            "\n",
            "OK\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Class: Auto"
      ],
      "metadata": {
        "id": "TbbIOisFWDBE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Auto:\n",
        "    def __init__(self, INIT_CONFIG: ConfigSpace, storage_dir: str = '/content/trial_storage/'):\n",
        "        if not isinstance(INIT_CONFIG, ConfigSpace):\n",
        "            raise ValueError(f\"Input 'INIT_CONFIG' must be an instance of ConfigSpace but found: {type(INIT_CONFIG)}\")\n",
        "\n",
        "        #Save it as a member.\n",
        "        self.init_config = INIT_CONFIG\n",
        "        #Verify that the newly-built ConfigSpace is correct BOTH in terms of syntax AND objective.\n",
        "        self.init_config.verify_syntax(warn_missing_sig_var_types=False)\n",
        "        self.init_config.verify_objective(warn_missing_var_types=False)\n",
        "\n",
        "        self.combined_sampler = CombinedSampler(INIT_CONFIG=self.init_config)\n",
        "\n",
        "        if self.combined_sampler.is_fixed():\n",
        "            raise ValueError(\"Input 'INIT_SOURCE' is completely determined from the beginning. Nothing to optimize for\")\n",
        "\n",
        "        self.prev_top_k_trials = []\n",
        "        self.storage_dir = storage_dir\n",
        "        os.makedirs(self.storage_dir, exist_ok=True)\n",
        "        self._n_study = None\n",
        "\n",
        "    def optimize(self, n_auto: int, top_k: int, update_state_window: int, device: torch.device):\n",
        "        #At the start of each optimization request, we set Objective.objective\n",
        "        setattr(Objective, 'execute_objective', self.init_config.objective.__func__)\n",
        "        #This verifies that the objective function, which might have been changed by the user,\n",
        "        try:\n",
        "            self.init_config.verify_syntax(warn_missing_sig_var_types=False)\n",
        "            self.init_config.verify_objective(warn_missing_var_types=False)\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Following error is raised, possibly due to user's modification to ConfigSpace's objective after Auto has been initialzied: {e}\")\n",
        "\n",
        "        #Now, do the optimization.\n",
        "        list_n_trials = self._get_list_n_trials(n_auto=n_auto)\n",
        "        print(\"*****************************************************************\")\n",
        "        print(f\"n_auto: {n_auto}, list_n_trials: {list_n_trials}\")\n",
        "        print()\n",
        "\n",
        "        for idx, n_trials in enumerate(list_n_trials):\n",
        "            hpo_pointer = HpoTrainer(\n",
        "                combined_sampler=self.combined_sampler,\n",
        "                n_trials=n_trials,\n",
        "                device=device,\n",
        "                prev_top_k_trials=self.prev_top_k_trials\n",
        "            )\n",
        "            optimized_study = hpo_pointer.run()\n",
        "            self._update(\n",
        "                idx=idx,\n",
        "                optimized_study=optimized_study,\n",
        "                top_k=top_k,\n",
        "                update_state_window=update_state_window,\n",
        "                device=device\n",
        "            )\n",
        "            self._n_study = len(list_n_trials)\n",
        "\n",
        "    def num_study(self):\n",
        "        return self._n_study\n",
        "\n",
        "    def get_study(self, idx: int):\n",
        "        if self._n_study is None:\n",
        "            raise SyntaxError(\".visualization_trials is requested w/o saved data. Call .optimize() first and once it's finished, request visualization again.\")\n",
        "        if idx < 0 or idx >= self._n_study:\n",
        "            raise ValueError(f\"Input 'idx':{idx} must satisfy 0 <= idx < {self._n_study} but failed.\")\n",
        "\n",
        "        return self._load_study_from_pickle(idx)\n",
        "\n",
        "    def _get_list_n_trials(self, n_auto: int):\n",
        "        \"\"\"\n",
        "        ACCEPTS:\n",
        "        n_auto := Positive integer power of 2. IF n_auto <= 1, THEN n_auto <- 2\n",
        "                                               IF n_auto > 1 but NOT a power of 2, THEN n_auto <- (nearest power of 2) - 1.\n",
        "        RETURNS:\n",
        "        Exponential distribution of [2^m, 2^(m-1), ..., 2^1, 2^0] + (n_auto==power of 2)*[2^0] trials returned.\n",
        "        \"\"\"\n",
        "        n_auto = max(n_auto, 2)\n",
        "        pow2 = n_auto & (n_auto-1) == 0\n",
        "\n",
        "        output = [1<<e for e in range(math.ceil(math.log2(n_auto)) - 1, -1, -1)]\n",
        "\n",
        "        if pow2:\n",
        "            output.append(1)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def _update(self, idx: int, optimized_study: optuna.study, top_k: int, update_state_window: int, device: torch.device):\n",
        "        prev_trials = optimized_study.trials\n",
        "        top_k = min(top_k, len(prev_trials))\n",
        "        update_state_window = min(update_state_window, top_k)\n",
        "\n",
        "        top_k_trials = sorted(prev_trials, key=lambda trial: trial.value if trial.value is not None else float('inf'))[:top_k]\n",
        "\n",
        "        #Save the whole study, not just top_k_trials\n",
        "        self._save_study_to_pickle(idx, optimized_study)\n",
        "        self.prev_top_k_trials = top_k_trials\n",
        "\n",
        "        self.combined_sampler.update(\n",
        "            top_k_trials=top_k_trials,\n",
        "            update_state_window=update_state_window,\n",
        "            device=device\n",
        "        )\n",
        "\n",
        "    def _save_study_to_pickle(self, idx: int, study: optuna.study):\n",
        "        save_path = os.path.join(self.storage_dir, f'study_{idx}.pkl')\n",
        "        with gzip.open(save_path, 'wb') as f:\n",
        "            pickle.dump(study, f)\n",
        "        print(f\"Saved study to: {save_path}\")\n",
        "\n",
        "    def _load_study_from_pickle(self, idx: int):\n",
        "        load_path = os.path.join(self.storage_dir, f'study_{idx}.pkl')\n",
        "        if not os.path.exists(load_path):\n",
        "            raise FileNotFoundError(f\"No study found at: {load_path}\")\n",
        "\n",
        "        with gzip.open(load_path, 'rb') as f:\n",
        "            study = pickle.load(f)\n",
        "        print(f\"Loaded study from: {load_path}\")\n",
        "        return study"
      ],
      "metadata": {
        "id": "GnUHhaN3lUyu"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Test: Auto"
      ],
      "metadata": {
        "id": "vDa7V_pjgwS7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    from torch.utils.data import Dataset\n",
        "\n",
        "    class PolynomialDataset(Dataset):\n",
        "        def __init__(self, coeffs: torch.tensor, input_min: float, input_max: float, input_size: int):\n",
        "            #coeffs.shape == [degree+1, 1]\n",
        "            self.coeffs = coeffs.clone().detach()\n",
        "            #inputs.shape == [inputs]\n",
        "            self.inputs = torch.empty(size=(input_size,)).uniform_(input_min, input_max)\n",
        "            #Preprocess the output and retrieve it upon request.\n",
        "            self.outputs = self._f(X=self.inputs)\n",
        "\n",
        "        def __getitem__(self, idx):\n",
        "            return self.inputs[idx], self.outputs[idx]\n",
        "\n",
        "        def __len__(self):\n",
        "            if self.inputs.size(0) != self.outputs.size(0):\n",
        "                return ValueError(f\"len(inputs): {self.inputs.size(0)} != len(outputs): {self.outputs.size(0)}\")\n",
        "            return self.inputs.size(0)\n",
        "\n",
        "        def _f(self, X):\n",
        "            #power.shape == [1, degree+1]\n",
        "            powers = torch.arange(self.coeffs.size(0)).unsqueeze(0)\n",
        "            Y = self.inputs.unsqueeze(-1)**powers\n",
        "            return torch.matmul(Y, self.coeffs).squeeze(-1)\n",
        "\n",
        "    degree = 6\n",
        "    coeffs_min, coeffs_max = -10.0, 10.0\n",
        "    coeffs = torch.empty(size=(degree+1,)).uniform_(coeffs_min, coeffs_max)\n",
        "\n",
        "    for c in coeffs:\n",
        "        print(f\"c: {c}\")\n",
        "\n",
        "    train_dset = PolynomialDataset(coeffs=coeffs, input_min=-100.0, input_max=100.0, input_size=8000)\n",
        "    valid_dset = PolynomialDataset(coeffs=coeffs, input_min=-300.12, input_max=500.78, input_size=2000)\n",
        "\n",
        "    #Test:\n",
        "    print(f\"Number of elements in Dataset; train_dset: {len(train_dset)}, valid_dset: {len(valid_dset)}\")\n",
        "    print()\n",
        "\n",
        "    print(f\"train first input: {train_dset[0][0]}, train first output: {train_dset[0][1]}\")\n",
        "    print()\n",
        "\n",
        "    print(f\"valid first input: {valid_dset[0][0]}, valid first output: {valid_dset[0][1]}\")\n",
        "    print()\n",
        "\n",
        "    \"\"\"\n",
        "    Action functions:\n",
        "    \"\"\"\n",
        "    class ModelDefault(nn.Module):\n",
        "        def __init__(self, activation: nn.Module, preprocess: bool, start_neurons: int):\n",
        "            super().__init__()\n",
        "            self.preprocess = preprocess\n",
        "            if self.preprocess: start_neurons = max(start_neurons,degree+1)\n",
        "            self.net = nn.Sequential(\n",
        "                nn.Linear(in_features=degree+1 if self.preprocess else 1,\n",
        "                        out_features=start_neurons),\n",
        "                nn.BatchNorm1d(num_features=start_neurons),\n",
        "                activation(),\n",
        "                nn.Linear(in_features=start_neurons,out_features=start_neurons*2),\n",
        "                nn.BatchNorm1d(num_features=start_neurons*2),\n",
        "                activation(),\n",
        "                nn.Linear(in_features=start_neurons*2,out_features=1)\n",
        "            )\n",
        "\n",
        "        def _preprocess(self, X):\n",
        "            N = X.size(0)\n",
        "            #X <- [64,1]x[1,7] == [64,7], where each index (i,j) yields X[i]**j.\n",
        "            X = X.unsqueeze(-1)**torch.arange(degree+1).unsqueeze(0)\n",
        "            if X.size() != torch.Size([N,degree+1]):\n",
        "                raise ValueError(f\"X.size() MUST be torch.Size([{N},{degree+1}]) but found X.size()=={X.size()}\")\n",
        "            return X\n",
        "\n",
        "        def forward(self, X):\n",
        "            if X.dim() != 1:\n",
        "                raise ValueError(f\"X.dim() MUST be 1 but found X.dim()=={X.dim()}\")\n",
        "\n",
        "            if self.preprocess:\n",
        "                X = self._preprocess(X)\n",
        "            else:\n",
        "                X = X.unsqueeze(-1)\n",
        "            return self.net(X)\n",
        "\n",
        "    class ModelDropout(nn.Module):\n",
        "        def __init__(self, activation: nn.Module, dropout_p: int, preprocess: int, start_neurons: Int):\n",
        "            super().__init__()\n",
        "            self.preprocess = preprocess\n",
        "            if self.preprocess: start_neurons = max(start_neurons,degree+1)\n",
        "            self.net = nn.Sequential(\n",
        "                nn.Linear(in_features=degree+1 if self.preprocess else 1,\n",
        "                        out_features=start_neurons),\n",
        "                nn.BatchNorm1d(num_features=start_neurons),\n",
        "                activation(),\n",
        "                nn.Dropout(p=dropout_p),\n",
        "                nn.Linear(in_features=start_neurons,out_features=start_neurons*2),\n",
        "                nn.BatchNorm1d(num_features=start_neurons*2),\n",
        "                activation(),\n",
        "                nn.Linear(in_features=start_neurons*2,out_features=1)\n",
        "            )\n",
        "\n",
        "        def _preprocess(self, X):\n",
        "            N = X.size(0)\n",
        "            #X <- [64,1]x[1,7] == [64,7], where each index (i,j) yields X[i]**j.\n",
        "            X = X.unsqueeze(-1)**torch.arange(degree+1).unsqueeze(0)\n",
        "            if X.size() != torch.Size([N,degree+1]):\n",
        "                raise ValueError(f\"X.size() MUST be torch.Size([{N},{degree+1}]) but found X.size()=={X.size()}\")\n",
        "            return X\n",
        "\n",
        "        def forward(self, X):\n",
        "            if X.dim() != 1:\n",
        "                raise ValueError(f\"X.dim() MUST be 1 but found X.dim()=={X.dim()}\")\n",
        "\n",
        "            if self.preprocess:\n",
        "                X = self._preprocess(X)\n",
        "            else:\n",
        "                X = X.unsqueeze(-1)\n",
        "            return self.net(X)\n",
        "\n",
        "    def action_default(activation: nn.Module, preprocess: bool, start_neurons: int, device: torch.device):\n",
        "        if start_neurons < 1:\n",
        "            raise ValueError(f\"start_neurons MUST be a positive integer but found start_neurons: {start_neurons}\")\n",
        "        return ModelDefault(activation=activation,\n",
        "                            preprocess=preprocess,\n",
        "                            start_neurons=start_neurons).to(device)\n",
        "\n",
        "    def action_dropout(activation: nn.Module, dropout_p: int, preprocess: bool, start_neurons: int, device: torch.device):\n",
        "        if start_neurons < 1:\n",
        "            raise ValueError(f\"start_neurons MUST be a positive integer but found start_neurons: {start_neurons}\")\n",
        "        if dropout_p < 0.0 or dropout_p >= 1.0:\n",
        "            raise ValueError(f\"0.0 <= dropout_p < 1.0 required but found dropout_p: {dropout_p}\")\n",
        "        return ModelDropout(activation=activation,\n",
        "                            dropout_p=dropout_p,\n",
        "                            preprocess=preprocess,\n",
        "                            start_neurons=start_neurons).to(device)\n",
        "\n",
        "    #Define DataLoaders and training process\n",
        "    def get_dls(batch_size: int):\n",
        "        train_dataloader = DataLoader(train_dset, batch_size=batch_size, shuffle=True)\n",
        "        valid_dataloader = DataLoader(valid_dset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "        dls = DataLoaders(train_dataloader, valid_dataloader)\n",
        "        return dls\n",
        "\n",
        "    # Now you can use this DataLoaders object with Learner\n",
        "    # Example usage:\n",
        "    train_dls, valid_dls = get_dls(batch_size=64)\n",
        "\n",
        "    print(f\"len(train_dls): {len(train_dls)}\")\n",
        "    print(f\"len(valid_dls): {len(valid_dls)}\")\n",
        "    print()\n",
        "\n",
        "    print(\"*****CHECKING data integrity of returned 'dls'******\")\n",
        "    for blob in iter(train_dls):\n",
        "        if not isinstance(blob, tuple):\n",
        "            raise ValueError(f\"Sampled 'blob' from 'iter(train_dls)' MUST have a type 'tuple' but found: {type(blob)}\")\n",
        "        inputs, outputs = blob\n",
        "        if inputs.size() != outputs.size():\n",
        "            raise ValueError(f\"train_dls contains (inputs,outputs) where inputs.size(): {inputs.size()} != outputs.size(): {outputs.size()}\")\n",
        "\n",
        "    for blob in iter(valid_dls):\n",
        "        if not isinstance(blob, tuple):\n",
        "            raise ValueError(f\"Sampled 'blob' from 'iter(valid_dls)' MUST have a type 'tuple' but found: {type(blob)}\")\n",
        "        inputs, outputs = blob\n",
        "        if inputs.size() != outputs.size():\n",
        "            raise ValueError(f\"valid_dls contains (inputs,outputs) where inputs.size(): {inputs.size()} != outputs.size(): {outputs.size()}\")\n",
        "\n",
        "    inputs, _ = next(iter(train_dls))\n",
        "    print(inputs.size())\n",
        "    inputs, _ = next(iter(valid_dls))\n",
        "    print(inputs.size())\n",
        "\n",
        "    model_default = ModelDefault(activation=nn.ReLU, preprocess=False, start_neurons=7)\n",
        "    model_dropout = ModelDropout(activation=nn.ReLU, dropout_p=0.5, preprocess=False, start_neurons=7)\n",
        "\n",
        "    print(\"*********************model_default*********************\")\n",
        "    print(model_default)\n",
        "    print()\n",
        "    print(\"*********************model_dropout*********************\")\n",
        "    print(model_dropout)\n",
        "    print()\n",
        "\n",
        "    model_container = Learner(\n",
        "        dls=get_dls(batch_size=64),\n",
        "        model=ModelDefault(activation=nn.ReLU, preprocess=False, start_neurons=7),\n",
        "        loss_func=custom_mse,\n",
        "        metrics=[custom_mse]\n",
        "    )\n",
        "    model_container.fit_one_cycle(n_epoch=10, lr_max=slice(1e-6,1e-3), pct_start=0.3, wd=0.01)"
      ],
      "metadata": {
        "id": "ObeKjyVhLgvX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fb079dfa-8d5d-4ffd-b457-3c34ba1540b7"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "c: -8.256911277770996\n",
            "c: 2.7582085132598877\n",
            "c: 8.531203269958496\n",
            "c: -5.975925922393799\n",
            "c: -7.2286248207092285\n",
            "c: 8.258957862854004\n",
            "c: -0.5422282218933105\n",
            "Number of elements in Dataset; train_dset: 8000, valid_dset: 2000\n",
            "\n",
            "train first input: 32.70071792602539, train first output: -362659904.0\n",
            "\n",
            "valid first input: 483.7815246582031, valid first output: -6733022124572672.0\n",
            "\n",
            "len(train_dls): 125\n",
            "len(valid_dls): 32\n",
            "\n",
            "*****CHECKING data integrity of returned 'dls'******\n",
            "torch.Size([64])\n",
            "torch.Size([64])\n",
            "*********************model_default*********************\n",
            "ModelDefault(\n",
            "  (net): Sequential(\n",
            "    (0): Linear(in_features=1, out_features=7, bias=True)\n",
            "    (1): BatchNorm1d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU()\n",
            "    (3): Linear(in_features=7, out_features=14, bias=True)\n",
            "    (4): BatchNorm1d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU()\n",
            "    (6): Linear(in_features=14, out_features=1, bias=True)\n",
            "  )\n",
            ")\n",
            "\n",
            "*********************model_dropout*********************\n",
            "ModelDropout(\n",
            "  (net): Sequential(\n",
            "    (0): Linear(in_features=1, out_features=7, bias=True)\n",
            "    (1): BatchNorm1d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU()\n",
            "    (3): Dropout(p=0.5, inplace=False)\n",
            "    (4): Linear(in_features=7, out_features=14, bias=True)\n",
            "    (5): BatchNorm1d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (6): ReLU()\n",
            "    (7): Linear(in_features=14, out_features=1, bias=True)\n",
            "  )\n",
            ")\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>custom_mse</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>23401500800951141793792.000000</td>\n",
              "      <td>3173531826257249212454387318784.000000</td>\n",
              "      <td>3173531826257249212454387318784.000000</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>24074018829706750590976.000000</td>\n",
              "      <td>3173532430720159019768974671872.000000</td>\n",
              "      <td>3173532430720159019768974671872.000000</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>23002779111541522300928.000000</td>\n",
              "      <td>3173532128488704116111680995328.000000</td>\n",
              "      <td>3173532128488704116111680995328.000000</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>23810074114545634246656.000000</td>\n",
              "      <td>3173532128488704116111680995328.000000</td>\n",
              "      <td>3173532128488704116111680995328.000000</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>24002530941021685022720.000000</td>\n",
              "      <td>3173532430720159019768974671872.000000</td>\n",
              "      <td>3173532430720159019768974671872.000000</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>23257342828678825902080.000000</td>\n",
              "      <td>3173532128488704116111680995328.000000</td>\n",
              "      <td>3173532128488704116111680995328.000000</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>23449169151207044808704.000000</td>\n",
              "      <td>3173532128488704116111680995328.000000</td>\n",
              "      <td>3173532128488704116111680995328.000000</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>23670347684306650923008.000000</td>\n",
              "      <td>3173531826257249212454387318784.000000</td>\n",
              "      <td>3173531826257249212454387318784.000000</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>23844515392695950114816.000000</td>\n",
              "      <td>3173531826257249212454387318784.000000</td>\n",
              "      <td>3173531826257249212454387318784.000000</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>24109068093806761476096.000000</td>\n",
              "      <td>3173532430720159019768974671872.000000</td>\n",
              "      <td>3173532430720159019768974671872.000000</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    def custom_mse(Y_hat, Y):\n",
        "        Y_hat = Y_hat.reshape(-1)\n",
        "        Y = Y.reshape(-1)\n",
        "        if Y_hat.size() != Y.size():\n",
        "            raise ValueError(f\"Y_hat.size(): {Y_hat.size()} != Y.size(): {Y.size()}\")\n",
        "        return nn.MSELoss()(Y_hat, Y)\n",
        "\n",
        "    INIT_DICT = {\n",
        "        ('dls_action--action', None): {\n",
        "            'params': {'choices': [get_dls]},\n",
        "            'sample': 'categorical'\n",
        "        },\n",
        "        ('freeze', None) : {\n",
        "            'params': {'choices': [True, False]},\n",
        "            'sample': 'categorical'\n",
        "        },\n",
        "        ('lr_low', None) : {\n",
        "            'params': {'low': 1e-7, 'high': 9e-4, 'log': True},\n",
        "            'sample': 'float'\n",
        "        },\n",
        "        ('lr_high', None) : {\n",
        "            'params': {'low': 9e-4, 'high': 1e-1, 'log': True},\n",
        "            'sample': 'float'\n",
        "        },\n",
        "        ('lr', None) : {\n",
        "            'params': {'low': 1e-6, 'high': 1e-1, 'log': True},\n",
        "            'sample': 'float'\n",
        "        },\n",
        "        ('wd', None) : {\n",
        "            'params': {'choices': [1e-4, 1e-3, 1e-2, 1e-1]},\n",
        "            'sample': 'categorical'\n",
        "        },\n",
        "        ('gradient_clip', None) : {\n",
        "            'params': {'choices': [True, False]},\n",
        "            'sample': 'categorical'\n",
        "        },\n",
        "        ('max_norm', None) : {\n",
        "            'params': {'low': 0.0, 'high': 15.0, 'log': False},\n",
        "            'sample': 'float'\n",
        "        },\n",
        "        ('one_cycle', None) : {\n",
        "            'params': {'choices': [True, False]},\n",
        "            'sample': 'categorical'\n",
        "        },\n",
        "        ('pct_start', None) : {\n",
        "            'params': {'low': 0.10, 'high': 0.95,  'log': False, 'step': 0.05},\n",
        "            'sample': 'float'\n",
        "        },\n",
        "        ('n_epoch', None) : {\n",
        "            'params': {'choices': [5, 10, 15]},\n",
        "            'sample': 'categorical'\n",
        "        },\n",
        "        ('model_action--action', None) : {\n",
        "            'params': {'choices': [action_default, action_dropout]},\n",
        "            'sample': 'categorical'\n",
        "        },\n",
        "        ('activation', 'model_action') : {\n",
        "            'params': {'choices': [nn.ReLU, nn.PReLU, nn.SiLU]},\n",
        "            'sample': 'categorical'\n",
        "        },\n",
        "        ('loss_func', None) : {\n",
        "            'params': {'choices': [custom_mse]},\n",
        "            'sample': 'categorical'\n",
        "        },\n",
        "        ('metric', None) : {\n",
        "            'params': {'choices': [custom_mse]},\n",
        "            'sample': 'categorical'\n",
        "        },\n",
        "        ('preprocess', 'model_action') : {\n",
        "            'params': {'choices': [True, False]},\n",
        "            'sample': 'categorical'\n",
        "        },\n",
        "        ('start_neurons', 'model_action') : {\n",
        "            'params': {'choices': [1, 2, 4, 8, 16, 32,]},\n",
        "            'sample': 'categorical'\n",
        "        },\n",
        "        ('dropout_p', 'model_action') : {\n",
        "            'params': {'low': 0.05, 'high': 0.95,  'log': False, 'step': 0.05},\n",
        "            'sample': 'float'\n",
        "        },\n",
        "        ('batch_size', 'dls_action') : {\n",
        "            'params': {'choices': [32, 64, 128]},\n",
        "            'sample': 'categorical'\n",
        "        }\n",
        "    }\n",
        "\n",
        "    init_config = UserConfigSpace(INIT_DICT)\n",
        "    auto = Auto(init_config)"
      ],
      "metadata": {
        "id": "xsIcM8kI8eWc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d84ee358-5d00-414e-fb46-75f11ab94e63"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-102-48ea770699ec>:193: UserWarning: Action hyperparameter 'model_action' requires additional hyperparameters: [('dropout_p', 'model_action'), ('start_neurons', 'model_action'), ('preprocess', 'model_action'), ('activation', 'model_action')]\n",
            "\n",
            "  warnings.warn(msg)\n",
            "<ipython-input-102-48ea770699ec>:193: UserWarning: Action hyperparameter 'model_action' requires additional hyperparameters: [('dropout_p', 'model_action'), ('start_neurons', 'model_action'), ('preprocess', 'model_action'), ('activation', 'model_action')]\n",
            "Action hyperparameter 'dls_action' requires additional hyperparameters: [('batch_size', 'dls_action')]\n",
            "\n",
            "  warnings.warn(msg)\n",
            "<ipython-input-102-48ea770699ec>:193: UserWarning: Action hyperparameter 'model_action' requires additional hyperparameters: [('dropout_p', 'model_action'), ('start_neurons', 'model_action'), ('preprocess', 'model_action')]\n",
            "Action hyperparameter 'dls_action' requires additional hyperparameters: [('batch_size', 'dls_action')]\n",
            "\n",
            "  warnings.warn(msg)\n",
            "<ipython-input-102-48ea770699ec>:193: UserWarning: Action hyperparameter 'model_action' requires additional hyperparameters: [('dropout_p', 'model_action'), ('start_neurons', 'model_action')]\n",
            "Action hyperparameter 'dls_action' requires additional hyperparameters: [('batch_size', 'dls_action')]\n",
            "\n",
            "  warnings.warn(msg)\n",
            "<ipython-input-102-48ea770699ec>:193: UserWarning: Action hyperparameter 'model_action' requires additional hyperparameters: [('dropout_p', 'model_action'), ('start_neurons', 'model_action')]\n",
            "\n",
            "  warnings.warn(msg)\n",
            "<ipython-input-102-48ea770699ec>:193: UserWarning: Action hyperparameter 'model_action' requires additional hyperparameters: [('dropout_p', 'model_action')]\n",
            "\n",
            "  warnings.warn(msg)\n",
            "<ipython-input-102-48ea770699ec>:142: UserWarning: Following calls in '.objective' are referring to non-existing var_type(s) and will simply return respective default_val(s): ['self.sample(min_delta,0.1)', 'self.sample(patience,15)'].\n",
            "  warnings.warn(f\"Following calls in '.objective' are referring to non-existing var_type(s) and will simply return respective default_val(s): {formatted}.\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#FINAL_TEST"
      ],
      "metadata": {
        "id": "jDCVotsAbqxV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "auto.optimize(\n",
        "    n_auto=10,\n",
        "    top_k=2,\n",
        "    update_state_window=2,\n",
        "    device=torch.device('cpu')\n",
        ")\n",
        "\n",
        "#Print out the optimized combined_sampler.\n",
        "print()\n",
        "print(\"*****************************************************************\")\n",
        "print()\n",
        "print(auto.combined_sampler)\n",
        "print()\n",
        "print(\"*****************************************************************\")\n",
        "print()\n",
        "\n",
        "#TEST: Optuna's visualization capabilities\n",
        "print(auto.num_study())\n",
        "study = auto.get_study(2)\n",
        "#One can now perform all the optuna visualizations as one sees fit\n",
        "optuna.visualization.plot_optimization_history(study)"
      ],
      "metadata": {
        "id": "7bBSlC_p-gIR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0c16044d-e7ab-41dc-82cd-b216251efb22"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-102-48ea770699ec>:142: UserWarning: Following calls in '.objective' are referring to non-existing var_type(s) and will simply return respective default_val(s): ['self.sample(min_delta,0.1)', 'self.sample(patience,15)'].\n",
            "  warnings.warn(f\"Following calls in '.objective' are referring to non-existing var_type(s) and will simply return respective default_val(s): {formatted}.\")\n",
            "[I 2024-10-07 04:20:03,124] A new study created in memory with name: no-name-53bb76d4-4842-4695-818e-1be2107cbffd\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*****************************************************************\n",
            "n_auto: 10, list_n_trials: [8, 4, 2, 1]\n",
            "\n",
            "*****************RUNNING: HPO PROCESS*****************\n",
            "A new study created in memory with name: no-name-53bb76d4-4842-4695-818e-1be2107cbffd\n",
            "A new study created in memory with name: no-name-53bb76d4-4842-4695-818e-1be2107cbffd\n",
            "A new study created in memory with name: no-name-53bb76d4-4842-4695-818e-1be2107cbffd\n",
            "A new study created in memory with name: no-name-53bb76d4-4842-4695-818e-1be2107cbffd\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-07 04:20:09,288] Trial 0 finished with value: 3.173531826257249e+30 and parameters: {'dls_action>None': 0, 'batch_size>dls_action': 2, 'model_action>None': 1, 'dropout_p>model_action': 0.8, 'start_neurons>model_action': 1, 'preprocess>model_action': 0, 'activation>model_action': 0, 'loss_func>None': 0, 'metric>None': 0, 'freeze>None': 0, 'gradient_clip>None': 1, 'one_cycle>None': 1, 'n_epoch>None': 2, 'lr>None': 0.004740235218431013, 'wd>None': 1}. Best is trial 0 with value: 3.173531826257249e+30.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 0 finished with value: 3.173531826257249e+30 and parameters: {'dls_action>None': 0, 'batch_size>dls_action': 2, 'model_action>None': 1, 'dropout_p>model_action': 0.8, 'start_neurons>model_action': 1, 'preprocess>model_action': 0, 'activation>model_action': 0, 'loss_func>None': 0, 'metric>None': 0, 'freeze>None': 0, 'gradient_clip>None': 1, 'one_cycle>None': 1, 'n_epoch>None': 2, 'lr>None': 0.004740235218431013, 'wd>None': 1}. Best is trial 0 with value: 3.173531826257249e+30.\n",
            "Trial 0 finished with value: 3.173531826257249e+30 and parameters: {'dls_action>None': 0, 'batch_size>dls_action': 2, 'model_action>None': 1, 'dropout_p>model_action': 0.8, 'start_neurons>model_action': 1, 'preprocess>model_action': 0, 'activation>model_action': 0, 'loss_func>None': 0, 'metric>None': 0, 'freeze>None': 0, 'gradient_clip>None': 1, 'one_cycle>None': 1, 'n_epoch>None': 2, 'lr>None': 0.004740235218431013, 'wd>None': 1}. Best is trial 0 with value: 3.173531826257249e+30.\n",
            "Trial 0 finished with value: 3.173531826257249e+30 and parameters: {'dls_action>None': 0, 'batch_size>dls_action': 2, 'model_action>None': 1, 'dropout_p>model_action': 0.8, 'start_neurons>model_action': 1, 'preprocess>model_action': 0, 'activation>model_action': 0, 'loss_func>None': 0, 'metric>None': 0, 'freeze>None': 0, 'gradient_clip>None': 1, 'one_cycle>None': 1, 'n_epoch>None': 2, 'lr>None': 0.004740235218431013, 'wd>None': 1}. Best is trial 0 with value: 3.173531826257249e+30.\n",
            "Trial 0 finished with value: 3.173531826257249e+30 and parameters: {'dls_action>None': 0, 'batch_size>dls_action': 2, 'model_action>None': 1, 'dropout_p>model_action': 0.8, 'start_neurons>model_action': 1, 'preprocess>model_action': 0, 'activation>model_action': 0, 'loss_func>None': 0, 'metric>None': 0, 'freeze>None': 0, 'gradient_clip>None': 1, 'one_cycle>None': 1, 'n_epoch>None': 2, 'lr>None': 0.004740235218431013, 'wd>None': 1}. Best is trial 0 with value: 3.173531826257249e+30.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-07 04:20:12,679] Trial 1 finished with value: 3.173532128488704e+30 and parameters: {'dls_action>None': 0, 'batch_size>dls_action': 1, 'model_action>None': 0, 'start_neurons>model_action': 5, 'preprocess>model_action': 0, 'activation>model_action': 2, 'loss_func>None': 0, 'metric>None': 0, 'freeze>None': 1, 'gradient_clip>None': 1, 'one_cycle>None': 1, 'n_epoch>None': 0, 'lr>None': 0.0024608937892963054, 'wd>None': 0}. Best is trial 0 with value: 3.173531826257249e+30.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 1 finished with value: 3.173532128488704e+30 and parameters: {'dls_action>None': 0, 'batch_size>dls_action': 1, 'model_action>None': 0, 'start_neurons>model_action': 5, 'preprocess>model_action': 0, 'activation>model_action': 2, 'loss_func>None': 0, 'metric>None': 0, 'freeze>None': 1, 'gradient_clip>None': 1, 'one_cycle>None': 1, 'n_epoch>None': 0, 'lr>None': 0.0024608937892963054, 'wd>None': 0}. Best is trial 0 with value: 3.173531826257249e+30.\n",
            "Trial 1 finished with value: 3.173532128488704e+30 and parameters: {'dls_action>None': 0, 'batch_size>dls_action': 1, 'model_action>None': 0, 'start_neurons>model_action': 5, 'preprocess>model_action': 0, 'activation>model_action': 2, 'loss_func>None': 0, 'metric>None': 0, 'freeze>None': 1, 'gradient_clip>None': 1, 'one_cycle>None': 1, 'n_epoch>None': 0, 'lr>None': 0.0024608937892963054, 'wd>None': 0}. Best is trial 0 with value: 3.173531826257249e+30.\n",
            "Trial 1 finished with value: 3.173532128488704e+30 and parameters: {'dls_action>None': 0, 'batch_size>dls_action': 1, 'model_action>None': 0, 'start_neurons>model_action': 5, 'preprocess>model_action': 0, 'activation>model_action': 2, 'loss_func>None': 0, 'metric>None': 0, 'freeze>None': 1, 'gradient_clip>None': 1, 'one_cycle>None': 1, 'n_epoch>None': 0, 'lr>None': 0.0024608937892963054, 'wd>None': 0}. Best is trial 0 with value: 3.173531826257249e+30.\n",
            "Trial 1 finished with value: 3.173532128488704e+30 and parameters: {'dls_action>None': 0, 'batch_size>dls_action': 1, 'model_action>None': 0, 'start_neurons>model_action': 5, 'preprocess>model_action': 0, 'activation>model_action': 2, 'loss_func>None': 0, 'metric>None': 0, 'freeze>None': 1, 'gradient_clip>None': 1, 'one_cycle>None': 1, 'n_epoch>None': 0, 'lr>None': 0.0024608937892963054, 'wd>None': 0}. Best is trial 0 with value: 3.173531826257249e+30.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-07 04:20:16,560] Trial 2 finished with value: 3.1735315240257943e+30 and parameters: {'dls_action>None': 0, 'batch_size>dls_action': 1, 'model_action>None': 0, 'start_neurons>model_action': 4, 'preprocess>model_action': 0, 'activation>model_action': 0, 'loss_func>None': 0, 'metric>None': 0, 'freeze>None': 1, 'gradient_clip>None': 0, 'max_norm>None': 13.19017290924006, 'one_cycle>None': 0, 'lr_low>None': 0.0005876567979976106, 'lr_high>None': 0.07571540414942228, 'n_epoch>None': 0, 'wd>None': 3, 'pct_start>None': 0.2}. Best is trial 2 with value: 3.1735315240257943e+30.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 2 finished with value: 3.1735315240257943e+30 and parameters: {'dls_action>None': 0, 'batch_size>dls_action': 1, 'model_action>None': 0, 'start_neurons>model_action': 4, 'preprocess>model_action': 0, 'activation>model_action': 0, 'loss_func>None': 0, 'metric>None': 0, 'freeze>None': 1, 'gradient_clip>None': 0, 'max_norm>None': 13.19017290924006, 'one_cycle>None': 0, 'lr_low>None': 0.0005876567979976106, 'lr_high>None': 0.07571540414942228, 'n_epoch>None': 0, 'wd>None': 3, 'pct_start>None': 0.2}. Best is trial 2 with value: 3.1735315240257943e+30.\n",
            "Trial 2 finished with value: 3.1735315240257943e+30 and parameters: {'dls_action>None': 0, 'batch_size>dls_action': 1, 'model_action>None': 0, 'start_neurons>model_action': 4, 'preprocess>model_action': 0, 'activation>model_action': 0, 'loss_func>None': 0, 'metric>None': 0, 'freeze>None': 1, 'gradient_clip>None': 0, 'max_norm>None': 13.19017290924006, 'one_cycle>None': 0, 'lr_low>None': 0.0005876567979976106, 'lr_high>None': 0.07571540414942228, 'n_epoch>None': 0, 'wd>None': 3, 'pct_start>None': 0.2}. Best is trial 2 with value: 3.1735315240257943e+30.\n",
            "Trial 2 finished with value: 3.1735315240257943e+30 and parameters: {'dls_action>None': 0, 'batch_size>dls_action': 1, 'model_action>None': 0, 'start_neurons>model_action': 4, 'preprocess>model_action': 0, 'activation>model_action': 0, 'loss_func>None': 0, 'metric>None': 0, 'freeze>None': 1, 'gradient_clip>None': 0, 'max_norm>None': 13.19017290924006, 'one_cycle>None': 0, 'lr_low>None': 0.0005876567979976106, 'lr_high>None': 0.07571540414942228, 'n_epoch>None': 0, 'wd>None': 3, 'pct_start>None': 0.2}. Best is trial 2 with value: 3.1735315240257943e+30.\n",
            "Trial 2 finished with value: 3.1735315240257943e+30 and parameters: {'dls_action>None': 0, 'batch_size>dls_action': 1, 'model_action>None': 0, 'start_neurons>model_action': 4, 'preprocess>model_action': 0, 'activation>model_action': 0, 'loss_func>None': 0, 'metric>None': 0, 'freeze>None': 1, 'gradient_clip>None': 0, 'max_norm>None': 13.19017290924006, 'one_cycle>None': 0, 'lr_low>None': 0.0005876567979976106, 'lr_high>None': 0.07571540414942228, 'n_epoch>None': 0, 'wd>None': 3, 'pct_start>None': 0.2}. Best is trial 2 with value: 3.1735315240257943e+30.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-07 04:20:18,909] Trial 3 finished with value: 3.173532430720159e+30 and parameters: {'dls_action>None': 0, 'batch_size>dls_action': 2, 'model_action>None': 0, 'start_neurons>model_action': 2, 'preprocess>model_action': 0, 'activation>model_action': 2, 'loss_func>None': 0, 'metric>None': 0, 'freeze>None': 1, 'gradient_clip>None': 0, 'max_norm>None': 0.2169150139280629, 'one_cycle>None': 0, 'lr_low>None': 3.92592323908208e-05, 'lr_high>None': 0.05810390856948783, 'n_epoch>None': 2, 'wd>None': 3, 'pct_start>None': 0.7000000000000001}. Best is trial 2 with value: 3.1735315240257943e+30.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 3 finished with value: 3.173532430720159e+30 and parameters: {'dls_action>None': 0, 'batch_size>dls_action': 2, 'model_action>None': 0, 'start_neurons>model_action': 2, 'preprocess>model_action': 0, 'activation>model_action': 2, 'loss_func>None': 0, 'metric>None': 0, 'freeze>None': 1, 'gradient_clip>None': 0, 'max_norm>None': 0.2169150139280629, 'one_cycle>None': 0, 'lr_low>None': 3.92592323908208e-05, 'lr_high>None': 0.05810390856948783, 'n_epoch>None': 2, 'wd>None': 3, 'pct_start>None': 0.7000000000000001}. Best is trial 2 with value: 3.1735315240257943e+30.\n",
            "Trial 3 finished with value: 3.173532430720159e+30 and parameters: {'dls_action>None': 0, 'batch_size>dls_action': 2, 'model_action>None': 0, 'start_neurons>model_action': 2, 'preprocess>model_action': 0, 'activation>model_action': 2, 'loss_func>None': 0, 'metric>None': 0, 'freeze>None': 1, 'gradient_clip>None': 0, 'max_norm>None': 0.2169150139280629, 'one_cycle>None': 0, 'lr_low>None': 3.92592323908208e-05, 'lr_high>None': 0.05810390856948783, 'n_epoch>None': 2, 'wd>None': 3, 'pct_start>None': 0.7000000000000001}. Best is trial 2 with value: 3.1735315240257943e+30.\n",
            "Trial 3 finished with value: 3.173532430720159e+30 and parameters: {'dls_action>None': 0, 'batch_size>dls_action': 2, 'model_action>None': 0, 'start_neurons>model_action': 2, 'preprocess>model_action': 0, 'activation>model_action': 2, 'loss_func>None': 0, 'metric>None': 0, 'freeze>None': 1, 'gradient_clip>None': 0, 'max_norm>None': 0.2169150139280629, 'one_cycle>None': 0, 'lr_low>None': 3.92592323908208e-05, 'lr_high>None': 0.05810390856948783, 'n_epoch>None': 2, 'wd>None': 3, 'pct_start>None': 0.7000000000000001}. Best is trial 2 with value: 3.1735315240257943e+30.\n",
            "Trial 3 finished with value: 3.173532430720159e+30 and parameters: {'dls_action>None': 0, 'batch_size>dls_action': 2, 'model_action>None': 0, 'start_neurons>model_action': 2, 'preprocess>model_action': 0, 'activation>model_action': 2, 'loss_func>None': 0, 'metric>None': 0, 'freeze>None': 1, 'gradient_clip>None': 0, 'max_norm>None': 0.2169150139280629, 'one_cycle>None': 0, 'lr_low>None': 3.92592323908208e-05, 'lr_high>None': 0.05810390856948783, 'n_epoch>None': 2, 'wd>None': 3, 'pct_start>None': 0.7000000000000001}. Best is trial 2 with value: 3.1735315240257943e+30.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-07 04:20:25,220] Trial 4 finished with value: 3.173532128488704e+30 and parameters: {'dls_action>None': 0, 'batch_size>dls_action': 0, 'model_action>None': 0, 'start_neurons>model_action': 1, 'preprocess>model_action': 1, 'activation>model_action': 0, 'loss_func>None': 0, 'metric>None': 0, 'freeze>None': 1, 'gradient_clip>None': 0, 'max_norm>None': 2.6203047347758863, 'one_cycle>None': 1, 'n_epoch>None': 0, 'lr>None': 0.00045754465785329034, 'wd>None': 0}. Best is trial 2 with value: 3.1735315240257943e+30.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 4 finished with value: 3.173532128488704e+30 and parameters: {'dls_action>None': 0, 'batch_size>dls_action': 0, 'model_action>None': 0, 'start_neurons>model_action': 1, 'preprocess>model_action': 1, 'activation>model_action': 0, 'loss_func>None': 0, 'metric>None': 0, 'freeze>None': 1, 'gradient_clip>None': 0, 'max_norm>None': 2.6203047347758863, 'one_cycle>None': 1, 'n_epoch>None': 0, 'lr>None': 0.00045754465785329034, 'wd>None': 0}. Best is trial 2 with value: 3.1735315240257943e+30.\n",
            "Trial 4 finished with value: 3.173532128488704e+30 and parameters: {'dls_action>None': 0, 'batch_size>dls_action': 0, 'model_action>None': 0, 'start_neurons>model_action': 1, 'preprocess>model_action': 1, 'activation>model_action': 0, 'loss_func>None': 0, 'metric>None': 0, 'freeze>None': 1, 'gradient_clip>None': 0, 'max_norm>None': 2.6203047347758863, 'one_cycle>None': 1, 'n_epoch>None': 0, 'lr>None': 0.00045754465785329034, 'wd>None': 0}. Best is trial 2 with value: 3.1735315240257943e+30.\n",
            "Trial 4 finished with value: 3.173532128488704e+30 and parameters: {'dls_action>None': 0, 'batch_size>dls_action': 0, 'model_action>None': 0, 'start_neurons>model_action': 1, 'preprocess>model_action': 1, 'activation>model_action': 0, 'loss_func>None': 0, 'metric>None': 0, 'freeze>None': 1, 'gradient_clip>None': 0, 'max_norm>None': 2.6203047347758863, 'one_cycle>None': 1, 'n_epoch>None': 0, 'lr>None': 0.00045754465785329034, 'wd>None': 0}. Best is trial 2 with value: 3.1735315240257943e+30.\n",
            "Trial 4 finished with value: 3.173532128488704e+30 and parameters: {'dls_action>None': 0, 'batch_size>dls_action': 0, 'model_action>None': 0, 'start_neurons>model_action': 1, 'preprocess>model_action': 1, 'activation>model_action': 0, 'loss_func>None': 0, 'metric>None': 0, 'freeze>None': 1, 'gradient_clip>None': 0, 'max_norm>None': 2.6203047347758863, 'one_cycle>None': 1, 'n_epoch>None': 0, 'lr>None': 0.00045754465785329034, 'wd>None': 0}. Best is trial 2 with value: 3.1735315240257943e+30.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-07 04:20:31,472] Trial 5 finished with value: 3.173532430720159e+30 and parameters: {'dls_action>None': 0, 'batch_size>dls_action': 0, 'model_action>None': 0, 'start_neurons>model_action': 2, 'preprocess>model_action': 0, 'activation>model_action': 0, 'loss_func>None': 0, 'metric>None': 0, 'freeze>None': 1, 'gradient_clip>None': 0, 'max_norm>None': 1.5529053621894773, 'one_cycle>None': 1, 'n_epoch>None': 0, 'lr>None': 1.0450210295757338e-05, 'wd>None': 0}. Best is trial 2 with value: 3.1735315240257943e+30.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 5 finished with value: 3.173532430720159e+30 and parameters: {'dls_action>None': 0, 'batch_size>dls_action': 0, 'model_action>None': 0, 'start_neurons>model_action': 2, 'preprocess>model_action': 0, 'activation>model_action': 0, 'loss_func>None': 0, 'metric>None': 0, 'freeze>None': 1, 'gradient_clip>None': 0, 'max_norm>None': 1.5529053621894773, 'one_cycle>None': 1, 'n_epoch>None': 0, 'lr>None': 1.0450210295757338e-05, 'wd>None': 0}. Best is trial 2 with value: 3.1735315240257943e+30.\n",
            "Trial 5 finished with value: 3.173532430720159e+30 and parameters: {'dls_action>None': 0, 'batch_size>dls_action': 0, 'model_action>None': 0, 'start_neurons>model_action': 2, 'preprocess>model_action': 0, 'activation>model_action': 0, 'loss_func>None': 0, 'metric>None': 0, 'freeze>None': 1, 'gradient_clip>None': 0, 'max_norm>None': 1.5529053621894773, 'one_cycle>None': 1, 'n_epoch>None': 0, 'lr>None': 1.0450210295757338e-05, 'wd>None': 0}. Best is trial 2 with value: 3.1735315240257943e+30.\n",
            "Trial 5 finished with value: 3.173532430720159e+30 and parameters: {'dls_action>None': 0, 'batch_size>dls_action': 0, 'model_action>None': 0, 'start_neurons>model_action': 2, 'preprocess>model_action': 0, 'activation>model_action': 0, 'loss_func>None': 0, 'metric>None': 0, 'freeze>None': 1, 'gradient_clip>None': 0, 'max_norm>None': 1.5529053621894773, 'one_cycle>None': 1, 'n_epoch>None': 0, 'lr>None': 1.0450210295757338e-05, 'wd>None': 0}. Best is trial 2 with value: 3.1735315240257943e+30.\n",
            "Trial 5 finished with value: 3.173532430720159e+30 and parameters: {'dls_action>None': 0, 'batch_size>dls_action': 0, 'model_action>None': 0, 'start_neurons>model_action': 2, 'preprocess>model_action': 0, 'activation>model_action': 0, 'loss_func>None': 0, 'metric>None': 0, 'freeze>None': 1, 'gradient_clip>None': 0, 'max_norm>None': 1.5529053621894773, 'one_cycle>None': 1, 'n_epoch>None': 0, 'lr>None': 1.0450210295757338e-05, 'wd>None': 0}. Best is trial 2 with value: 3.1735315240257943e+30.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-07 04:20:38,164] Trial 6 finished with value: 3.173532128488704e+30 and parameters: {'dls_action>None': 0, 'batch_size>dls_action': 0, 'model_action>None': 1, 'dropout_p>model_action': 0.9000000000000001, 'start_neurons>model_action': 2, 'preprocess>model_action': 1, 'activation>model_action': 1, 'loss_func>None': 0, 'metric>None': 0, 'freeze>None': 0, 'gradient_clip>None': 1, 'one_cycle>None': 0, 'lr_low>None': 4.630554785630503e-06, 'lr_high>None': 0.04000642978214308, 'n_epoch>None': 0, 'wd>None': 2, 'pct_start>None': 0.25}. Best is trial 2 with value: 3.1735315240257943e+30.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 6 finished with value: 3.173532128488704e+30 and parameters: {'dls_action>None': 0, 'batch_size>dls_action': 0, 'model_action>None': 1, 'dropout_p>model_action': 0.9000000000000001, 'start_neurons>model_action': 2, 'preprocess>model_action': 1, 'activation>model_action': 1, 'loss_func>None': 0, 'metric>None': 0, 'freeze>None': 0, 'gradient_clip>None': 1, 'one_cycle>None': 0, 'lr_low>None': 4.630554785630503e-06, 'lr_high>None': 0.04000642978214308, 'n_epoch>None': 0, 'wd>None': 2, 'pct_start>None': 0.25}. Best is trial 2 with value: 3.1735315240257943e+30.\n",
            "Trial 6 finished with value: 3.173532128488704e+30 and parameters: {'dls_action>None': 0, 'batch_size>dls_action': 0, 'model_action>None': 1, 'dropout_p>model_action': 0.9000000000000001, 'start_neurons>model_action': 2, 'preprocess>model_action': 1, 'activation>model_action': 1, 'loss_func>None': 0, 'metric>None': 0, 'freeze>None': 0, 'gradient_clip>None': 1, 'one_cycle>None': 0, 'lr_low>None': 4.630554785630503e-06, 'lr_high>None': 0.04000642978214308, 'n_epoch>None': 0, 'wd>None': 2, 'pct_start>None': 0.25}. Best is trial 2 with value: 3.1735315240257943e+30.\n",
            "Trial 6 finished with value: 3.173532128488704e+30 and parameters: {'dls_action>None': 0, 'batch_size>dls_action': 0, 'model_action>None': 1, 'dropout_p>model_action': 0.9000000000000001, 'start_neurons>model_action': 2, 'preprocess>model_action': 1, 'activation>model_action': 1, 'loss_func>None': 0, 'metric>None': 0, 'freeze>None': 0, 'gradient_clip>None': 1, 'one_cycle>None': 0, 'lr_low>None': 4.630554785630503e-06, 'lr_high>None': 0.04000642978214308, 'n_epoch>None': 0, 'wd>None': 2, 'pct_start>None': 0.25}. Best is trial 2 with value: 3.1735315240257943e+30.\n",
            "Trial 6 finished with value: 3.173532128488704e+30 and parameters: {'dls_action>None': 0, 'batch_size>dls_action': 0, 'model_action>None': 1, 'dropout_p>model_action': 0.9000000000000001, 'start_neurons>model_action': 2, 'preprocess>model_action': 1, 'activation>model_action': 1, 'loss_func>None': 0, 'metric>None': 0, 'freeze>None': 0, 'gradient_clip>None': 1, 'one_cycle>None': 0, 'lr_low>None': 4.630554785630503e-06, 'lr_high>None': 0.04000642978214308, 'n_epoch>None': 0, 'wd>None': 2, 'pct_start>None': 0.25}. Best is trial 2 with value: 3.1735315240257943e+30.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-07 04:20:41,974] Trial 7 finished with value: 3.173532128488704e+30 and parameters: {'dls_action>None': 0, 'batch_size>dls_action': 2, 'model_action>None': 0, 'start_neurons>model_action': 1, 'preprocess>model_action': 1, 'activation>model_action': 1, 'loss_func>None': 0, 'metric>None': 0, 'freeze>None': 0, 'gradient_clip>None': 1, 'one_cycle>None': 1, 'n_epoch>None': 1, 'lr>None': 0.020329845638255805, 'wd>None': 0}. Best is trial 2 with value: 3.1735315240257943e+30.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 7 finished with value: 3.173532128488704e+30 and parameters: {'dls_action>None': 0, 'batch_size>dls_action': 2, 'model_action>None': 0, 'start_neurons>model_action': 1, 'preprocess>model_action': 1, 'activation>model_action': 1, 'loss_func>None': 0, 'metric>None': 0, 'freeze>None': 0, 'gradient_clip>None': 1, 'one_cycle>None': 1, 'n_epoch>None': 1, 'lr>None': 0.020329845638255805, 'wd>None': 0}. Best is trial 2 with value: 3.1735315240257943e+30.\n",
            "Trial 7 finished with value: 3.173532128488704e+30 and parameters: {'dls_action>None': 0, 'batch_size>dls_action': 2, 'model_action>None': 0, 'start_neurons>model_action': 1, 'preprocess>model_action': 1, 'activation>model_action': 1, 'loss_func>None': 0, 'metric>None': 0, 'freeze>None': 0, 'gradient_clip>None': 1, 'one_cycle>None': 1, 'n_epoch>None': 1, 'lr>None': 0.020329845638255805, 'wd>None': 0}. Best is trial 2 with value: 3.1735315240257943e+30.\n",
            "Trial 7 finished with value: 3.173532128488704e+30 and parameters: {'dls_action>None': 0, 'batch_size>dls_action': 2, 'model_action>None': 0, 'start_neurons>model_action': 1, 'preprocess>model_action': 1, 'activation>model_action': 1, 'loss_func>None': 0, 'metric>None': 0, 'freeze>None': 0, 'gradient_clip>None': 1, 'one_cycle>None': 1, 'n_epoch>None': 1, 'lr>None': 0.020329845638255805, 'wd>None': 0}. Best is trial 2 with value: 3.1735315240257943e+30.\n",
            "Trial 7 finished with value: 3.173532128488704e+30 and parameters: {'dls_action>None': 0, 'batch_size>dls_action': 2, 'model_action>None': 0, 'start_neurons>model_action': 1, 'preprocess>model_action': 1, 'activation>model_action': 1, 'loss_func>None': 0, 'metric>None': 0, 'freeze>None': 0, 'gradient_clip>None': 1, 'one_cycle>None': 1, 'n_epoch>None': 1, 'lr>None': 0.020329845638255805, 'wd>None': 0}. Best is trial 2 with value: 3.1735315240257943e+30.\n",
            "Saved study to: /content/trial_storage/study_0.pkl\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-07 04:20:50,527] A new study created in memory with name: no-name-64f4ed20-b449-42d8-aa78-075fbd1646c4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*****************RUNNING: HPO PROCESS*****************\n",
            "A new study created in memory with name: no-name-64f4ed20-b449-42d8-aa78-075fbd1646c4\n",
            "A new study created in memory with name: no-name-64f4ed20-b449-42d8-aa78-075fbd1646c4\n",
            "A new study created in memory with name: no-name-64f4ed20-b449-42d8-aa78-075fbd1646c4\n",
            "A new study created in memory with name: no-name-64f4ed20-b449-42d8-aa78-075fbd1646c4\n",
            "A new study created in memory with name: no-name-64f4ed20-b449-42d8-aa78-075fbd1646c4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-07 04:20:54,359] Trial 0 finished with value: 3.1735315240257943e+30 and parameters: {'dls_action>None': 0, 'batch_size>dls_action': 1, 'model_action>None': 0, 'loss_func>None': 0, 'metric>None': 0, 'gradient_clip>None': 0, 'max_norm>None': 13.19017290924006, 'one_cycle>None': 0, 'lr_low>None': 0.0005876567979976106, 'lr_high>None': 0.07571540414942228, 'n_epoch>None': 0, 'wd>None': 3, 'pct_start>None': 0.2}. Best is trial 0 with value: 3.1735315240257943e+30.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 0 finished with value: 3.1735315240257943e+30 and parameters: {'dls_action>None': 0, 'batch_size>dls_action': 1, 'model_action>None': 0, 'loss_func>None': 0, 'metric>None': 0, 'gradient_clip>None': 0, 'max_norm>None': 13.19017290924006, 'one_cycle>None': 0, 'lr_low>None': 0.0005876567979976106, 'lr_high>None': 0.07571540414942228, 'n_epoch>None': 0, 'wd>None': 3, 'pct_start>None': 0.2}. Best is trial 0 with value: 3.1735315240257943e+30.\n",
            "Trial 0 finished with value: 3.1735315240257943e+30 and parameters: {'dls_action>None': 0, 'batch_size>dls_action': 1, 'model_action>None': 0, 'loss_func>None': 0, 'metric>None': 0, 'gradient_clip>None': 0, 'max_norm>None': 13.19017290924006, 'one_cycle>None': 0, 'lr_low>None': 0.0005876567979976106, 'lr_high>None': 0.07571540414942228, 'n_epoch>None': 0, 'wd>None': 3, 'pct_start>None': 0.2}. Best is trial 0 with value: 3.1735315240257943e+30.\n",
            "Trial 0 finished with value: 3.1735315240257943e+30 and parameters: {'dls_action>None': 0, 'batch_size>dls_action': 1, 'model_action>None': 0, 'loss_func>None': 0, 'metric>None': 0, 'gradient_clip>None': 0, 'max_norm>None': 13.19017290924006, 'one_cycle>None': 0, 'lr_low>None': 0.0005876567979976106, 'lr_high>None': 0.07571540414942228, 'n_epoch>None': 0, 'wd>None': 3, 'pct_start>None': 0.2}. Best is trial 0 with value: 3.1735315240257943e+30.\n",
            "Trial 0 finished with value: 3.1735315240257943e+30 and parameters: {'dls_action>None': 0, 'batch_size>dls_action': 1, 'model_action>None': 0, 'loss_func>None': 0, 'metric>None': 0, 'gradient_clip>None': 0, 'max_norm>None': 13.19017290924006, 'one_cycle>None': 0, 'lr_low>None': 0.0005876567979976106, 'lr_high>None': 0.07571540414942228, 'n_epoch>None': 0, 'wd>None': 3, 'pct_start>None': 0.2}. Best is trial 0 with value: 3.1735315240257943e+30.\n",
            "Trial 0 finished with value: 3.1735315240257943e+30 and parameters: {'dls_action>None': 0, 'batch_size>dls_action': 1, 'model_action>None': 0, 'loss_func>None': 0, 'metric>None': 0, 'gradient_clip>None': 0, 'max_norm>None': 13.19017290924006, 'one_cycle>None': 0, 'lr_low>None': 0.0005876567979976106, 'lr_high>None': 0.07571540414942228, 'n_epoch>None': 0, 'wd>None': 3, 'pct_start>None': 0.2}. Best is trial 0 with value: 3.1735315240257943e+30.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-07 04:21:00,412] Trial 1 finished with value: 3.173532128488704e+30 and parameters: {'dls_action>None': 0, 'batch_size>dls_action': 2, 'model_action>None': 1, 'loss_func>None': 0, 'metric>None': 0, 'gradient_clip>None': 1, 'one_cycle>None': 1, 'n_epoch>None': 2, 'lr>None': 0.004740235218431013, 'wd>None': 1}. Best is trial 0 with value: 3.1735315240257943e+30.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 1 finished with value: 3.173532128488704e+30 and parameters: {'dls_action>None': 0, 'batch_size>dls_action': 2, 'model_action>None': 1, 'loss_func>None': 0, 'metric>None': 0, 'gradient_clip>None': 1, 'one_cycle>None': 1, 'n_epoch>None': 2, 'lr>None': 0.004740235218431013, 'wd>None': 1}. Best is trial 0 with value: 3.1735315240257943e+30.\n",
            "Trial 1 finished with value: 3.173532128488704e+30 and parameters: {'dls_action>None': 0, 'batch_size>dls_action': 2, 'model_action>None': 1, 'loss_func>None': 0, 'metric>None': 0, 'gradient_clip>None': 1, 'one_cycle>None': 1, 'n_epoch>None': 2, 'lr>None': 0.004740235218431013, 'wd>None': 1}. Best is trial 0 with value: 3.1735315240257943e+30.\n",
            "Trial 1 finished with value: 3.173532128488704e+30 and parameters: {'dls_action>None': 0, 'batch_size>dls_action': 2, 'model_action>None': 1, 'loss_func>None': 0, 'metric>None': 0, 'gradient_clip>None': 1, 'one_cycle>None': 1, 'n_epoch>None': 2, 'lr>None': 0.004740235218431013, 'wd>None': 1}. Best is trial 0 with value: 3.1735315240257943e+30.\n",
            "Trial 1 finished with value: 3.173532128488704e+30 and parameters: {'dls_action>None': 0, 'batch_size>dls_action': 2, 'model_action>None': 1, 'loss_func>None': 0, 'metric>None': 0, 'gradient_clip>None': 1, 'one_cycle>None': 1, 'n_epoch>None': 2, 'lr>None': 0.004740235218431013, 'wd>None': 1}. Best is trial 0 with value: 3.1735315240257943e+30.\n",
            "Trial 1 finished with value: 3.173532128488704e+30 and parameters: {'dls_action>None': 0, 'batch_size>dls_action': 2, 'model_action>None': 1, 'loss_func>None': 0, 'metric>None': 0, 'gradient_clip>None': 1, 'one_cycle>None': 1, 'n_epoch>None': 2, 'lr>None': 0.004740235218431013, 'wd>None': 1}. Best is trial 0 with value: 3.1735315240257943e+30.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-07 04:21:02,623] Trial 2 finished with value: 3.173532430720159e+30 and parameters: {'dls_action>None': 0, 'batch_size>dls_action': 2, 'model_action>None': 1, 'loss_func>None': 0, 'metric>None': 0, 'gradient_clip>None': 1, 'one_cycle>None': 0, 'lr_low>None': 0.0005876567979976106, 'lr_high>None': 0.07571540414942228, 'n_epoch>None': 0, 'wd>None': 3, 'pct_start>None': 0.2}. Best is trial 0 with value: 3.1735315240257943e+30.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 2 finished with value: 3.173532430720159e+30 and parameters: {'dls_action>None': 0, 'batch_size>dls_action': 2, 'model_action>None': 1, 'loss_func>None': 0, 'metric>None': 0, 'gradient_clip>None': 1, 'one_cycle>None': 0, 'lr_low>None': 0.0005876567979976106, 'lr_high>None': 0.07571540414942228, 'n_epoch>None': 0, 'wd>None': 3, 'pct_start>None': 0.2}. Best is trial 0 with value: 3.1735315240257943e+30.\n",
            "Trial 2 finished with value: 3.173532430720159e+30 and parameters: {'dls_action>None': 0, 'batch_size>dls_action': 2, 'model_action>None': 1, 'loss_func>None': 0, 'metric>None': 0, 'gradient_clip>None': 1, 'one_cycle>None': 0, 'lr_low>None': 0.0005876567979976106, 'lr_high>None': 0.07571540414942228, 'n_epoch>None': 0, 'wd>None': 3, 'pct_start>None': 0.2}. Best is trial 0 with value: 3.1735315240257943e+30.\n",
            "Trial 2 finished with value: 3.173532430720159e+30 and parameters: {'dls_action>None': 0, 'batch_size>dls_action': 2, 'model_action>None': 1, 'loss_func>None': 0, 'metric>None': 0, 'gradient_clip>None': 1, 'one_cycle>None': 0, 'lr_low>None': 0.0005876567979976106, 'lr_high>None': 0.07571540414942228, 'n_epoch>None': 0, 'wd>None': 3, 'pct_start>None': 0.2}. Best is trial 0 with value: 3.1735315240257943e+30.\n",
            "Trial 2 finished with value: 3.173532430720159e+30 and parameters: {'dls_action>None': 0, 'batch_size>dls_action': 2, 'model_action>None': 1, 'loss_func>None': 0, 'metric>None': 0, 'gradient_clip>None': 1, 'one_cycle>None': 0, 'lr_low>None': 0.0005876567979976106, 'lr_high>None': 0.07571540414942228, 'n_epoch>None': 0, 'wd>None': 3, 'pct_start>None': 0.2}. Best is trial 0 with value: 3.1735315240257943e+30.\n",
            "Trial 2 finished with value: 3.173532430720159e+30 and parameters: {'dls_action>None': 0, 'batch_size>dls_action': 2, 'model_action>None': 1, 'loss_func>None': 0, 'metric>None': 0, 'gradient_clip>None': 1, 'one_cycle>None': 0, 'lr_low>None': 0.0005876567979976106, 'lr_high>None': 0.07571540414942228, 'n_epoch>None': 0, 'wd>None': 3, 'pct_start>None': 0.2}. Best is trial 0 with value: 3.1735315240257943e+30.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-07 04:21:04,763] Trial 3 finished with value: 3.173531826257249e+30 and parameters: {'dls_action>None': 0, 'batch_size>dls_action': 2, 'model_action>None': 1, 'loss_func>None': 0, 'metric>None': 0, 'gradient_clip>None': 1, 'one_cycle>None': 0, 'lr_low>None': 0.0005876567979976106, 'lr_high>None': 0.07571540414942228, 'n_epoch>None': 2, 'wd>None': 3, 'pct_start>None': 0.2}. Best is trial 0 with value: 3.1735315240257943e+30.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 3 finished with value: 3.173531826257249e+30 and parameters: {'dls_action>None': 0, 'batch_size>dls_action': 2, 'model_action>None': 1, 'loss_func>None': 0, 'metric>None': 0, 'gradient_clip>None': 1, 'one_cycle>None': 0, 'lr_low>None': 0.0005876567979976106, 'lr_high>None': 0.07571540414942228, 'n_epoch>None': 2, 'wd>None': 3, 'pct_start>None': 0.2}. Best is trial 0 with value: 3.1735315240257943e+30.\n",
            "Trial 3 finished with value: 3.173531826257249e+30 and parameters: {'dls_action>None': 0, 'batch_size>dls_action': 2, 'model_action>None': 1, 'loss_func>None': 0, 'metric>None': 0, 'gradient_clip>None': 1, 'one_cycle>None': 0, 'lr_low>None': 0.0005876567979976106, 'lr_high>None': 0.07571540414942228, 'n_epoch>None': 2, 'wd>None': 3, 'pct_start>None': 0.2}. Best is trial 0 with value: 3.1735315240257943e+30.\n",
            "Trial 3 finished with value: 3.173531826257249e+30 and parameters: {'dls_action>None': 0, 'batch_size>dls_action': 2, 'model_action>None': 1, 'loss_func>None': 0, 'metric>None': 0, 'gradient_clip>None': 1, 'one_cycle>None': 0, 'lr_low>None': 0.0005876567979976106, 'lr_high>None': 0.07571540414942228, 'n_epoch>None': 2, 'wd>None': 3, 'pct_start>None': 0.2}. Best is trial 0 with value: 3.1735315240257943e+30.\n",
            "Trial 3 finished with value: 3.173531826257249e+30 and parameters: {'dls_action>None': 0, 'batch_size>dls_action': 2, 'model_action>None': 1, 'loss_func>None': 0, 'metric>None': 0, 'gradient_clip>None': 1, 'one_cycle>None': 0, 'lr_low>None': 0.0005876567979976106, 'lr_high>None': 0.07571540414942228, 'n_epoch>None': 2, 'wd>None': 3, 'pct_start>None': 0.2}. Best is trial 0 with value: 3.1735315240257943e+30.\n",
            "Trial 3 finished with value: 3.173531826257249e+30 and parameters: {'dls_action>None': 0, 'batch_size>dls_action': 2, 'model_action>None': 1, 'loss_func>None': 0, 'metric>None': 0, 'gradient_clip>None': 1, 'one_cycle>None': 0, 'lr_low>None': 0.0005876567979976106, 'lr_high>None': 0.07571540414942228, 'n_epoch>None': 2, 'wd>None': 3, 'pct_start>None': 0.2}. Best is trial 0 with value: 3.1735315240257943e+30.\n",
            "Saved study to: /content/trial_storage/study_1.pkl\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-07 04:21:13,790] A new study created in memory with name: no-name-404772ae-0cd3-47f7-877b-a051c83695b1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*****************RUNNING: HPO PROCESS*****************\n",
            "A new study created in memory with name: no-name-404772ae-0cd3-47f7-877b-a051c83695b1\n",
            "A new study created in memory with name: no-name-404772ae-0cd3-47f7-877b-a051c83695b1\n",
            "A new study created in memory with name: no-name-404772ae-0cd3-47f7-877b-a051c83695b1\n",
            "A new study created in memory with name: no-name-404772ae-0cd3-47f7-877b-a051c83695b1\n",
            "A new study created in memory with name: no-name-404772ae-0cd3-47f7-877b-a051c83695b1\n",
            "A new study created in memory with name: no-name-404772ae-0cd3-47f7-877b-a051c83695b1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-07 04:21:17,682] Trial 0 finished with value: 3.173531826257249e+30 and parameters: {'dls_action>None': 0, 'batch_size>dls_action': 1, 'model_action>None': 0, 'loss_func>None': 0, 'metric>None': 0, 'gradient_clip>None': 0, 'max_norm>None': 13.19017290924006, 'one_cycle>None': 0, 'lr_low>None': 0.0005876567979976106, 'lr_high>None': 0.07571540414942228, 'n_epoch>None': 0, 'wd>None': 3, 'pct_start>None': 0.2}. Best is trial 0 with value: 3.173531826257249e+30.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 0 finished with value: 3.173531826257249e+30 and parameters: {'dls_action>None': 0, 'batch_size>dls_action': 1, 'model_action>None': 0, 'loss_func>None': 0, 'metric>None': 0, 'gradient_clip>None': 0, 'max_norm>None': 13.19017290924006, 'one_cycle>None': 0, 'lr_low>None': 0.0005876567979976106, 'lr_high>None': 0.07571540414942228, 'n_epoch>None': 0, 'wd>None': 3, 'pct_start>None': 0.2}. Best is trial 0 with value: 3.173531826257249e+30.\n",
            "Trial 0 finished with value: 3.173531826257249e+30 and parameters: {'dls_action>None': 0, 'batch_size>dls_action': 1, 'model_action>None': 0, 'loss_func>None': 0, 'metric>None': 0, 'gradient_clip>None': 0, 'max_norm>None': 13.19017290924006, 'one_cycle>None': 0, 'lr_low>None': 0.0005876567979976106, 'lr_high>None': 0.07571540414942228, 'n_epoch>None': 0, 'wd>None': 3, 'pct_start>None': 0.2}. Best is trial 0 with value: 3.173531826257249e+30.\n",
            "Trial 0 finished with value: 3.173531826257249e+30 and parameters: {'dls_action>None': 0, 'batch_size>dls_action': 1, 'model_action>None': 0, 'loss_func>None': 0, 'metric>None': 0, 'gradient_clip>None': 0, 'max_norm>None': 13.19017290924006, 'one_cycle>None': 0, 'lr_low>None': 0.0005876567979976106, 'lr_high>None': 0.07571540414942228, 'n_epoch>None': 0, 'wd>None': 3, 'pct_start>None': 0.2}. Best is trial 0 with value: 3.173531826257249e+30.\n",
            "Trial 0 finished with value: 3.173531826257249e+30 and parameters: {'dls_action>None': 0, 'batch_size>dls_action': 1, 'model_action>None': 0, 'loss_func>None': 0, 'metric>None': 0, 'gradient_clip>None': 0, 'max_norm>None': 13.19017290924006, 'one_cycle>None': 0, 'lr_low>None': 0.0005876567979976106, 'lr_high>None': 0.07571540414942228, 'n_epoch>None': 0, 'wd>None': 3, 'pct_start>None': 0.2}. Best is trial 0 with value: 3.173531826257249e+30.\n",
            "Trial 0 finished with value: 3.173531826257249e+30 and parameters: {'dls_action>None': 0, 'batch_size>dls_action': 1, 'model_action>None': 0, 'loss_func>None': 0, 'metric>None': 0, 'gradient_clip>None': 0, 'max_norm>None': 13.19017290924006, 'one_cycle>None': 0, 'lr_low>None': 0.0005876567979976106, 'lr_high>None': 0.07571540414942228, 'n_epoch>None': 0, 'wd>None': 3, 'pct_start>None': 0.2}. Best is trial 0 with value: 3.173531826257249e+30.\n",
            "Trial 0 finished with value: 3.173531826257249e+30 and parameters: {'dls_action>None': 0, 'batch_size>dls_action': 1, 'model_action>None': 0, 'loss_func>None': 0, 'metric>None': 0, 'gradient_clip>None': 0, 'max_norm>None': 13.19017290924006, 'one_cycle>None': 0, 'lr_low>None': 0.0005876567979976106, 'lr_high>None': 0.07571540414942228, 'n_epoch>None': 0, 'wd>None': 3, 'pct_start>None': 0.2}. Best is trial 0 with value: 3.173531826257249e+30.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-07 04:21:24,230] Trial 1 finished with value: 3.1735315240257943e+30 and parameters: {'dls_action>None': 0, 'batch_size>dls_action': 2, 'model_action>None': 1, 'loss_func>None': 0, 'metric>None': 0, 'gradient_clip>None': 1, 'one_cycle>None': 0, 'lr_low>None': 0.0005876567979976106, 'lr_high>None': 0.07571540414942228, 'n_epoch>None': 2, 'wd>None': 3, 'pct_start>None': 0.2}. Best is trial 1 with value: 3.1735315240257943e+30.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 1 finished with value: 3.1735315240257943e+30 and parameters: {'dls_action>None': 0, 'batch_size>dls_action': 2, 'model_action>None': 1, 'loss_func>None': 0, 'metric>None': 0, 'gradient_clip>None': 1, 'one_cycle>None': 0, 'lr_low>None': 0.0005876567979976106, 'lr_high>None': 0.07571540414942228, 'n_epoch>None': 2, 'wd>None': 3, 'pct_start>None': 0.2}. Best is trial 1 with value: 3.1735315240257943e+30.\n",
            "Trial 1 finished with value: 3.1735315240257943e+30 and parameters: {'dls_action>None': 0, 'batch_size>dls_action': 2, 'model_action>None': 1, 'loss_func>None': 0, 'metric>None': 0, 'gradient_clip>None': 1, 'one_cycle>None': 0, 'lr_low>None': 0.0005876567979976106, 'lr_high>None': 0.07571540414942228, 'n_epoch>None': 2, 'wd>None': 3, 'pct_start>None': 0.2}. Best is trial 1 with value: 3.1735315240257943e+30.\n",
            "Trial 1 finished with value: 3.1735315240257943e+30 and parameters: {'dls_action>None': 0, 'batch_size>dls_action': 2, 'model_action>None': 1, 'loss_func>None': 0, 'metric>None': 0, 'gradient_clip>None': 1, 'one_cycle>None': 0, 'lr_low>None': 0.0005876567979976106, 'lr_high>None': 0.07571540414942228, 'n_epoch>None': 2, 'wd>None': 3, 'pct_start>None': 0.2}. Best is trial 1 with value: 3.1735315240257943e+30.\n",
            "Trial 1 finished with value: 3.1735315240257943e+30 and parameters: {'dls_action>None': 0, 'batch_size>dls_action': 2, 'model_action>None': 1, 'loss_func>None': 0, 'metric>None': 0, 'gradient_clip>None': 1, 'one_cycle>None': 0, 'lr_low>None': 0.0005876567979976106, 'lr_high>None': 0.07571540414942228, 'n_epoch>None': 2, 'wd>None': 3, 'pct_start>None': 0.2}. Best is trial 1 with value: 3.1735315240257943e+30.\n",
            "Trial 1 finished with value: 3.1735315240257943e+30 and parameters: {'dls_action>None': 0, 'batch_size>dls_action': 2, 'model_action>None': 1, 'loss_func>None': 0, 'metric>None': 0, 'gradient_clip>None': 1, 'one_cycle>None': 0, 'lr_low>None': 0.0005876567979976106, 'lr_high>None': 0.07571540414942228, 'n_epoch>None': 2, 'wd>None': 3, 'pct_start>None': 0.2}. Best is trial 1 with value: 3.1735315240257943e+30.\n",
            "Trial 1 finished with value: 3.1735315240257943e+30 and parameters: {'dls_action>None': 0, 'batch_size>dls_action': 2, 'model_action>None': 1, 'loss_func>None': 0, 'metric>None': 0, 'gradient_clip>None': 1, 'one_cycle>None': 0, 'lr_low>None': 0.0005876567979976106, 'lr_high>None': 0.07571540414942228, 'n_epoch>None': 2, 'wd>None': 3, 'pct_start>None': 0.2}. Best is trial 1 with value: 3.1735315240257943e+30.\n",
            "Saved study to: /content/trial_storage/study_2.pkl\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-07 04:21:38,888] A new study created in memory with name: no-name-f790750f-cc95-475d-b8d2-41345da64cf5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*****************RUNNING: HPO PROCESS*****************\n",
            "A new study created in memory with name: no-name-f790750f-cc95-475d-b8d2-41345da64cf5\n",
            "A new study created in memory with name: no-name-f790750f-cc95-475d-b8d2-41345da64cf5\n",
            "A new study created in memory with name: no-name-f790750f-cc95-475d-b8d2-41345da64cf5\n",
            "A new study created in memory with name: no-name-f790750f-cc95-475d-b8d2-41345da64cf5\n",
            "A new study created in memory with name: no-name-f790750f-cc95-475d-b8d2-41345da64cf5\n",
            "A new study created in memory with name: no-name-f790750f-cc95-475d-b8d2-41345da64cf5\n",
            "A new study created in memory with name: no-name-f790750f-cc95-475d-b8d2-41345da64cf5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-10-07 04:21:45,362] Trial 0 finished with value: 3.173531826257249e+30 and parameters: {'dls_action>None': 0, 'batch_size>dls_action': 2, 'model_action>None': 1, 'loss_func>None': 0, 'metric>None': 0, 'gradient_clip>None': 1, 'one_cycle>None': 0, 'lr_low>None': 0.0005876567979976106, 'lr_high>None': 0.07571540414942228, 'n_epoch>None': 2, 'wd>None': 3, 'pct_start>None': 0.2}. Best is trial 0 with value: 3.173531826257249e+30.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 0 finished with value: 3.173531826257249e+30 and parameters: {'dls_action>None': 0, 'batch_size>dls_action': 2, 'model_action>None': 1, 'loss_func>None': 0, 'metric>None': 0, 'gradient_clip>None': 1, 'one_cycle>None': 0, 'lr_low>None': 0.0005876567979976106, 'lr_high>None': 0.07571540414942228, 'n_epoch>None': 2, 'wd>None': 3, 'pct_start>None': 0.2}. Best is trial 0 with value: 3.173531826257249e+30.\n",
            "Trial 0 finished with value: 3.173531826257249e+30 and parameters: {'dls_action>None': 0, 'batch_size>dls_action': 2, 'model_action>None': 1, 'loss_func>None': 0, 'metric>None': 0, 'gradient_clip>None': 1, 'one_cycle>None': 0, 'lr_low>None': 0.0005876567979976106, 'lr_high>None': 0.07571540414942228, 'n_epoch>None': 2, 'wd>None': 3, 'pct_start>None': 0.2}. Best is trial 0 with value: 3.173531826257249e+30.\n",
            "Trial 0 finished with value: 3.173531826257249e+30 and parameters: {'dls_action>None': 0, 'batch_size>dls_action': 2, 'model_action>None': 1, 'loss_func>None': 0, 'metric>None': 0, 'gradient_clip>None': 1, 'one_cycle>None': 0, 'lr_low>None': 0.0005876567979976106, 'lr_high>None': 0.07571540414942228, 'n_epoch>None': 2, 'wd>None': 3, 'pct_start>None': 0.2}. Best is trial 0 with value: 3.173531826257249e+30.\n",
            "Trial 0 finished with value: 3.173531826257249e+30 and parameters: {'dls_action>None': 0, 'batch_size>dls_action': 2, 'model_action>None': 1, 'loss_func>None': 0, 'metric>None': 0, 'gradient_clip>None': 1, 'one_cycle>None': 0, 'lr_low>None': 0.0005876567979976106, 'lr_high>None': 0.07571540414942228, 'n_epoch>None': 2, 'wd>None': 3, 'pct_start>None': 0.2}. Best is trial 0 with value: 3.173531826257249e+30.\n",
            "Trial 0 finished with value: 3.173531826257249e+30 and parameters: {'dls_action>None': 0, 'batch_size>dls_action': 2, 'model_action>None': 1, 'loss_func>None': 0, 'metric>None': 0, 'gradient_clip>None': 1, 'one_cycle>None': 0, 'lr_low>None': 0.0005876567979976106, 'lr_high>None': 0.07571540414942228, 'n_epoch>None': 2, 'wd>None': 3, 'pct_start>None': 0.2}. Best is trial 0 with value: 3.173531826257249e+30.\n",
            "Trial 0 finished with value: 3.173531826257249e+30 and parameters: {'dls_action>None': 0, 'batch_size>dls_action': 2, 'model_action>None': 1, 'loss_func>None': 0, 'metric>None': 0, 'gradient_clip>None': 1, 'one_cycle>None': 0, 'lr_low>None': 0.0005876567979976106, 'lr_high>None': 0.07571540414942228, 'n_epoch>None': 2, 'wd>None': 3, 'pct_start>None': 0.2}. Best is trial 0 with value: 3.173531826257249e+30.\n",
            "Trial 0 finished with value: 3.173531826257249e+30 and parameters: {'dls_action>None': 0, 'batch_size>dls_action': 2, 'model_action>None': 1, 'loss_func>None': 0, 'metric>None': 0, 'gradient_clip>None': 1, 'one_cycle>None': 0, 'lr_low>None': 0.0005876567979976106, 'lr_high>None': 0.07571540414942228, 'n_epoch>None': 2, 'wd>None': 3, 'pct_start>None': 0.2}. Best is trial 0 with value: 3.173531826257249e+30.\n",
            "Saved study to: /content/trial_storage/study_3.pkl\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "*****************************************************************\n",
            "\n",
            "CombinedSampler:\n",
            "**********ActionSampler**********\n",
            "Var of type: model_action, signature of: None\n",
            "Configuration Space:\n",
            "'choices' = [<__main__.ModelAction object at 0x7f72bfc9cb50>]\n",
            "\n",
            "Var of type: dls_action, signature of: None\n",
            "Configuration Space:\n",
            "'choices' = [<__main__.Action object at 0x7f72bfc9d6c0>]\n",
            "\n",
            "\n",
            "**********PrimitiveSampler**********\n",
            "Var of type: pct_start, signature of: None\n",
            "Configuration Space:\n",
            "'low': 0.2, 'high': 0.2, 'log': False, 'step': 0.05\n",
            "\n",
            "Var of type: wd, signature of: None\n",
            "Configuration Space:\n",
            "'choices' = [0.1]\n",
            "\n",
            "Var of type: one_cycle, signature of: None\n",
            "Configuration Space:\n",
            "'choices' = [True]\n",
            "\n",
            "Var of type: n_epoch, signature of: None\n",
            "Configuration Space:\n",
            "'choices' = [15]\n",
            "\n",
            "Var of type: lr_low, signature of: None\n",
            "Configuration Space:\n",
            "'low': 0.0005876567979976106, 'high': 0.0005876567979976106, 'log': True, 'step': unspecified\n",
            "\n",
            "Var of type: lr, signature of: None\n",
            "Configuration Space:\n",
            "'low': 0.004740235218431013, 'high': 0.004740235218431013, 'log': True, 'step': unspecified\n",
            "\n",
            "Var of type: metric, signature of: None\n",
            "Configuration Space:\n",
            "'choices' = [<function custom_mse at 0x7f72bd1d77f0>]\n",
            "\n",
            "Var of type: gradient_clip, signature of: None\n",
            "Configuration Space:\n",
            "'choices' = [False]\n",
            "\n",
            "Var of type: loss_func, signature of: None\n",
            "Configuration Space:\n",
            "'choices' = [<function custom_mse at 0x7f72bd1d77f0>]\n",
            "\n",
            "Var of type: lr_high, signature of: None\n",
            "Configuration Space:\n",
            "'low': 0.07571540414942228, 'high': 0.07571540414942228, 'log': True, 'step': unspecified\n",
            "\n",
            "Var of type: freeze, signature of: None\n",
            "Configuration Space:\n",
            "'choices' = [True, False]\n",
            "\n",
            "Var of type: max_norm, signature of: None\n",
            "Configuration Space:\n",
            "'low': 13.19017290924006, 'high': 13.19017290924006, 'log': False, 'step': unspecified\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*****************************************************************\n",
            "\n",
            "4\n",
            "Loaded study from: /content/trial_storage/study_2.pkl\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"c45e8a99-d765-4fc7-ac35-f6fc3c6b3d38\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"c45e8a99-d765-4fc7-ac35-f6fc3c6b3d38\")) {                    Plotly.newPlot(                        \"c45e8a99-d765-4fc7-ac35-f6fc3c6b3d38\",                        [{\"mode\":\"markers\",\"name\":\"Objective Value\",\"x\":[0,1],\"y\":[3.173531826257249e+30,3.1735315240257943e+30],\"type\":\"scatter\"},{\"mode\":\"lines\",\"name\":\"Best Value\",\"x\":[0,1],\"y\":[3.173531826257249e+30,3.1735315240257943e+30],\"type\":\"scatter\"},{\"marker\":{\"color\":\"#cccccc\"},\"mode\":\"markers\",\"name\":\"Infeasible Trial\",\"showlegend\":false,\"x\":[],\"y\":[],\"type\":\"scatter\"}],                        {\"title\":{\"text\":\"Optimization History Plot\"},\"xaxis\":{\"title\":{\"text\":\"Trial\"}},\"yaxis\":{\"title\":{\"text\":\"Objective Value\"}},\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('c45e8a99-d765-4fc7-ac35-f6fc3c6b3d38');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3xsKPGuzNKd2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}